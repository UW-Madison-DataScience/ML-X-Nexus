{
  "hash": "0fb0a4e6db106f31117811054d7f0c60",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Titanic Dataset\"\nauthor: \n  - name: Chris Endemann\n    email: endemann@wisc.edu\n    \ndate: 2024-10-07\ndate-format: long\nimage: \"../../../images/titanic.png\"\n\ncategories: \n  - EDA\n  - Tabular\n  \njupyter: python3\n---\n\n\n\n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/UW-Madison-DataScience/ML-X-Nexus/blob/main/Applications/EDA-examples/Titanic-Dataset.ipynb)\n\n### Data & Problem Intro\nThe Titanic dataset is a well-known dataset that contains information about the passengers of the Titanic ship.It includes variables such as age, gender, class, fare, and whether each passenger survived.\n\nThe problem we are exploring is binary classification: predicting whether a passenger survived based on their features.The goal of this EDA is to uncover insights that can guide our modeling decisions, such as identifying important features, handling missing data, and addressing bias in the dataset (e.g., imbalances in survival rate by gender or class).\n\n## Step 0: Looking Up Each Feature\nBefore diving into the analysis, it's important to understand what each feature in the dataset represents. This ensures we're interpreting the data correctly and allows us to make informed decisions during the analysis.\n\n### Titanic Dataset Features:\n- **survived**: Whether the passenger survived (0 = No, 1 = Yes).\n- **pclass**: Ticket class (1 = 1st, 2 = 2nd, 3 = 3rd).\n- **sex**: Gender of the passenger.\n- **age**: Age of the passenger in years. Some values are missing.\n- **sibsp**: Number of siblings/spouses aboard the Titanic.\n- **parch**: Number of parents/children aboard the Titanic.\n- **fare**: Passenger fare.\n- **embarked**: Port of embarkation (C = Cherbourg; Q = Queenstown; S = Southampton).\n- **class**: Duplicate of 'pclass' (used for plotting by Seaborn).\n- **who**: Describes whether the passenger is a man, woman, or child.\n- **adult_male**: Indicates whether the passenger is an adult male (True/False).\n- **deck**: The deck the passenger was on (missing for many passengers).\n- **embark_town**: The name of the town where the passenger boarded.\n- **alive**: Indicator of whether the passenger survived (Yes/No, derived from 'survived').\n- **alone**: Indicates whether the passenger was traveling alone (True/False).\n\n## Step 1: Visualizing data in its rawest form\nLet's take a look at a small sample of the dataset to understand the raw data we're working with. This gives us a chance to spot obvious issues or patterns.\n\n### Import libraries and load dataset\n\n::: {#KwXSUvuxAU6f .cell outputId='78324c3f-a518-4686-d059-97af32f58f42' execution_count=1}\n``` {.python .cell-code}\n# Import necessary libraries\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load the Titanic dataset from seaborn\ndf = sns.load_dataset('titanic')\n\n# Display the first few rows of the dataset\ndf.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>survived</th>\n      <th>pclass</th>\n      <th>sex</th>\n      <th>age</th>\n      <th>sibsp</th>\n      <th>parch</th>\n      <th>fare</th>\n      <th>embarked</th>\n      <th>class</th>\n      <th>who</th>\n      <th>adult_male</th>\n      <th>deck</th>\n      <th>embark_town</th>\n      <th>alive</th>\n      <th>alone</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>3</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7.2500</td>\n      <td>S</td>\n      <td>Third</td>\n      <td>man</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>Southampton</td>\n      <td>no</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>71.2833</td>\n      <td>C</td>\n      <td>First</td>\n      <td>woman</td>\n      <td>False</td>\n      <td>C</td>\n      <td>Cherbourg</td>\n      <td>yes</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>3</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.9250</td>\n      <td>S</td>\n      <td>Third</td>\n      <td>woman</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>Southampton</td>\n      <td>yes</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n      <td>female</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>53.1000</td>\n      <td>S</td>\n      <td>First</td>\n      <td>woman</td>\n      <td>False</td>\n      <td>C</td>\n      <td>Southampton</td>\n      <td>yes</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>3</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.0500</td>\n      <td>S</td>\n      <td>Third</td>\n      <td>man</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>Southampton</td>\n      <td>no</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#Evcu5I2oBAPG .cell outputId='88dd76e7-dbf5-4b1f-8a3e-b0606c932020' execution_count=2}\n``` {.python .cell-code}\n# Show a small random sample of the data\nprint(\"Sample of 10 passengers:\")\ndf.sample(30)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSample of 10 passengers:\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>survived</th>\n      <th>pclass</th>\n      <th>sex</th>\n      <th>age</th>\n      <th>sibsp</th>\n      <th>parch</th>\n      <th>fare</th>\n      <th>embarked</th>\n      <th>class</th>\n      <th>who</th>\n      <th>adult_male</th>\n      <th>deck</th>\n      <th>embark_town</th>\n      <th>alive</th>\n      <th>alone</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>866</th>\n      <td>1</td>\n      <td>2</td>\n      <td>female</td>\n      <td>27.00</td>\n      <td>1</td>\n      <td>0</td>\n      <td>13.8583</td>\n      <td>C</td>\n      <td>Second</td>\n      <td>woman</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>Cherbourg</td>\n      <td>yes</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>663</th>\n      <td>0</td>\n      <td>3</td>\n      <td>male</td>\n      <td>36.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.4958</td>\n      <td>S</td>\n      <td>Third</td>\n      <td>man</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>Southampton</td>\n      <td>no</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>279</th>\n      <td>1</td>\n      <td>3</td>\n      <td>female</td>\n      <td>35.00</td>\n      <td>1</td>\n      <td>1</td>\n      <td>20.2500</td>\n      <td>S</td>\n      <td>Third</td>\n      <td>woman</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>Southampton</td>\n      <td>yes</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>623</th>\n      <td>0</td>\n      <td>3</td>\n      <td>male</td>\n      <td>21.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.8542</td>\n      <td>S</td>\n      <td>Third</td>\n      <td>man</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>Southampton</td>\n      <td>no</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>565</th>\n      <td>0</td>\n      <td>3</td>\n      <td>male</td>\n      <td>24.00</td>\n      <td>2</td>\n      <td>0</td>\n      <td>24.1500</td>\n      <td>S</td>\n      <td>Third</td>\n      <td>man</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>Southampton</td>\n      <td>no</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>390</th>\n      <td>1</td>\n      <td>1</td>\n      <td>male</td>\n      <td>36.00</td>\n      <td>1</td>\n      <td>2</td>\n      <td>120.0000</td>\n      <td>S</td>\n      <td>First</td>\n      <td>man</td>\n      <td>True</td>\n      <td>B</td>\n      <td>Southampton</td>\n      <td>yes</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>364</th>\n      <td>0</td>\n      <td>3</td>\n      <td>male</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>0</td>\n      <td>15.5000</td>\n      <td>Q</td>\n      <td>Third</td>\n      <td>man</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>Queenstown</td>\n      <td>no</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>589</th>\n      <td>0</td>\n      <td>3</td>\n      <td>male</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.0500</td>\n      <td>S</td>\n      <td>Third</td>\n      <td>man</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>Southampton</td>\n      <td>no</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>0</td>\n      <td>2</td>\n      <td>male</td>\n      <td>19.00</td>\n      <td>1</td>\n      <td>1</td>\n      <td>36.7500</td>\n      <td>S</td>\n      <td>Second</td>\n      <td>man</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>Southampton</td>\n      <td>no</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>539</th>\n      <td>1</td>\n      <td>1</td>\n      <td>female</td>\n      <td>22.00</td>\n      <td>0</td>\n      <td>2</td>\n      <td>49.5000</td>\n      <td>C</td>\n      <td>First</td>\n      <td>woman</td>\n      <td>False</td>\n      <td>B</td>\n      <td>Cherbourg</td>\n      <td>yes</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>619</th>\n      <td>0</td>\n      <td>2</td>\n      <td>male</td>\n      <td>26.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>10.5000</td>\n      <td>S</td>\n      <td>Second</td>\n      <td>man</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>Southampton</td>\n      <td>no</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>756</th>\n      <td>0</td>\n      <td>3</td>\n      <td>male</td>\n      <td>28.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.7958</td>\n      <td>S</td>\n      <td>Third</td>\n      <td>man</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>Southampton</td>\n      <td>no</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>784</th>\n      <td>0</td>\n      <td>3</td>\n      <td>male</td>\n      <td>25.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.0500</td>\n      <td>S</td>\n      <td>Third</td>\n      <td>man</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>Southampton</td>\n      <td>no</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>681</th>\n      <td>1</td>\n      <td>1</td>\n      <td>male</td>\n      <td>27.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>76.7292</td>\n      <td>C</td>\n      <td>First</td>\n      <td>man</td>\n      <td>True</td>\n      <td>D</td>\n      <td>Cherbourg</td>\n      <td>yes</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>725</th>\n      <td>0</td>\n      <td>3</td>\n      <td>male</td>\n      <td>20.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.6625</td>\n      <td>S</td>\n      <td>Third</td>\n      <td>man</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>Southampton</td>\n      <td>no</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>305</th>\n      <td>1</td>\n      <td>1</td>\n      <td>male</td>\n      <td>0.92</td>\n      <td>1</td>\n      <td>2</td>\n      <td>151.5500</td>\n      <td>S</td>\n      <td>First</td>\n      <td>child</td>\n      <td>False</td>\n      <td>C</td>\n      <td>Southampton</td>\n      <td>yes</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>631</th>\n      <td>0</td>\n      <td>3</td>\n      <td>male</td>\n      <td>51.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.0542</td>\n      <td>S</td>\n      <td>Third</td>\n      <td>man</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>Southampton</td>\n      <td>no</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>238</th>\n      <td>0</td>\n      <td>2</td>\n      <td>male</td>\n      <td>19.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>10.5000</td>\n      <td>S</td>\n      <td>Second</td>\n      <td>man</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>Southampton</td>\n      <td>no</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>777</th>\n      <td>1</td>\n      <td>3</td>\n      <td>female</td>\n      <td>5.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>12.4750</td>\n      <td>S</td>\n      <td>Third</td>\n      <td>child</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>Southampton</td>\n      <td>yes</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>327</th>\n      <td>1</td>\n      <td>2</td>\n      <td>female</td>\n      <td>36.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>13.0000</td>\n      <td>S</td>\n      <td>Second</td>\n      <td>woman</td>\n      <td>False</td>\n      <td>D</td>\n      <td>Southampton</td>\n      <td>yes</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>83</th>\n      <td>0</td>\n      <td>1</td>\n      <td>male</td>\n      <td>28.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>47.1000</td>\n      <td>S</td>\n      <td>First</td>\n      <td>man</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>Southampton</td>\n      <td>no</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>381</th>\n      <td>1</td>\n      <td>3</td>\n      <td>female</td>\n      <td>1.00</td>\n      <td>0</td>\n      <td>2</td>\n      <td>15.7417</td>\n      <td>C</td>\n      <td>Third</td>\n      <td>child</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>Cherbourg</td>\n      <td>yes</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>103</th>\n      <td>0</td>\n      <td>3</td>\n      <td>male</td>\n      <td>33.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.6542</td>\n      <td>S</td>\n      <td>Third</td>\n      <td>man</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>Southampton</td>\n      <td>no</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>265</th>\n      <td>0</td>\n      <td>2</td>\n      <td>male</td>\n      <td>36.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>10.5000</td>\n      <td>S</td>\n      <td>Second</td>\n      <td>man</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>Southampton</td>\n      <td>no</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>489</th>\n      <td>1</td>\n      <td>3</td>\n      <td>male</td>\n      <td>9.00</td>\n      <td>1</td>\n      <td>1</td>\n      <td>15.9000</td>\n      <td>S</td>\n      <td>Third</td>\n      <td>child</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>Southampton</td>\n      <td>yes</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>197</th>\n      <td>0</td>\n      <td>3</td>\n      <td>male</td>\n      <td>42.00</td>\n      <td>0</td>\n      <td>1</td>\n      <td>8.4042</td>\n      <td>S</td>\n      <td>Third</td>\n      <td>man</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>Southampton</td>\n      <td>no</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>596</th>\n      <td>1</td>\n      <td>2</td>\n      <td>female</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>33.0000</td>\n      <td>S</td>\n      <td>Second</td>\n      <td>woman</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>Southampton</td>\n      <td>yes</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>1</td>\n      <td>3</td>\n      <td>female</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.7500</td>\n      <td>Q</td>\n      <td>Third</td>\n      <td>woman</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>Queenstown</td>\n      <td>yes</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>858</th>\n      <td>1</td>\n      <td>3</td>\n      <td>female</td>\n      <td>24.00</td>\n      <td>0</td>\n      <td>3</td>\n      <td>19.2583</td>\n      <td>C</td>\n      <td>Third</td>\n      <td>woman</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>Cherbourg</td>\n      <td>yes</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>448</th>\n      <td>1</td>\n      <td>3</td>\n      <td>female</td>\n      <td>5.00</td>\n      <td>2</td>\n      <td>1</td>\n      <td>19.2583</td>\n      <td>C</td>\n      <td>Third</td>\n      <td>child</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>Cherbourg</td>\n      <td>yes</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### Insights\n- We see various features such as age, sex, class, fare, and whether the passenger survived.\n- This helps us get a quick overview of what kind of data we're working with, including potential issues like missing values.\n- Appears we have some redundant columns.\n- Some NaNs clearly visible in deck and age.\n\n#### Remove redundant columns\nIt looks like alive and survived are identical. Same for embarked and emback_town. Let's run a check to see if these columns are truly identical and remove them, if so.\n\n::: {#4oth-qmMAkYm .cell outputId='17836d3b-2e74-4d2f-dff4-171406fbd2b7' execution_count=3}\n``` {.python .cell-code}\n# Check if the two columns are identical\nare_identical = df['survived'].equals(df['alive'].apply(lambda x: 1 if x == 'yes' else 0))\n\n# Print the result\nprint(f\"Are 'survived' and 'alive' identical? {are_identical}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAre 'survived' and 'alive' identical? True\n```\n:::\n:::\n\n\n::: {#Ths0um_ZBHto .cell outputId='8057781d-6461-4b8e-e5be-4cf30065abfc' execution_count=4}\n``` {.python .cell-code}\n# Check unique values in both columns\nprint(\"Unique values in 'embarked':\", df['embarked'].unique())\nprint(\"Unique values in 'embark_town':\", df['embark_town'].unique())\n\n# Map 'embarked' codes to 'embark_town' names\nembarked_mapping = {'S': 'Southampton', 'C': 'Cherbourg', 'Q': 'Queenstown'}\n\n# Apply the mapping to the 'embarked' column\ndf['embarked_mapped'] = df['embarked'].map(embarked_mapping)\n\n# Check if the mapped 'embarked' column is identical to 'embark_town'\nare_identical = df['embarked_mapped'].equals(df['embark_town'])\n\n# Print the result\nprint(f\"Are 'embarked' and 'embark_town' identical? {are_identical}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUnique values in 'embarked': ['S' 'C' 'Q' nan]\nUnique values in 'embark_town': ['Southampton' 'Cherbourg' 'Queenstown' nan]\nAre 'embarked' and 'embark_town' identical? True\n```\n:::\n:::\n\n\n::: {#2ZIwIaWQA4j5 .cell execution_count=5}\n``` {.python .cell-code}\ndf.drop('alive',axis=1,inplace=True)\ndf.drop('embark_town',axis=1,inplace=True)\ndf.drop('embarked_mapped',axis=1,inplace=True)\ndf.drop('class',axis=1,inplace=True) #explicitly equiv. from documentation\n```\n:::\n\n\n## Step 2: Check data types\n\n::: {#BT1hyZ-pV8XF .cell outputId='1cc28853-848e-49d6-884b-b0261cb25213' execution_count=6}\n``` {.python .cell-code}\ndf.dtypes\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\nsurvived         int64\npclass           int64\nsex             object\nage            float64\nsibsp            int64\nparch            int64\nfare           float64\nembarked        object\nwho             object\nadult_male        bool\ndeck          category\nalone             bool\ndtype: object\n```\n:::\n:::\n\n\n## Step 3: Basic Statistics\nNow we'll summarize the numerical and categorical columns to better understand the central tendencies, variability, and potential missing data.\n\nWhen exploring numerical data:\n- **Mean vs Median**: Compare the mean and median. If the mean is much higher or lower than the median, this suggests skewness in the data, possibly due to outliers.\n- **Min and Max**: Look at the minimum and maximum values to detect extreme outliers or potential data entry errors.\n- **Standard deviation**: A high standard deviation indicates that the data points are spread out over a wide range of values, while a low standard deviation suggests that data points are clustered around the mean.\n\nWhen exploring **categorical data**:\n- **Unique counts**: Check the number of unique categories. For example, the 'sex' column has two unique categories ('male' and 'female'), while 'pclass' has three.\n- **Mode**: The mode (most frequent category) helps us understand which category dominates the dataset. For example, if most passengers are male or most belong to a particular class, this would inform our analysis.\n- **Frequency distribution**: Look at the frequency of each category to identify potential imbalances. For example, are there more passengers from a particular class or embarkation point? Imbalanced categories can bias the model if not handled properly.\n- **Missing or rare categories**: If a category has very few occurrences, this might suggest noise or anomalies in the data.\n\n::: {#NLLehBBTBE2a .cell outputId='329798ee-a7a2-4853-b543-7c966b899d11' execution_count=7}\n``` {.python .cell-code}\n# Summarize the numerical and categorical columns\nprint(\"\\nBasic statistics for numerical columns:\")\ndf.describe(include='all') # include='all' ensures both numeric and binary features are described\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nBasic statistics for numerical columns:\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=7}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>survived</th>\n      <th>pclass</th>\n      <th>sex</th>\n      <th>age</th>\n      <th>sibsp</th>\n      <th>parch</th>\n      <th>fare</th>\n      <th>embarked</th>\n      <th>who</th>\n      <th>adult_male</th>\n      <th>deck</th>\n      <th>alone</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>891.000000</td>\n      <td>891.000000</td>\n      <td>891</td>\n      <td>714.000000</td>\n      <td>891.000000</td>\n      <td>891.000000</td>\n      <td>891.000000</td>\n      <td>889</td>\n      <td>891</td>\n      <td>891</td>\n      <td>203</td>\n      <td>891</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>3</td>\n      <td>2</td>\n      <td>7</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>male</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td>man</td>\n      <td>True</td>\n      <td>C</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>577</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>644</td>\n      <td>537</td>\n      <td>537</td>\n      <td>59</td>\n      <td>537</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.383838</td>\n      <td>2.308642</td>\n      <td>NaN</td>\n      <td>29.699118</td>\n      <td>0.523008</td>\n      <td>0.381594</td>\n      <td>32.204208</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.486592</td>\n      <td>0.836071</td>\n      <td>NaN</td>\n      <td>14.526497</td>\n      <td>1.102743</td>\n      <td>0.806057</td>\n      <td>49.693429</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>NaN</td>\n      <td>0.420000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>NaN</td>\n      <td>20.125000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>7.910400</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.000000</td>\n      <td>3.000000</td>\n      <td>NaN</td>\n      <td>28.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>14.454200</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>NaN</td>\n      <td>38.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>31.000000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>NaN</td>\n      <td>80.000000</td>\n      <td>8.000000</td>\n      <td>6.000000</td>\n      <td>512.329200</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### Insights\n- **Age**: The mean age is around 30, but with missing values and some extreme values (min = 0.42, max = 80). The mean and median are close, suggesting a fairly symmetric distribution for most passengers.\n- **Fare**: The fare has a wide range, from 0 to 512. This large range and the difference between mean and median (mean > median) suggest the presence of outliers, with some passengers paying much higher fares.\n- **Parch and SibSp**: Most passengers had few or no relatives onboard, with a median of 0 for both columns.\n- **Pclass**: Most passengers are in 3rd class (mode: 3).\n\n\nThe df.describe(include='object') part of the code is used to generate summary statistics specifically for categorical (or object) columns in a DataFrame.\n\n::: {#canVv40UFY57 .cell outputId='d4dd6b5f-2b74-40a9-c352-a73c5c86ac74' execution_count=8}\n``` {.python .cell-code}\nprint(\"\\nBasic statistics for categorical columns:\")\ndf.describe(include='object')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nBasic statistics for categorical columns:\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sex</th>\n      <th>embarked</th>\n      <th>who</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>891</td>\n      <td>889</td>\n      <td>891</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>2</td>\n      <td>3</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>male</td>\n      <td>S</td>\n      <td>man</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>577</td>\n      <td>644</td>\n      <td>537</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### Insights\n- A significant portion of passengers did not survive (survival rate is less than 50%).\n- There are more males than females in this data, with adult males being the most common (vs boys)\n- Southampton (S) is the most common embarkation point (out of three options)\n\n\nWhile this summary provides useful information, it does not reveal the distribution of all categories, especially the rare categories. For instance, it only shows the most frequent category (the mode) and its frequency, but it doesn't show how the other categories are distributed, especially if there are categories with very few occurrences.\n\n## Step 2.1: Identifying Rare Categories\nTo detect rare categories in the dataset, we will examine the frequency distribution of each categorical column. This will help us understand if there are categories with very few occurrences, which could either be noise or anomalies.\n\nRare categories are important to identify because they can introduce bias or affect model performance if not handled properly.\n\n::: {#LOzLR7QzCjj2 .cell outputId='97e1d61d-42f4-426e-c0cd-32dad7cdc744' execution_count=9}\n``` {.python .cell-code}\n# Frequency distribution for categorical columns\ncategorical_cols = df.select_dtypes(include='object').columns\n\nfor col in categorical_cols:\n    print(f\"Value counts for {col}:\")\n    print(df[col].value_counts())\n    print(\"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nValue counts for sex:\nsex\nmale      577\nfemale    314\nName: count, dtype: int64\n\n\nValue counts for embarked:\nembarked\nS    644\nC    168\nQ     77\nName: count, dtype: int64\n\n\nValue counts for who:\nwho\nman      537\nwoman    271\nchild     83\nName: count, dtype: int64\n\n\n```\n:::\n:::\n\n\n### Insights\n- **Sex**: No rare categories, with a fairly even distribution between males and females.\n- **Pclass**: Most passengers are in 3rd class, but no rare categories.\n- **Embarked**: The 'C' and 'Q' embarkation points are far less common than 'S' (Southampton).\n\n## Step 3: Counting Missing Values (NaNs)\nTo get a better understanding of where data is missing, we'll count the number of NaN values in each column.\nThis is important for understanding which features will need imputation or may need to be excluded from analysis.\n\n::: {#zdavDF5TEOjh .cell outputId='4f3a5444-ab2a-44d2-bd20-7adfd9183700' execution_count=10}\n``` {.python .cell-code}\n# Count the number of NaN values in each column\nprint(\"Number of NaNs per column:\")\nprint(df.isna().sum())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of NaNs per column:\nsurvived        0\npclass          0\nsex             0\nage           177\nsibsp           0\nparch           0\nfare            0\nembarked        2\nwho             0\nadult_male      0\ndeck          688\nalone           0\ndtype: int64\n```\n:::\n:::\n\n\n## Step 3.1: Visualizing Missing Data\nVisualizing missing data helps us understand how much data is missing and where. This informs how we should handle missing values during preprocessing.\n\nThe 'age' column has many missing values, which could affect our analysis, especially when trying to assess survival rates across different age groups. One potential approach to deal with missing 'age' values is to use the 'who' feature, which categorizes passengers as men, women, or children. By looking at the distribution of age within each 'who' group, we could potentially im\n\n::: {#9x2gPkaoIsdk .cell outputId='b7ba76b1-cb30-478a-a991-89fe24535211' execution_count=11}\n``` {.python .cell-code}\nimport missingno as msno\n\n# Visualize missing data using missingno\nmsno.matrix(df)\nplt.show()\n\nmsno.bar(df)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Titanic-Dataset_files/figure-html/cell-12-output-1.png){width=1963 height=882}\n:::\n\n::: {.cell-output .cell-output-display}\n![](Titanic-Dataset_files/figure-html/cell-12-output-2.png){width=1972 height=919}\n:::\n:::\n\n\n### Insights\n- The 'age' and 'deck' columns have a significant number of missing values.\n- 'embarked' has a couple of missing values\n- We'll need to decide how to handle these NaNs before proceeding with modeling. The 'deck' column may have too many missing values to be useful without significant imputation.\n\n::: {#ocFGp75g6bOi .cell execution_count=12}\n``` {.python .cell-code}\n# remove deck column (not enough info here)\ndf.drop(['deck'],axis=1,inplace=True)\n```\n:::\n\n\n::: {#WxdildJSJgzT .cell outputId='ba77c253-2a9f-47f9-cf58-68770813a65d' execution_count=13}\n``` {.python .cell-code}\n# Let's explore the relationship between 'who' and 'age' to see if we can use 'who' for imputing missing ages\nsns.boxplot(x='who', y='age', data=df)\nplt.title(\"Age Distribution by Who (Men, Women, Children)\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Titanic-Dataset_files/figure-html/cell-14-output-1.png){width=585 height=449}\n:::\n:::\n\n\n### Insights\n- The 'age' column has many missing values, and visualizing the missing data shows that age is a significant feature with gaps. We need to address this issue to avoid biasing the model.\n- By plotting 'age' against 'who', we see distinct distributions: children tend to be younger, while men and women have overlapping but distinct age ranges.\n- A potential strategy is to impute missing age values based on the 'who' category, filling in likely ages for children, women, and men based on these distributions.\n- This could provide a more accurate imputation than using the overall mean or median, especially in the case of children who are expected to have significantly lower ages.\n\n### Remove NaNs\nImputing age could be a good stratgey here. For simplicity, we will just remove the rows where age has any NaNs, but keep in mind that this effectively tosses out ~20% of the data/information we have. In a real-world scenario, imputing is worth testing out.\n\n::: {#uK9r9VCi6_MV .cell outputId='e9e2e9e7-a22b-4c81-9f34-214a7c294de4' execution_count=14}\n``` {.python .cell-code}\n# Drop all rows containing NaN values\nprint(df.shape)\ndf_clean = df.dropna()\ndf_clean.shape\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(891, 11)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\n(712, 11)\n```\n:::\n:::\n\n\n::: {#omXn0W2i-L5A .cell outputId='963993a1-ba63-4073-f300-7059bb583e41' execution_count=15}\n``` {.python .cell-code}\n# Count the number of NaN values in each column\nprint(\"Number of NaNs per column:\")\nprint(df_clean.isna().sum())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of NaNs per column:\nsurvived      0\npclass        0\nsex           0\nage           0\nsibsp         0\nparch         0\nfare          0\nembarked      0\nwho           0\nadult_male    0\nalone         0\ndtype: int64\n```\n:::\n:::\n\n\n## Step 4: Identifying Outliers (Across Multiple Features)\nOutliers can distort model performance and influence the relationships between features. We will use boxplots to identify outliers across multiple numerical columns, including 'age', 'fare', 'sibsp', and 'parch'.\n\n### Why Look for Outliers?\n- **Age**: Extreme values (e.g., very young or old passengers) might influence survival predictions.\n- **Fare**: We've already identified skewness in the fare data, and high fares could represent wealthy individuals who had better chances of survival.\n\n::: {#Xj6XmP1UJ3zr .cell outputId='4f036c94-ef62-4b3d-ba07-c44e36ed3136' execution_count=16}\n``` {.python .cell-code}\nimport seaborn as sns\n# Create boxplots for multiple numerical features to check for outliers\nnumerical_cols = ['age', 'fare']\n\nfor col in numerical_cols:\n    sns.boxplot(x=df_clean[col])\n    plt.title(f\"Boxplot of {col.capitalize()}\")\n    plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Titanic-Dataset_files/figure-html/cell-17-output-1.png){width=545 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](Titanic-Dataset_files/figure-html/cell-17-output-2.png){width=545 height=449}\n:::\n:::\n\n\n### Insights\n- **Age**: Most passengers fall within a reasonable range, but there are a few extreme values for older passengers. These could be outliers that might need special attention during modeling.\n- **Fare**: The boxplot confirms the presence of outliers, with several passengers paying significantly more than the majority.\n\n## Step 5.1: Probability Density Plot for Fare\nIn addition to the boxplot for identifying outliers, we can draw a probability density plot (PDF) to visualize the overall distribution of the 'fare' column.\n\nThis plot will show the likelihood of different fare values occurring, highlighting any skewness or concentration of values in certain ranges.\n\n::: {#6o76wqE5LJxb .cell outputId='cc39e8ba-618a-46bb-eacd-7cd69e0c35f3' execution_count=17}\n``` {.python .cell-code}\n# Plot the probability density plot (PDF) for fare\nsns.kdeplot(df_clean['fare'].dropna(), fill=True)\nplt.title(\"Probability Density Plot for Fare\")\nplt.xlabel(\"Fare\")\nplt.ylabel(\"Density\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Titanic-Dataset_files/figure-html/cell-18-output-1.png){width=614 height=449}\n:::\n:::\n\n\n### Insights\n- The PDF for the fare column shows a strong right skew, with most passengers paying lower fares, but a few passengers paying significantly higher fares.\n- This confirms the presence of outliers at the higher end, which we also observed in the boxplot.\n- The density plot helps visualize how fares are concentrated in lower ranges and taper off gradually toward the higher end.\n\n## Step 5.2: Log Scaling for Fare\nTo deal with the strong skewness and outliers in the 'fare' column, we can apply a log transformation. This will compress the range of the fare values, reducing the influence of extreme outliers while keeping the relative differences intact.\n\nLog scaling is particularly useful for highly skewed distributions, making them more normal-like and easier for models to handle.\n\n::: {#EmY3COTGLs77 .cell outputId='05ce6b1b-f3c7-403e-e11a-242b4f3aa8c1' execution_count=18}\n``` {.python .cell-code}\nimport numpy as np\n# Apply log scaling to the fare column (adding 1 to avoid log(0))\ndf_clean.loc[:, 'log_fare'] = df_clean['fare'].apply(lambda x: np.log(x + 1))\n\n# Plot the PDF for the log-transformed fare column\nsns.kdeplot(df_clean['log_fare'].dropna(), fill=True)\nplt.title(\"Probability Density Plot for Log-Scaled Fare\")\nplt.xlabel(\"Log(Fare + 1)\")\nplt.ylabel(\"Density\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/tmp/ipykernel_2839/3551019692.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_clean.loc[:, 'log_fare'] = df_clean['fare'].apply(lambda x: np.log(x + 1))\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Titanic-Dataset_files/figure-html/cell-19-output-2.png){width=589 height=449}\n:::\n:::\n\n\n## Step 6: Exploring Correlations\nNext, we'll check for correlations between numerical features. This helps us see whether some features are strongly related and could introduce multicollinearity.\n\nFor example:\n- Correlations close to 1 or -1 indicate a strong relationship between features.\n- Correlations close to 0 indicate little to no linear relationship between features.\n\n\n## Step 6.1: Encode categoriecal data as numeric\nEncoding the categorical data will allow us to measure correlations across different levels of our categorical variables. Encoded data is also needed for the modeling step. After encoding, you may want to visit some of the previous steps in this notebook to ensure there aren't any problems with the encoded version of the data. Some people like to encode right after loading their data, but this can make the data unnecessarily complicated while we do some basic browsing of the data (e.g., check for redundnat columns, check for NaNs, check data types, etc.)\n\nCode explanation:\n\n* pd.get_dummies(df, drop_first=True): This one-hot encodes all categorical columns, converting them into binary columns. The drop_first=True argument prevents multicollinearity by removing one of the categories in each column (since they are mutually exclusive).\n\n::: {#lzdLJD_4ClnT .cell outputId='4c3c907a-12e7-4a02-d884-9b3d4e5b621e' execution_count=19}\n``` {.python .cell-code}\n# One-hot encode the categorical columns in the dataset\ndf_encoded = pd.get_dummies(df, drop_first=True)\ndf_encoded.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>survived</th>\n      <th>pclass</th>\n      <th>age</th>\n      <th>sibsp</th>\n      <th>parch</th>\n      <th>fare</th>\n      <th>adult_male</th>\n      <th>alone</th>\n      <th>sex_male</th>\n      <th>embarked_Q</th>\n      <th>embarked_S</th>\n      <th>who_man</th>\n      <th>who_woman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>3</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7.2500</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>71.2833</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>3</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.9250</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>53.1000</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>3</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.0500</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nCode explanation.\n\n* corr_matrix_encoded = df_encoded.corr(): This computes the correlation matrix for both the numerical and newly one-hot encoded features.\n* Seaborn heatmap: The heatmap will visualize correlations across all features, both numerical and categorical (now encoded).\n\n::: {#1_tWTPd_-yrm .cell outputId='24739113-3d11-4883-8614-47db4bd8d8aa' execution_count=20}\n``` {.python .cell-code}\n# Calculate the correlation matrix for all features (including one-hot encoded)\ncorr_matrix_encoded = df_encoded.corr()\n\n# Plot the heatmap of the correlation matrix\nplt.figure(figsize=(16, 12))\nsns.heatmap(corr_matrix_encoded, annot=False, cmap='coolwarm')\nplt.title(\"Correlation Matrix (Numerical and One-Hot Encoded Features)\")\nplt.show()\ncorr_matrix_encoded\n```\n\n::: {.cell-output .cell-output-display}\n![](Titanic-Dataset_files/figure-html/cell-21-output-1.png){width=1219 height=1020}\n:::\n\n::: {.cell-output .cell-output-display execution_count=20}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>survived</th>\n      <th>pclass</th>\n      <th>age</th>\n      <th>sibsp</th>\n      <th>parch</th>\n      <th>fare</th>\n      <th>adult_male</th>\n      <th>alone</th>\n      <th>sex_male</th>\n      <th>embarked_Q</th>\n      <th>embarked_S</th>\n      <th>who_man</th>\n      <th>who_woman</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>survived</th>\n      <td>1.000000</td>\n      <td>-0.338481</td>\n      <td>-0.077221</td>\n      <td>-0.035322</td>\n      <td>0.081629</td>\n      <td>0.257307</td>\n      <td>-0.557080</td>\n      <td>-0.203367</td>\n      <td>-0.543351</td>\n      <td>0.003650</td>\n      <td>-0.155660</td>\n      <td>-0.557080</td>\n      <td>0.506562</td>\n    </tr>\n    <tr>\n      <th>pclass</th>\n      <td>-0.338481</td>\n      <td>1.000000</td>\n      <td>-0.369226</td>\n      <td>0.083081</td>\n      <td>0.018443</td>\n      <td>-0.549500</td>\n      <td>0.094035</td>\n      <td>0.135207</td>\n      <td>0.131900</td>\n      <td>0.221009</td>\n      <td>0.081720</td>\n      <td>0.094035</td>\n      <td>-0.177049</td>\n    </tr>\n    <tr>\n      <th>age</th>\n      <td>-0.077221</td>\n      <td>-0.369226</td>\n      <td>1.000000</td>\n      <td>-0.308247</td>\n      <td>-0.189119</td>\n      <td>0.096067</td>\n      <td>0.280328</td>\n      <td>0.198270</td>\n      <td>0.093254</td>\n      <td>-0.022405</td>\n      <td>-0.032523</td>\n      <td>0.280328</td>\n      <td>0.105081</td>\n    </tr>\n    <tr>\n      <th>sibsp</th>\n      <td>-0.035322</td>\n      <td>0.083081</td>\n      <td>-0.308247</td>\n      <td>1.000000</td>\n      <td>0.414838</td>\n      <td>0.159651</td>\n      <td>-0.253586</td>\n      <td>-0.584471</td>\n      <td>-0.114631</td>\n      <td>-0.026354</td>\n      <td>0.070941</td>\n      <td>-0.253586</td>\n      <td>0.047071</td>\n    </tr>\n    <tr>\n      <th>parch</th>\n      <td>0.081629</td>\n      <td>0.018443</td>\n      <td>-0.189119</td>\n      <td>0.414838</td>\n      <td>1.000000</td>\n      <td>0.216225</td>\n      <td>-0.349943</td>\n      <td>-0.583398</td>\n      <td>-0.245489</td>\n      <td>-0.081228</td>\n      <td>0.063036</td>\n      <td>-0.349943</td>\n      <td>0.150167</td>\n    </tr>\n    <tr>\n      <th>fare</th>\n      <td>0.257307</td>\n      <td>-0.549500</td>\n      <td>0.096067</td>\n      <td>0.159651</td>\n      <td>0.216225</td>\n      <td>1.000000</td>\n      <td>-0.182024</td>\n      <td>-0.271832</td>\n      <td>-0.182333</td>\n      <td>-0.117216</td>\n      <td>-0.166603</td>\n      <td>-0.182024</td>\n      <td>0.191243</td>\n    </tr>\n    <tr>\n      <th>adult_male</th>\n      <td>-0.557080</td>\n      <td>0.094035</td>\n      <td>0.280328</td>\n      <td>-0.253586</td>\n      <td>-0.349943</td>\n      <td>-0.182024</td>\n      <td>1.000000</td>\n      <td>0.404744</td>\n      <td>0.908578</td>\n      <td>-0.076789</td>\n      <td>0.112035</td>\n      <td>1.000000</td>\n      <td>-0.814281</td>\n    </tr>\n    <tr>\n      <th>alone</th>\n      <td>-0.203367</td>\n      <td>0.135207</td>\n      <td>0.198270</td>\n      <td>-0.584471</td>\n      <td>-0.583398</td>\n      <td>-0.271832</td>\n      <td>0.404744</td>\n      <td>1.000000</td>\n      <td>0.303646</td>\n      <td>0.086464</td>\n      <td>0.024929</td>\n      <td>0.404744</td>\n      <td>-0.211036</td>\n    </tr>\n    <tr>\n      <th>sex_male</th>\n      <td>-0.543351</td>\n      <td>0.131900</td>\n      <td>0.093254</td>\n      <td>-0.114631</td>\n      <td>-0.245489</td>\n      <td>-0.182333</td>\n      <td>0.908578</td>\n      <td>0.303646</td>\n      <td>1.000000</td>\n      <td>-0.074115</td>\n      <td>0.125722</td>\n      <td>0.908578</td>\n      <td>-0.896214</td>\n    </tr>\n    <tr>\n      <th>embarked_Q</th>\n      <td>0.003650</td>\n      <td>0.221009</td>\n      <td>-0.022405</td>\n      <td>-0.026354</td>\n      <td>-0.081228</td>\n      <td>-0.117216</td>\n      <td>-0.076789</td>\n      <td>0.086464</td>\n      <td>-0.074115</td>\n      <td>1.000000</td>\n      <td>-0.496624</td>\n      <td>-0.076789</td>\n      <td>0.100544</td>\n    </tr>\n    <tr>\n      <th>embarked_S</th>\n      <td>-0.155660</td>\n      <td>0.081720</td>\n      <td>-0.032523</td>\n      <td>0.070941</td>\n      <td>0.063036</td>\n      <td>-0.166603</td>\n      <td>0.112035</td>\n      <td>0.024929</td>\n      <td>0.125722</td>\n      <td>-0.496624</td>\n      <td>1.000000</td>\n      <td>0.112035</td>\n      <td>-0.119217</td>\n    </tr>\n    <tr>\n      <th>who_man</th>\n      <td>-0.557080</td>\n      <td>0.094035</td>\n      <td>0.280328</td>\n      <td>-0.253586</td>\n      <td>-0.349943</td>\n      <td>-0.182024</td>\n      <td>1.000000</td>\n      <td>0.404744</td>\n      <td>0.908578</td>\n      <td>-0.076789</td>\n      <td>0.112035</td>\n      <td>1.000000</td>\n      <td>-0.814281</td>\n    </tr>\n    <tr>\n      <th>who_woman</th>\n      <td>0.506562</td>\n      <td>-0.177049</td>\n      <td>0.105081</td>\n      <td>0.047071</td>\n      <td>0.150167</td>\n      <td>0.191243</td>\n      <td>-0.814281</td>\n      <td>-0.211036</td>\n      <td>-0.896214</td>\n      <td>0.100544</td>\n      <td>-0.119217</td>\n      <td>-0.814281</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### Insights\n- There is a strong correlation between 'fare' and 'pclass', which makes sense since higher-class tickets typically have higher fares.\n- 'SibSp' and 'Parch' have a weak positive correlation, indicating that larger families might be traveling together.\n- There arent many strong correlations with 'survived', suggesting that more advanced feature engineering might be needed to improve model performance.\n- who_man and adult_male are 100% correlated. We can remove one of these columns.\n\n**Pro-tip**: The variance-inflation factor score can also be very helpful for assessing correlation. This measure looks at how well you can predict a given predictor (y) using all other predictors (X) as input variables to a linear regression model. The nice thing about it is that it gives you a different score for each predictor, which can be helpful when deciding which problematic features to remove.\n\n::: {#qTvqsjJ6HYGa .cell execution_count=21}\n``` {.python .cell-code}\ndf_encoded.drop('who_man',axis=1,inplace=True)\n```\n:::\n\n\n## Step 6.1: Pairplot for Visualizing Pairwise Relationships\nWell use Seaborn's pairplot to visualize pairwise relationships between the numerical features, colored by whether the passenger survived.\n\nThis can help us identify any patterns or clusters that may inform our modeling decisions.\n\n::: {#Egyidy8eMmDG .cell outputId='008c9a0c-728c-44e7-e097-aed3ec3b310c' execution_count=22}\n``` {.python .cell-code}\n# Pairplot to explore relationships between features, colored by 'survived'\nsns.pairplot(df_encoded, hue='survived', diag_kind='kde')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Titanic-Dataset_files/figure-html/cell-23-output-1.png){width=2686 height=2593}\n:::\n:::\n\n\n### Insights\n- The pairplot shows some separation between survivors and non-survivors for certain features, such as 'pclass' and 'adult_male'\n- The pairwise relationships between 'age', 'fare', and other numerical features are not perfectly linear, suggesting that non-linear models might perform better.\n- Visualizing these relationships helps in identifying where additional feature engineering may be required to boost model performance.\n\n## Conclusion: Next Steps for Modeling and Iterative EDA\n\nNow that we've explored the Titanic dataset through extensive EDA, we've gained valuable insights that can guide our next steps in the modeling process. However, it's important to remember that EDA is not a one-time processit's iterative and should continue as new patterns or issues arise during modeling.\n\n### Key Takeaways:\n1. **Feature Engineering**:\n   - We've identified that the 'fare' and 'pclass' columns are strongly correlated, suggesting that we might combine or transform these features for better model performance.\n   - Log scaling 'fare' has helped reduce skewness, making this feature more suitable for modeling. We can apply similar transformations to other skewed features as necessary.\n   - Features like 'who' and 'age' might benefit from imputation or interaction terms to capture deeper relationships with survival outcomes.\n\n2. **Handling Missing Data**:\n   - 'Age' and 'deck' have substantial missing values. Imputing missing values based on insights from other features (e.g., using 'who' to impute 'age') could improve model robustness. Alternatively, we could explore more advanced techniques like multiple imputation or train models that handle missing data natively.\n   \n3. **Addressing Outliers**:\n   - The high outliers in 'fare' present a potential challenge for models like linear regression. In addition to log scaling, other techniques such as robust models or trimming/capping the extreme values could be useful.\n\n4. **Model Selection**:\n   - With weak correlations between 'survived' and other numerical features, we may need to consider more complex, non-linear models like random forests, gradient boosting, or even deep learning methods that can capture non-linear patterns and interactions.\n   - The insights from the pairplot suggest that non-linear relationships might exist between certain features, making tree-based models or ensemble methods a promising direction.\n\n5. **Iterative EDA**:\n   - EDA doesn't end here. As we start building models, we may encounter unexpected patterns or issues (e.g., poor model performance on certain subgroups, overfitting due to outliers). This will prompt us to revisit the EDA, iterating on feature engineering, transforming variables, or handling missing data more effectively.\n   - Evaluating model performance through techniques like cross-validation will provide additional insights, leading to further refinements in both data preprocessing and feature selection.\n\n### Inspirational Next Steps:\n- **Begin Modeling**: Start by testing simple models (e.g., logistic regression) with the current feature set to get a baseline understanding of model performance. Use these models as a foundation for experimenting with more advanced methods.\n- **Keep Exploring**: Stay curious and open to revisiting your EDA. As you iterate through feature engineering and model development, new questions will arise that could lead to even deeper insights.\n- **Experiment**: Try different combinations of features, scaling techniques, and models. Use the insights from the EDA to inform these decisions but be prepared to experiment and validate your assumptions.\n- **Iterate and Improve**: Each iteration of modeling and EDA will bring you closer to a robust solution. Keep refining your approach as new patterns emerge and as your understanding of the dataset deepens.\n\nRemember, successful data science projects are not linearthey involve constant refinement, exploration, and learning. Keep iterating, keep questioning, and keep improving!\n\nGood luck on your journey from EDA to building powerful predictive models!\n\n",
    "supporting": [
      "Titanic-Dataset_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}