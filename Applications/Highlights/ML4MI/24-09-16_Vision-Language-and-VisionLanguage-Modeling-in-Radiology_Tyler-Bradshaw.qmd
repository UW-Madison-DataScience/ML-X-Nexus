---
title: "Vision, Language, and Vision-Language Modeling in Radiology"
date: 2024-09-16
author: 
  - "Tyler Bradshaw, PhD"
image: "https://img.youtube.com/vi/milM6-2RF18/maxresdefault.jpg"
description: "ML4MI Seminar"

language: 
  title-block-author-single: "Presenter"
  title-block-author-plural: "Presenters"
  title-block-published: "Date"
categories:
  - ML4MI
  - Medical imaging
  - VLM
  - ViT
  - UNET
  - Computer vision
  - CNN
  - LLM
  - Deep learning
  - Multimodal learning
---
### About this resource
In this talk from the Machine Learning for Medical Imaging (ML4MI) community, Tyler Bradshaw (PhD) discusses the historical context (e.g., CNN, VGG) leading up to the new era of multimodal learning (e.g., vision-language models), and explores how these models are currently being leveraged in the radiology field. Some key points include:

- Vision Timeline: CNN (vision, ~1989-2018) -> UNET (segmentation, 2015) -> Transformer -> Vision transformer
  - place convnext somewhere in here, plz, chatGPT
  - include the segment anything model
- CNNs are still competitive with more modern (e.g., Vision-transformers or ViTs) and larger models. ViTs tend to only do better than CNNs when you have a massively large dataset. For midsized datasets (e.g., most medical datasets), CNNs may still hold the edge.
- More marginal gains with these models as time goes on
- The future of vision models may include foundation models, which are very large models trained on very large and diverse datasets. With these models, the goal is for the models to acquire world knowledge across a variety of domains; such that the model can perform downstream tasks better than a model specificly trained on such a downstream task. This is an active area of research, and how "foundational" these models truly are remains to be seen.
- In a medical context, we can better predict the presence of tumor cells from  PET images by incorporating PET images from both before/after patient receives treatment. This temporal timeline act as support in the model's prediciton of tumor cells when there is low tumor contrast (e.g., after receiving treatment). The temporal aspect of this data is well-suited to a transformer. Summary sentence: ViTs allow you to model longitudinal effects.

- Language Timeline: TF-IDF -> Word2Vec & GloVe, 2013 -> Transformer, 2017 -> Bert, 2018 -> GPT
- Transformers and language models, in general, form embeddings that represent words/tokens. Words of similar meanings will be grouped together closely in this embedding space. 
- These embeddings are contextually aware: words that have different meanings in different contexts can be encoded using the self-attention mechanism, which...
- LLMs can be used for report summarization (e.g., compared to physicians)

**Video (requires UW netID)**: This talk is made available through UW-Madison's Kaltura Mediaspace and requires a NetID to view: [View 24-09-16 ML4MI Recording](https://mediaspace.wisc.edu/media/Vision%2C+Language%2C+and+Vision-Language+Modeling+in+Radiology+-+Tyler+Bradshaw+-+Sep+2024/1_metec4s2/355339002).

## See also
- [**Model**: UNET](https://uw-madison-datascience.github.io/ML-X-Nexus/Learn/Workshops/Intro-Deeplearning_PyTorch.html): Learn more about the UNET model.
