---
title: "World Knowledge in the Time of Large Models"
date: 2023-11-22
author: 
  - name: "Kenneth Marino, PhD"
    affiliation: "Research Scientist, Google DeepMind"

image: "https://vumbnail.com/891935467.jpg"
description: "This talk will discuss the massive shift that has come about in the vision and ML community as a result of the large pre-trained language and language and vision models such as Flamingo, GPT-4, and other models."

language: 
  title-block-author-single: "Presenter"
  title-block-author-plural: "Presenters"
  title-block-published: "Date"
categories:
  - Videos
  - SILO
  - VLM
  - LLM
  - LMM
  - Multimodal learning
  - Foundation models

---
### Summary from [SILO website](https://silo.wisc.edu/talk/11222023/?wcs_timestamp=1700656200)

> **Bio**: Kenneth Marino is Research Scientist at Google DeepMind in NYC, focusing on improving knowledge-based systems such as retrieval and information extraction as well as embodied reasoning with language. He graduated in 2021 from Carnegie Mellon University advised by Abhinav Gupta, where his thesis focused on incorporating knowledge into embodied systems. He has an adjunct appointment at Columbia University where he teaches a class focused on the impact of datasets on machine learning and how to collect good datasets. He received his undergraduate degree from the Georgia Institute of Technology where he studied Computer Engineering and Computer Science.

> **Abstract**: This talk will discuss the massive shift that has come about in the vision and ML community as a result of the large pre-trained language and language and vision models such as Flamingo, GPT-4, and other models. We begin by looking at the work on knowledge-based systems in CV and robotics before the large model revolution and discuss the impact it had. This impact can be broken down into three areas in which world knowledge should be studied in the context of these new models: evaluation, harnessing large models, and building outside knowledge. First, evaluating world knowledge is even more important as the large model revolution gives more easy access to world knowledge. Next, we discuss recent work in harnessing models such as Flamingo and Chinchilla for visual and procedural knowledge. Finally, the talk discusses how, by focusing on knowledge acquisition as an agent-centric problem, we can make developments in retrieving and collecting world knowledge.

### Links
- About the Speaker → [kennethmarino.com](https://kennethmarino.com/)
- OK-VQA paper and dataset → [okvqa.allenai.org/index.html](https://okvqa.allenai.org/index.html)
- KRISP paper → [arxiv.org/abs/2012.11014](https://arxiv.org/abs/2012.11014)
- Same Object, Different Grasps paper → [arxiv.org/abs/2011.06431](https://arxiv.org/abs/2011.06431)
- A-OKVQA dataset/GitHub → [github.com/allenai/aokvqa](https://github.com/allenai/aokvqa)
- Distilling Internet-Scale Vision-Language Models into Embodied Agents paper → [arxiv.org/abs/2301.12507](https://arxiv.org/abs/2301.12507)

{{< video https://vimeo.com/891935467 >}}

### Jump to section

- <a href="https://vimeo.com/891935467#t=0s" target="_blank">[0:00] Introducing Kenneth Marino</a>
- <a href="https://vimeo.com/891935467#t=71s" target="_blank">[1:11] Begin presentation</a>
- <a href="https://vimeo.com/891935467#t=97s" target="_blank">[1:37] What do we want from AI?</a>
- <a href="https://vimeo.com/891935467#t=180s" target="_blank">[3:00] The old way: treating all tasks individually</a>
- <a href="https://vimeo.com/891935467#t=230s" target="_blank">[3:50] Knowledge / priors matter</a>
- <a href="https://vimeo.com/891935467#t=300s" target="_blank">[5:00] LMs have built-in knowledge</a>
- <a href="https://vimeo.com/891935467#t=434s" target="_blank">[7:14] Prologue: Before the LLM/VLM revolution</a>
- <a href="https://vimeo.com/891935467#t=495s" target="_blank">[8:15] Evaluating knowledge</a>
- <a href="https://vimeo.com/891935467#t=570s" target="_blank">[9:30] Evaluating knowledge with OK-VQA</a>
- <a href="https://vimeo.com/891935467#t=630s" target="_blank">[10:30] KRISP: Incorporating knowledge graphs</a>
- <a href="https://vimeo.com/891935467#t=675s" target="_blank">[11:15] LLMs and VLMs: Accessible world knowledge</a>
- <a href="https://vimeo.com/891935467#t=808s" target="_blank">[13:28] Evaluating knowledge in LLMs/VLMs</a>
- <a href="https://vimeo.com/891935467#t=880s" target="_blank">[14:40] Many kinds of knowledge</a>
- <a href="https://vimeo.com/891935467#t=977s" target="_blank">[16:17] Evaluating knowledge with A-OKVQA</a>
- <a href="https://vimeo.com/891935467#t=1358s" target="_blank">[22:38] From evaluating to using LLMs/VLMs</a>
- <a href="https://vimeo.com/891935467#t=1531s" target="_blank">[25:31] Extracting knoweldeg from LLMs</a>
- <a href="https://vimeo.com/891935467#t=1575s" target="_blank">[26:15] Bringing Flamingo's knowledge into agents</a>
- <a href="https://vimeo.com/891935467#t=2148s" target="_blank">[35:48] The only constant is change</a>
- <a href="https://vimeo.com/891935467#t=2205s" target="_blank">[36:45] Inquisitive agents</a>
- <a href="https://vimeo.com/891935467#t=2411s" target="_blank">[40:11] Wikipedia navigation as a benchmark</a>
- <a href="https://vimeo.com/891935467#t=3180s" target="_blank">[53:00] Takeaways</a>
- <a href="https://vimeo.com/891935467#t=3235s" target="_blank">[53:55] Q&A</a>







