---
title: "AI's Environmental Footprint: Insights and Actions"
date: 2025-09-09
author:
  - "Chris Endemann"
  - "Nidhal Jegham"
image: "https://img.youtube.com/vi/2dCQS1jAbUo/maxresdefault.jpg"

language:
  title-block-author-single: "Presenter"
  title-block-author-plural: "Presenters"
  title-block-published: "Date"

categories:
  - Videos
  - ML+X
  - UW-Madison
  - Ethical AI
  - Trustworthy AI
  - Sustainability
  - Energy
  - Benchmarking
  - LLM
  - RAG
  - Retrieval
  - Cloud
  - GPU
---

## About this resource
This forum explores how ML/AI practitioners can measure and reduce the environmental costs of AI. It pairs two complementary efforts: one that retrieves emissions and cost data from sustainability reports using RAG, and another that benchmarks energy, water, and carbon footprints across large language models.

#### WattBot: Estimating AI Emissions and Costs with RAG — Chris Endemann [02:24](https://www.youtube.com/watch?v=2dCQS1jAbUo&t=144s){target=_blank}  
Chris introduces WattBot, a Kaggle challenge and retrieval-augmented generation (RAG) framework for estimating AI emissions and compute costs. Using 35+ papers and 300+ curated Q&A pairs, teams build systems that return citation-backed answers or explicitly state when evidence is missing—promoting transparency and reproducibility in sustainability reporting.  

- **Kaggle challenge**: [kaggle.com/competitions/WattBot2025/overview](https://www.kaggle.com/competitions/WattBot2025/overview)

#### How Hungry is AI? Benchmarking Energy, Water, and Carbon Footprint of LLM Inference — Nidhal Jegham [09:07](https://www.youtube.com/watch?v=2dCQS1jAbUo&t=547s){target=_blank}  
Nidhal presents a reproducible framework to estimate per-request energy, water, and carbon use for open and proprietary LLMs. The method combines hardware assumptions (A100–H200 GPUs), data center multipliers (PUE, WUE, CIF), and a DEA-style efficiency score that balances model accuracy against environmental cost.  

- **Preprint**: [https://arxiv.org/abs/2505.09598](https://arxiv.org/abs/2505.09598)  
- **Dashboard**: [https://app.powerbi.com/view?r=eyJr9](https://app.powerbi.com/view?r=eyJrIjoiZjVmOTI0MmMtY2U2Mi00ZTE2LTk2MGYtY2ZjNDMzODZkMjlmIiwidCI6IjQyNmQyYThkLTljY2QtNDI1NS04OTNkLTA2ODZhMzJjMTY4ZCIsImMiOjF9)

{{< video https://www.youtube.com/watch?v=2dCQS1jAbUo&t >}}

### Key points
- Data center efficiency and GPU generation (A100–H200) drive impact as much as model size.  
- Environmental multipliers like PUE (Power Usage Effectiveness) and WUE (Water Usage Effectiveness) are critical to cross-site comparisons.
- Efficiency is not absolute: the Jevons paradox applies—lower per-query cost can increase overall usage.  
- U.S. regulation remains minimal, making voluntary transparency efforts (like Mistral's) especially important.  
- Renewable energy sourcing and liquid cooling are among the most actionable interventions.  
- Academic and industry collaborations can close data gaps through open benchmarking.
- Aggregate usage, not single-query cost, drives total environmental footprint.  
- Reporting environmental impact alongside accuracy metrics is an emerging best practice.  

## See also  
- [**Notebook**: Exploring RAG with Romeo and Juliet](https://uw-madison-datascience.github.io/ML-X-Nexus/Learn/Notebooks/2025-05-07_RAG-Romeo-Juliet.html): Learn how to build an end-to-end retrieval augmented generation (RAG) pipeline using Shakespeare's Romeo and Juliet as example text.
- [**Video Archive**: ML+X forum archive](https://uw-madison-datascience.github.io/ML-X-Nexus/Applications/Videos/Forums/): Check out other recorded forums from ML+X.
- [Machine Learning Marathon](https://ml-marathon.wisc.edu/): Learn about the annual Machine Learning Marathon (3-month AI/ML hackathon) hosted by ML+X each fall.
