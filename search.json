[
  {
    "objectID": "Toolbox/GenAI/index.html",
    "href": "Toolbox/GenAI/index.html",
    "title": "GenAI",
    "section": "",
    "text": "Explore genAI tools that you can leverage for your next AI/ML project.\nUW–Madison faculty, staff, students, and affiliates are required to follow campus policies relevant to AI use. Uses of generative AI that are explicitly prohibited by policy include, but are not limited to, the following:\n\nEntering any sensitive, restricted or otherwise protected institutional data – including hard-coded passwords – into any generative AI tool or service;\nUsing AI-generated code for institutional IT systems or services without review by a human to verify the absence of malicious elements;\nUsing generative AI to violate laws; institutional policies, rules or guidelines; or agreements or contracts.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUW Generative AI Services & Policies\n\n\n\nGenAI\n\nMicrosoft Copilot\n\nGemini\n\nWebex\n\nZoom\n\nUW-Madison\n\nLLM\n\nFoundation models\n\n\n\n\n\n\n\nChris Endemann\n\n\n2025-01-07\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotebookLM: A GenAI Summarization Tool\n\n\n\nGenAI\n\nNLP\n\nSummarization\n\nGemini\n\nLLM\n\nFoundation models\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-12-09\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Toolbox",
      "GenAI"
    ]
  },
  {
    "objectID": "Toolbox/GenAI/GenAI-at-UW-Madison.html",
    "href": "Toolbox/GenAI/GenAI-at-UW-Madison.html",
    "title": "UW Generative AI Services & Policies",
    "section": "",
    "text": "The University of Wisconsin–‍Madison is committed to responsibly harnessing the power of generative artificial intelligence to enhance teaching, learning, research and university operations. The Division of Information Technology (DoIT) oversees university-wide AI initiatives focused on providing secure enterprise AI tools. These tools are available university-wide for free and provide higher data security and privacy protection than public services. Please consider these options before exploring unvetted generative AI services for university work.\nTo learn more about vetted tools and genAI use policies, visit the Generative AI Services at UW-Madison webpage. The current list of free supported tools include:\n\nGoogle Gemini: A conversational AI chatbot for text generation, coding, and image creation.\nWebex AI Assistant: A meeting tool with real-time translation, transcription, and voice commands.\nMicrosoft Copilot: A digital assistant integrated with Microsoft 365 for creative tasks and problem-solving.\nZoom AI Companion: Summarizes meetings and identifies actionable items.\n\nIn addition, the following tools are availble on a pay-as-you-go basis:\n\nAmazon Web Services (AWS) Bedrock: Amazon Bedrock is a cloud service that makes it easy to build AI applications using foundation models from leading AI companies. It provides a single way to access and use these models and tools to customize them with public or internal data and create AI assistants that can interact with university systems and information.\nGoogle Cloud Platform AI Services: Google Cloud Platform (GCP) offers cloud computing resources and access to more than 90 foundation models, such as Claude from Anthropic and Mixtral from Mistral AI, for processing and storing public or internal data. GCP is available to faculty and staff but requires Cybersecurity evaluation for uses involving sensitive or restricted data.\nMicrosoft Azure AI Services: UW–‍Madison’s Azure service offers cloud computing resources, including access to OpenAI’s API, for processing and storing public or internal data. It’s available to faculty and staff but requires Cybersecurity evaluation for uses involving sensitive or restricted data.\n\n\n\nWhile some of the enterprise tools can be used with “internal data”, most genAI tools cannot be used for senstive/restriced data. Review the FAQ section to learn which tools are appropriate for your data. If you have questions about data classification, contact the appropriate data steward. For more information on genAI use policies, visit it.wisc.edu/generative-ai-uw-madison-use-policies/"
  },
  {
    "objectID": "Toolbox/GenAI/GenAI-at-UW-Madison.html#about-this-resource",
    "href": "Toolbox/GenAI/GenAI-at-UW-Madison.html#about-this-resource",
    "title": "UW Generative AI Services & Policies",
    "section": "",
    "text": "The University of Wisconsin–‍Madison is committed to responsibly harnessing the power of generative artificial intelligence to enhance teaching, learning, research and university operations. The Division of Information Technology (DoIT) oversees university-wide AI initiatives focused on providing secure enterprise AI tools. These tools are available university-wide for free and provide higher data security and privacy protection than public services. Please consider these options before exploring unvetted generative AI services for university work.\nTo learn more about vetted tools and genAI use policies, visit the Generative AI Services at UW-Madison webpage. The current list of free supported tools include:\n\nGoogle Gemini: A conversational AI chatbot for text generation, coding, and image creation.\nWebex AI Assistant: A meeting tool with real-time translation, transcription, and voice commands.\nMicrosoft Copilot: A digital assistant integrated with Microsoft 365 for creative tasks and problem-solving.\nZoom AI Companion: Summarizes meetings and identifies actionable items.\n\nIn addition, the following tools are availble on a pay-as-you-go basis:\n\nAmazon Web Services (AWS) Bedrock: Amazon Bedrock is a cloud service that makes it easy to build AI applications using foundation models from leading AI companies. It provides a single way to access and use these models and tools to customize them with public or internal data and create AI assistants that can interact with university systems and information.\nGoogle Cloud Platform AI Services: Google Cloud Platform (GCP) offers cloud computing resources and access to more than 90 foundation models, such as Claude from Anthropic and Mixtral from Mistral AI, for processing and storing public or internal data. GCP is available to faculty and staff but requires Cybersecurity evaluation for uses involving sensitive or restricted data.\nMicrosoft Azure AI Services: UW–‍Madison’s Azure service offers cloud computing resources, including access to OpenAI’s API, for processing and storing public or internal data. It’s available to faculty and staff but requires Cybersecurity evaluation for uses involving sensitive or restricted data.\n\n\n\nWhile some of the enterprise tools can be used with “internal data”, most genAI tools cannot be used for senstive/restriced data. Review the FAQ section to learn which tools are appropriate for your data. If you have questions about data classification, contact the appropriate data steward. For more information on genAI use policies, visit it.wisc.edu/generative-ai-uw-madison-use-policies/"
  },
  {
    "objectID": "Toolbox/Libraries/AI-Fairness-360.html",
    "href": "Toolbox/Libraries/AI-Fairness-360.html",
    "title": "AI Fairness 360 (AIF360)",
    "section": "",
    "text": "AI Fairness 360 (AIF360) is a scikit-learn-compatible open-source Python library designed to detect and mitigate bias in machine learning models. Its compatibility with scikit-learn pipelines allows seamless integration into workflows for tabular data tasks, which is the library’s primary focus. Many of AIF360’s fairness metrics and mitigation algorithms operate directly on model outputs (y_pred and y_true), making it easy to adapt for use with other models, even outside the tabular domain.\nAIF360 is particularly suited for tasks where fairness is a critical concern. Its extensibility and modular design make it a powerful tool for ethical AI development across a range of applications, including preprocessing, in-processing, and post-processing bias mitigation.\n\n\n\nBias detection: Comprehensive metrics to measure fairness at both individual and group levels.\n\nGroup fairness: Metrics like disparate impact ratio and equal opportunity difference.\nIndividual fairness: Measures such as consistency score to evaluate treatment of similar individuals.\nDistributional fairness: Metrics like generalized entropy error to quantify benefit distribution.\n\nBias mitigation algorithms:\n\nPre-processing: Techniques like reweighing and learned fair representations to adjust data before training.\nIn-processing: Models like adversarial debiasing to apply fairness constraints during training.\nPost-processing: Tools like reject option classifier for fair adjustments to model outputs.\n\nDataset integration: Includes commonly used datasets for fairness research (e.g., COMPAS).\nScikit-learn compatibility: Built with scikit-learn conventions, AIF360 integrates easily into standard pipelines, enabling compatibility with a wide range of estimators and transformers.\nModular and extensible design: Easily add new datasets, metrics, or algorithms."
  },
  {
    "objectID": "Toolbox/Libraries/AI-Fairness-360.html#about-this-library",
    "href": "Toolbox/Libraries/AI-Fairness-360.html#about-this-library",
    "title": "AI Fairness 360 (AIF360)",
    "section": "",
    "text": "AI Fairness 360 (AIF360) is a scikit-learn-compatible open-source Python library designed to detect and mitigate bias in machine learning models. Its compatibility with scikit-learn pipelines allows seamless integration into workflows for tabular data tasks, which is the library’s primary focus. Many of AIF360’s fairness metrics and mitigation algorithms operate directly on model outputs (y_pred and y_true), making it easy to adapt for use with other models, even outside the tabular domain.\nAIF360 is particularly suited for tasks where fairness is a critical concern. Its extensibility and modular design make it a powerful tool for ethical AI development across a range of applications, including preprocessing, in-processing, and post-processing bias mitigation.\n\n\n\nBias detection: Comprehensive metrics to measure fairness at both individual and group levels.\n\nGroup fairness: Metrics like disparate impact ratio and equal opportunity difference.\nIndividual fairness: Measures such as consistency score to evaluate treatment of similar individuals.\nDistributional fairness: Metrics like generalized entropy error to quantify benefit distribution.\n\nBias mitigation algorithms:\n\nPre-processing: Techniques like reweighing and learned fair representations to adjust data before training.\nIn-processing: Models like adversarial debiasing to apply fairness constraints during training.\nPost-processing: Tools like reject option classifier for fair adjustments to model outputs.\n\nDataset integration: Includes commonly used datasets for fairness research (e.g., COMPAS).\nScikit-learn compatibility: Built with scikit-learn conventions, AIF360 integrates easily into standard pipelines, enabling compatibility with a wide range of estimators and transformers.\nModular and extensible design: Easily add new datasets, metrics, or algorithms."
  },
  {
    "objectID": "Toolbox/Libraries/AI-Fairness-360.html#integration-and-compatibility",
    "href": "Toolbox/Libraries/AI-Fairness-360.html#integration-and-compatibility",
    "title": "AI Fairness 360 (AIF360)",
    "section": "Integration and compatibility",
    "text": "Integration and compatibility\nAIF360 is fully compatible with scikit-learn pipelines, which explains its primary focus on tabular data. While scikit-learn is less commonly used for image and text tasks, AIF360’s ability to operate directly on y_pred and y_true makes it versatile and easy to extend to other domains, such as computer vision and NLP.\n\nFrameworks supported: Scikit-learn, Python, R\nInstallation instructions:\n\nFor basic installation: pip install aif360\nFor full functionality: pip install 'aif360[all]'\nFor R: install.packages(\"aif360\")"
  },
  {
    "objectID": "Toolbox/Libraries/AI-Fairness-360.html#extending-aif360-for-computer-vision-and-nlp",
    "href": "Toolbox/Libraries/AI-Fairness-360.html#extending-aif360-for-computer-vision-and-nlp",
    "title": "AI Fairness 360 (AIF360)",
    "section": "Extending AIF360 for computer vision and NLP",
    "text": "Extending AIF360 for computer vision and NLP\nAlthough AIF360 is focused on tabular data, its compatibility with y_pred and y_true outputs enables its fairness metrics and algorithms to be applied in other domains:\n\nComputer vision: Convert image-based predictions into tabular formats using model outputs and metadata (e.g., race, gender). This allows AIF360’s fairness metrics and mitigation strategies to evaluate bias in vision tasks, such as object detection.\nNatural language processing (NLP): Similarly, AIF360 can be used to assess fairness in text models by processing structured outputs, such as sentiment scores or embeddings. Combining AIF360 with libraries like FairSeq or Hugging Face Transformers offers powerful solutions for fairness in text applications.\nGeneral extensibility: Since many metrics and algorithms in AIF360 require only y_pred, y_true, and protected attribute data, the library can be adapted to virtually any domain where these inputs are available."
  },
  {
    "objectID": "Toolbox/Libraries/AI-Fairness-360.html#why-use-aif360",
    "href": "Toolbox/Libraries/AI-Fairness-360.html#why-use-aif360",
    "title": "AI Fairness 360 (AIF360)",
    "section": "Why use AIF360?",
    "text": "Why use AIF360?\n\nEthical AI development: Essential for ensuring fairness in AI models, reducing societal bias, and improving trustworthiness.\nScikit-learn compatibility: Makes it easy to integrate fairness assessments and mitigation into existing workflows.\nExtensibility: Flexible design enables adaptation for custom datasets, metrics, and algorithms across domains.\nEducational value: A resourceful tool for teaching fairness concepts in machine learning."
  },
  {
    "objectID": "Toolbox/Libraries/AI-Fairness-360.html#use-cases",
    "href": "Toolbox/Libraries/AI-Fairness-360.html#use-cases",
    "title": "AI Fairness 360 (AIF360)",
    "section": "Use cases",
    "text": "Use cases\n\nHealthcare: Ensuring fairness in predictive models for patient outcomes.\nFinance: Mitigating bias in credit scoring models.\nHiring: Addressing bias in applicant ranking systems.\nCriminal justice: Evaluating bias in risk assessment tools (e.g., COMPAS).\nComputer vision: Adapting fairness metrics for image data (e.g., object detection outcomes and metadata).\nNLP: Assessing fairness in text models using structured metadata or embeddings."
  },
  {
    "objectID": "Toolbox/Libraries/AI-Fairness-360.html#tutorials-and-resources",
    "href": "Toolbox/Libraries/AI-Fairness-360.html#tutorials-and-resources",
    "title": "AI Fairness 360 (AIF360)",
    "section": "Tutorials and resources",
    "text": "Tutorials and resources\n\nThe AIF360 README lists several notebooks and tutorails for getting started.\nThe Trustworthy AI/ML workshop also has an AIF360-focused episode that implements various fairness metrics. Be sure to complete the workshop setup first to follow along with that material."
  },
  {
    "objectID": "Toolbox/Libraries/AI-Fairness-360.html#questions",
    "href": "Toolbox/Libraries/AI-Fairness-360.html#questions",
    "title": "AI Fairness 360 (AIF360)",
    "section": "Questions?",
    "text": "Questions?\nFor any questions, please post to the Nexus Q&A on GitHub."
  },
  {
    "objectID": "Toolbox/Libraries/AI-Fairness-360.html#see-also",
    "href": "Toolbox/Libraries/AI-Fairness-360.html#see-also",
    "title": "AI Fairness 360 (AIF360)",
    "section": "See also",
    "text": "See also\n\nWorkshop: Trustworthy AI: Explainability, Bias, Fairness, and Safety: A beginner-friendly workshop on Trustworthy AI/ML concepts and mitigation tools, including a hands-on tutorial using AIF360, OOD deteciton, and explainability methods.\nTalk: Trustworthy LLMs & Ethical AI: Learn how DeTox can be used to remove toxic embeddings in LLMs."
  },
  {
    "objectID": "Toolbox/Libraries/kornia.html",
    "href": "Toolbox/Libraries/kornia.html",
    "title": "Kornia",
    "section": "",
    "text": "Kornia is a differentiable library that allows classical computer vision to be integrated into deep learning models. Developed by E. Riba, D. Mishkin, D. Ponsa, E. Rublee and G. Bradski and introduced in 2020, it is built on top of PyTorch to offer tools for image processing and transformer-based computer vision models (e.g., SAM, ViT, LoFTR, and RT-DETR). It’s perfect for researchers and practitioners who want GPU-optimized tools for complex vision tasks without sacrificing classic techniques.\n\n\n\nImage augmentation: Standard image augmentation functionalities such as gaussian noise, random flips, and color jiggle.\nFeature detection: Classical computer vision algorithms such hessian blobs, Harris corner detector, and difference of gaussians. Deep learning-based algorithms such as Dexined, KeyNet, DISK, and DeDoDe.\nTransformer models: Optimized transformers-based computer vision models such as SAM, ViT, LoFTR, and RT-DETR\n\nSAM (Segment Anything Model): A state-of-the-art segmentation model from Meta that can segment objects in images without predefined categories using self-supervised learning.\nViT (Vision Transformer): Adapts the transformer architecture, traditionally used in NLP, to process images by dividing them into patches, leading to high performance on large datasets.\nLoFTR (Local Feature Transformer): A model for establishing correspondences between images through transformer-based attention, which is highly effective for tasks like image stitching and 3D reconstruction.\nRT-DETR (Real-Time Detection Transformer): An object detection model optimized for real-time processing, making it suitable for rapid detection tasks like video processing and autonomous navigation."
  },
  {
    "objectID": "Toolbox/Libraries/kornia.html#about-this-library",
    "href": "Toolbox/Libraries/kornia.html#about-this-library",
    "title": "Kornia",
    "section": "",
    "text": "Kornia is a differentiable library that allows classical computer vision to be integrated into deep learning models. Developed by E. Riba, D. Mishkin, D. Ponsa, E. Rublee and G. Bradski and introduced in 2020, it is built on top of PyTorch to offer tools for image processing and transformer-based computer vision models (e.g., SAM, ViT, LoFTR, and RT-DETR). It’s perfect for researchers and practitioners who want GPU-optimized tools for complex vision tasks without sacrificing classic techniques.\n\n\n\nImage augmentation: Standard image augmentation functionalities such as gaussian noise, random flips, and color jiggle.\nFeature detection: Classical computer vision algorithms such hessian blobs, Harris corner detector, and difference of gaussians. Deep learning-based algorithms such as Dexined, KeyNet, DISK, and DeDoDe.\nTransformer models: Optimized transformers-based computer vision models such as SAM, ViT, LoFTR, and RT-DETR\n\nSAM (Segment Anything Model): A state-of-the-art segmentation model from Meta that can segment objects in images without predefined categories using self-supervised learning.\nViT (Vision Transformer): Adapts the transformer architecture, traditionally used in NLP, to process images by dividing them into patches, leading to high performance on large datasets.\nLoFTR (Local Feature Transformer): A model for establishing correspondences between images through transformer-based attention, which is highly effective for tasks like image stitching and 3D reconstruction.\nRT-DETR (Real-Time Detection Transformer): An object detection model optimized for real-time processing, making it suitable for rapid detection tasks like video processing and autonomous navigation."
  },
  {
    "objectID": "Toolbox/Libraries/kornia.html#integration-and-compatibility",
    "href": "Toolbox/Libraries/kornia.html#integration-and-compatibility",
    "title": "Kornia",
    "section": "Integration and compatibility",
    "text": "Integration and compatibility\nKornia is compatible with PyTorch and any libraries built on top of PyTorch. In addition, all features from Kornia can utilize the GPU of the host machine.\n\nFrameworks Supported: PyTorch, PyTorch-Lightning, and Fastai\nInstallation Instructions: pip install kornia"
  },
  {
    "objectID": "Toolbox/Libraries/kornia.html#why-mix-traditional-methods-with-transformers",
    "href": "Toolbox/Libraries/kornia.html#why-mix-traditional-methods-with-transformers",
    "title": "Kornia",
    "section": "Why mix traditional methods with transformers?",
    "text": "Why mix traditional methods with transformers?\nBlending traditional computer vision techniques with transformer-based models offers a balanced approach that can improve model performance, flexibility, and efficiency. More specificically, this allow us to…\n\nLeverage established techniques: Many traditional methods, like edge detection and keypoint matching, have been fine-tuned over decades and are computationally efficient. They often work well as preprocessing steps or for augmenting transformer-based models with reliable, low-complexity features.\nReduce computational costs: Transformers are data- and computation-intensive, especially for complex tasks. By using traditional methods in early or intermediate stages, you can reduce the workload on transformers, enabling real-time processing and lowering hardware requirements.\nImprove generalization: Combining classical methods with transformers can provide models with a broader understanding of images, as they benefit from both deterministic feature engineering (traditional methods) and learned feature representations (transformers). This synergy often enhances the model’s generalization to varied datasets.\nTargeted feature enhancement: Traditional feature detectors can focus on specific areas, like edges or corners, enhancing input data that transformers process. This combination can lead to improved performance on tasks such as image matching, object detection, and segmentation."
  },
  {
    "objectID": "Toolbox/Libraries/kornia.html#use-cases",
    "href": "Toolbox/Libraries/kornia.html#use-cases",
    "title": "Kornia",
    "section": "Use cases",
    "text": "Use cases\nHere are some examples of how Kornia can be applied to different machine learning tasks.\n\nImage preprocessing: Apply the Shi-Tomasi cornerness function to preprocess images for 3D reconstruction.\nImage matching: Apply the KeyNet algorithm to match images from different perspectives/angles."
  },
  {
    "objectID": "Toolbox/Libraries/kornia.html#tutorials-and-resources",
    "href": "Toolbox/Libraries/kornia.html#tutorials-and-resources",
    "title": "Kornia",
    "section": "Tutorials and resources",
    "text": "Tutorials and resources\n\nOfficial tutorials: Link to several tutorials/guides on using their models: kornia.github.io/tutorials/"
  },
  {
    "objectID": "Toolbox/Libraries/kornia.html#questions",
    "href": "Toolbox/Libraries/kornia.html#questions",
    "title": "Kornia",
    "section": "Questions?",
    "text": "Questions?\nIf you have any lingering questions about this library, please post to the Nexus Q&A on GitHub."
  },
  {
    "objectID": "Toolbox/Libraries/kornia.html#see-also",
    "href": "Toolbox/Libraries/kornia.html#see-also",
    "title": "Kornia",
    "section": "See also",
    "text": "See also\n\nML4MI Seminar: Vision, Language, and Vision-Language Modeling in Radiology: In this talk from the Machine Learning for Medical Imaging (ML4MI) community, Tyler Bradshaw (PhD) discusses the historical context (e.g., CNN, VGG) leading up to the new era of multimodal learning (e.g., vision-language models), and explores how these models are currently being leveraged in the radiology field.\nWorkshop: Intro to Deep Learning with PyTorch: Explore the popular PyTorch deep learning framework."
  },
  {
    "objectID": "Toolbox/Libraries/PyTesseract.html",
    "href": "Toolbox/Libraries/PyTesseract.html",
    "title": "Pytesseract: OCR with Tesseract (LSTM) in Python",
    "section": "",
    "text": "Pytesseract is a Python wrapper for Google’s Tesseract Optical Character Recognition (OCR) engine, used for recognizing and extracting text from images. It works on a wide range of image types (e.g., JPEG, PNG, TIFF) and supports over 100 languages, including Chinese, Arabic, and Devanagari.\nTesseract uses a character-level LSTM model and runs entirely on CPU, making it easy to deploy in low-resource environments. While it’s not state-of-the-art for complex layout or scene text, it’s fast, scriptable, and widely supported — ideal for lightweight OCR use cases."
  },
  {
    "objectID": "Toolbox/Libraries/PyTesseract.html#about-this-resource",
    "href": "Toolbox/Libraries/PyTesseract.html#about-this-resource",
    "title": "Pytesseract: OCR with Tesseract (LSTM) in Python",
    "section": "",
    "text": "Pytesseract is a Python wrapper for Google’s Tesseract Optical Character Recognition (OCR) engine, used for recognizing and extracting text from images. It works on a wide range of image types (e.g., JPEG, PNG, TIFF) and supports over 100 languages, including Chinese, Arabic, and Devanagari.\nTesseract uses a character-level LSTM model and runs entirely on CPU, making it easy to deploy in low-resource environments. While it’s not state-of-the-art for complex layout or scene text, it’s fast, scriptable, and widely supported — ideal for lightweight OCR use cases."
  },
  {
    "objectID": "Toolbox/Libraries/PyTesseract.html#key-features",
    "href": "Toolbox/Libraries/PyTesseract.html#key-features",
    "title": "Pytesseract: OCR with Tesseract (LSTM) in Python",
    "section": "Key features",
    "text": "Key features\n\nReads printed text from standard image formats\nWorks with file paths, Pillow/PIL (Python Imaging Library), or OpenCV arrays\nSupports multilingual text recognition\nOutputs plain text, bounding boxes, PDFs, TSV, and XML formats\nFast CPU-based inference with no GPU dependencies"
  },
  {
    "objectID": "Toolbox/Libraries/PyTesseract.html#when-to-use",
    "href": "Toolbox/Libraries/PyTesseract.html#when-to-use",
    "title": "Pytesseract: OCR with Tesseract (LSTM) in Python",
    "section": "When to use",
    "text": "When to use\n\nYou need fast OCR on clean documents or small image batches\nYou want to automate extraction from scanned forms, labels, or tables\nYou’re working in a CPU-only or resource-constrained environment\nYou want a scriptable fallback tool before reaching for ViT-based OCR"
  },
  {
    "objectID": "Toolbox/Libraries/PyTesseract.html#pros-and-limitations",
    "href": "Toolbox/Libraries/PyTesseract.html#pros-and-limitations",
    "title": "Pytesseract: OCR with Tesseract (LSTM) in Python",
    "section": "Pros and limitations",
    "text": "Pros and limitations\n\n\n\n\n\n\n\nPros\nLimitations\n\n\n\n\nEasy to install and use on most systems\nNo GPU acceleration — slower on large datasets\n\n\nMultilingual out of the box\nCannot be fine-tuned or retrained\n\n\nGood for simple forms and documents\nStruggles with complex layouts or visual context\n\n\nCPU-only — works in low-resource environments\nLower accuracy than transformer-based models on cluttered or noisy inputs\n\n\n\nTesseract’s fast CPU performance and no-frills setup make it great for small-scale OCR, but it’s not optimized for high-volume pipelines or scene text recognition."
  },
  {
    "objectID": "Toolbox/Libraries/PyTesseract.html#model-architecture",
    "href": "Toolbox/Libraries/PyTesseract.html#model-architecture",
    "title": "Pytesseract: OCR with Tesseract (LSTM) in Python",
    "section": "Model architecture",
    "text": "Model architecture\nTesseract relies on an LSTM pipeline trained on character-level text. It performs well when the input is clean and straightforward — such as scanned documents or forms — but struggles with visual ambiguity, clutter, or layout-sensitive content.\nFor more robust use cases, newer models like TrOCR, Donut, and PaddleOCR use Vision Transformers (ViTs). PaddleOCR in particular includes both CNN- and transformer-based backends. These models are better suited for tasks where text is visually entangled with surrounding context — like reading overlaid labels on maps or structured forms."
  },
  {
    "objectID": "Toolbox/Libraries/PyTesseract.html#installation-and-usage",
    "href": "Toolbox/Libraries/PyTesseract.html#installation-and-usage",
    "title": "Pytesseract: OCR with Tesseract (LSTM) in Python",
    "section": "Installation and usage",
    "text": "Installation and usage\nTo use pytesseract, you need to install both the Tesseract OCR engine and the Python wrapper.\n\nUbuntu / Debian\nsudo apt update\nsudo apt install tesseract-ocr\npip install pytesseract\n\n\nmacOS\nbrew install tesseract\npip install pytesseract\n\n\nWindows\n\nDownload and install the Tesseract binary from the UB Mannheim builds\n\nNote the install location, typically:\nC:\\Program Files\\Tesseract-OCR\\tesseract.exe\nEither add this location to your system PATH, or set it manually in your script:\n\nimport pytesseract\npytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n\nInstall the Python wrapper:\n\npip install pytesseract\n\n\nBasic usage\nfrom PIL import Image  # Pillow is the Python Imaging Library\nimport pytesseract\n\n# Extract plain text\ntext = pytesseract.image_to_string(Image.open(\"example.png\"))\n\n# Structured output with positions and confidences\ndf = pytesseract.image_to_data(Image.open(\"example.png\"), output_type=pytesseract.Output.DATAFRAME)\n\n# Character-level bounding boxes\nboxes = pytesseract.image_to_boxes(Image.open(\"example.png\"))\nReplace \"example.png\" with your own image file containing text. Pytesseract supports both in-memory images and file paths."
  },
  {
    "objectID": "Toolbox/Libraries/PyTesseract.html#questions",
    "href": "Toolbox/Libraries/PyTesseract.html#questions",
    "title": "Pytesseract: OCR with Tesseract (LSTM) in Python",
    "section": "Questions?",
    "text": "Questions?\nWorking on OCR for maps, handwritten notes, or multilingual scans? Curious whether Tesseract is the right fit for your pipeline? Post in the Nexus Q&A to share examples or get advice."
  },
  {
    "objectID": "Toolbox/Libraries/PyTesseract.html#see-also",
    "href": "Toolbox/Libraries/PyTesseract.html#see-also",
    "title": "Pytesseract: OCR with Tesseract (LSTM) in Python",
    "section": "See also",
    "text": "See also\n\nGitHub repo: madmaze/pytesseract – Source code and examples\nPaddleOCR – End-to-end OCR with detection, recognition, and layout modeling (CNN and ViT backends)\nTrOCR – Transformer-based OCR with multilingual support\nDonut – OCR + document understanding via vision-language modeling\nEasyOCR – Lightweight OCR tool with CNN + LSTM backends"
  },
  {
    "objectID": "Toolbox/Compute/CHTC.html",
    "href": "Toolbox/Compute/CHTC.html",
    "title": "Center for High Throughput Computing (CHTC)",
    "section": "",
    "text": "Established in 2006, the Center for High Throughput Computing (CHTC) is committed to democratizing access to powerful computing resources across all research domains. From their website: &gt; We are the University of Wisconsin-Madison’s core computational service provider for large scale computing. CHTC services are open to UW-Madison staff, students, faculty, and external collaborators. &gt; We offer both a High Throughput Computing system and a High Performance Computing cluster. Access to CPUs/GPUs, high-memory servers, data storage capacity, as well as personalized consultations and classroom support, are provided at no-cost.\nAre you a researcher at UW-Madison seeking to extend your computing capabilities beyond local resources, particularly for machine learning tasks? Request an account now to take advantage of the open computing services offered by the CHTC!\n\n\nThis guide provides some of our recommendations for success in running machine learning (specifically deep learning) jobs in CHTC. If you need help leveraging any of the CHTC resources, please reach out to chtc@cs.wisc.edu."
  },
  {
    "objectID": "Toolbox/Compute/CHTC.html#about-this-resource",
    "href": "Toolbox/Compute/CHTC.html#about-this-resource",
    "title": "Center for High Throughput Computing (CHTC)",
    "section": "",
    "text": "Established in 2006, the Center for High Throughput Computing (CHTC) is committed to democratizing access to powerful computing resources across all research domains. From their website: &gt; We are the University of Wisconsin-Madison’s core computational service provider for large scale computing. CHTC services are open to UW-Madison staff, students, faculty, and external collaborators. &gt; We offer both a High Throughput Computing system and a High Performance Computing cluster. Access to CPUs/GPUs, high-memory servers, data storage capacity, as well as personalized consultations and classroom support, are provided at no-cost.\nAre you a researcher at UW-Madison seeking to extend your computing capabilities beyond local resources, particularly for machine learning tasks? Request an account now to take advantage of the open computing services offered by the CHTC!\n\n\nThis guide provides some of our recommendations for success in running machine learning (specifically deep learning) jobs in CHTC. If you need help leveraging any of the CHTC resources, please reach out to chtc@cs.wisc.edu."
  },
  {
    "objectID": "Toolbox/Data/LVD2021_Leaf-Vein-Dataset.html",
    "href": "Toolbox/Data/LVD2021_Leaf-Vein-Dataset.html",
    "title": "Leaf Vein Dataset (LVD2021)",
    "section": "",
    "text": "The Leaf Vein Dataset 2021 (LVD2021), introduced in the paper Leaf vein segmentation with self-supervision, contains high-resolution images of leaves with pixel-wise annotations of vein structures. This dataset can be used to develop machine learning models for leaf vein segmentation and plant phenotyping. The dataset includes 4977 images across 36 different types of leaves, with over 100 images per type. The pixel-level annotations make it suitable for training models for fine-grained segmentation tasks. To request access to the Leaf Vein Dataset 2021 (LVD2021), please fill out the request form.\nThe paper’s code can also be accessed here: github.com/LeryLee/vein_segmentation?tab=readme-ov-filew.\n\n\n\nHigh-resolution images: Each image in the dataset is 2736x3648 pixels, providing detailed views of leaf structures.\nPixel-wise annotations: Each leaf image comes with annotations that precisely mark the vein architecture, enabling fine-grained segmentation.\nVariety of leaf types: The dataset covers a wide range of plant species, allowing for diverse research opportunities in botany and plant phenotyping.\n\n\n\n\n\nLeaf vein segmentation: Develop models to extract vein features such as vein continuity, branching, and intersections. Models like U-Net can be adapted for this task.\nPlant phenotyping: Analyze leaf vein characteristics, which are important for understanding water and nutrient transport in plants. This can be useful for ecological and environmental monitoring.\nSelf-supervised learning: The dataset’s fine annotations make it suitable for self-supervised learning techniques like pseudo-labeling, which allows models to be trained with only a small number of labeled samples.\nSegmentation in noisy environments: Develop segmentation models that can handle complex leaf images with indistinguishable mesophyll, breakpoints, and blurred vein boundaries.\n\n\n\n\n\nQuantitative Plant Dataset: A resource for phenotyping plant structures that can be used alongside LVD2021 for plant-related research.\nUW-Madison Infected Tomato Leaf - Vein Segmentation: A Kaggle competition that focuses on segmenting primary and secondary vein structures from tomato leaf images at varying stages of disease progression."
  },
  {
    "objectID": "Toolbox/Data/LVD2021_Leaf-Vein-Dataset.html#about-this-resource",
    "href": "Toolbox/Data/LVD2021_Leaf-Vein-Dataset.html#about-this-resource",
    "title": "Leaf Vein Dataset (LVD2021)",
    "section": "",
    "text": "The Leaf Vein Dataset 2021 (LVD2021), introduced in the paper Leaf vein segmentation with self-supervision, contains high-resolution images of leaves with pixel-wise annotations of vein structures. This dataset can be used to develop machine learning models for leaf vein segmentation and plant phenotyping. The dataset includes 4977 images across 36 different types of leaves, with over 100 images per type. The pixel-level annotations make it suitable for training models for fine-grained segmentation tasks. To request access to the Leaf Vein Dataset 2021 (LVD2021), please fill out the request form.\nThe paper’s code can also be accessed here: github.com/LeryLee/vein_segmentation?tab=readme-ov-filew.\n\n\n\nHigh-resolution images: Each image in the dataset is 2736x3648 pixels, providing detailed views of leaf structures.\nPixel-wise annotations: Each leaf image comes with annotations that precisely mark the vein architecture, enabling fine-grained segmentation.\nVariety of leaf types: The dataset covers a wide range of plant species, allowing for diverse research opportunities in botany and plant phenotyping.\n\n\n\n\n\nLeaf vein segmentation: Develop models to extract vein features such as vein continuity, branching, and intersections. Models like U-Net can be adapted for this task.\nPlant phenotyping: Analyze leaf vein characteristics, which are important for understanding water and nutrient transport in plants. This can be useful for ecological and environmental monitoring.\nSelf-supervised learning: The dataset’s fine annotations make it suitable for self-supervised learning techniques like pseudo-labeling, which allows models to be trained with only a small number of labeled samples.\nSegmentation in noisy environments: Develop segmentation models that can handle complex leaf images with indistinguishable mesophyll, breakpoints, and blurred vein boundaries.\n\n\n\n\n\nQuantitative Plant Dataset: A resource for phenotyping plant structures that can be used alongside LVD2021 for plant-related research.\nUW-Madison Infected Tomato Leaf - Vein Segmentation: A Kaggle competition that focuses on segmenting primary and secondary vein structures from tomato leaf images at varying stages of disease progression."
  },
  {
    "objectID": "Toolbox/Data/LVD2021_Leaf-Vein-Dataset.html#questions",
    "href": "Toolbox/Data/LVD2021_Leaf-Vein-Dataset.html#questions",
    "title": "Leaf Vein Dataset (LVD2021)",
    "section": "Questions?",
    "text": "Questions?\nIf you have any lingering questions about this resource, feel free to post them on the ML+X Nexus Q&A on GitHub. We will update this resource as new information or applications arise."
  },
  {
    "objectID": "Toolbox/Data/LVD2021_Leaf-Vein-Dataset.html#see-also",
    "href": "Toolbox/Data/LVD2021_Leaf-Vein-Dataset.html#see-also",
    "title": "Leaf Vein Dataset (LVD2021)",
    "section": "See also",
    "text": "See also\n\nModels: U-Net: Explore one of the most popular segmentation models."
  },
  {
    "objectID": "Toolbox/Data/CIFAR.html",
    "href": "Toolbox/Data/CIFAR.html",
    "title": "CIFAR Dataset",
    "section": "",
    "text": "CIFAR-10 (Canadian Institute for Advanced Research) is a widely used dataset consisting of 60,000 color images at a resolution of 32×32 pixels, spanning 10 distinct classes with 6,000 images per class. To be exact, the classes are airplanes, automobiles, cats, deer, dogs, horses, ships, and trucks. It is a labeled subset of the 80 Million Tiny Images dataset, originally created by researchers at MIT and NYU, with image annotations provided by paid student annotators. Despite its simplicity, CIFAR-10 covers a diverse set of real-world objects, making it a standard benchmark for evaluating and training computer vision models.\nCIFAR-100 is a more fine-grained variant of CIFAR-10, containing the same images but categorized into 100 classes instead of 10. These 100 classes are grouped into 20 superclasses, with 600 images per class. This finer categorization makes CIFAR-100 a more challenging dataset, requiring models to distinguish between more nuanced variations of objects within the same broad categories. For example, in CIFAR-10, all dogs fall under a single “dog” category. In CIFAR-100, this is broken down into multiple specific dog breeds such as beagle, dalmatian, and golden retriever. Similarly, the “automobile” category in CIFAR-10 is split into more detailed vehicle types in CIFAR-100, such as pickup truck, race car, and tractor. This increased level of detail makes CIFAR-100 useful for training models that need to recognize finer-grained object differences.\n\n\n\nSmall size: Each image is only 32x32 pixels, and with only 60,000 images in total, the dataset is computationally manageable.\nDiversity: The dataset cover a broad range of real-world objects, mainly consisting to animals and transportation vehicles. With this diveristy, CIFAR-10 ensures that models generalize well across different types of objects rather than specializing in a narrow domain.\nBalance: Each class contains an equal number of images, preventing class imbalance issues.\n\n\n\n\n\nImage classification benchmarking: CIFAR has historically been a widely used benchmark dataset for evaluating the performance of image classification models, including convolutional neural networks (CNNs), vision transformers (ViTs), and other deep learning architectures.\nTraining, testing, and experimentation: CIFAR is commonly used in supervised learning tasks such as training CNNs for object recognition, evaluating feature extraction techniques (e.g., PCA), and testing optimization methods.\n\nTransfer learning: Pre-trained models on CIFAR are often used as a starting point for fine-tuning CNNs when working with limited training data.\n\nData augmentation: CIFAR images can be augmented using transformations such as random rotations, flips, cropping, and color adjustments to artificially expand the dataset and improve model generalization. More advanced augmentation methods like MixUp and CutMix can further enhance classification accuracy.\n\n\n\n\nCIFAR’s parent dataset has been widely criticized for its content and collection method. 80 Million Tiny Images was created by scraping images off the internet without knowledge or consent of any of the owners of the photos. Also, in 2020, 80 Million Tiny Images was found to contain a range of racist, sexist, and other offensive labels, and for that reason was taken offline by MIT ans NYU in 2020, who also requested other researchers and users refrain from using copies of the dataset.\n\n\n\n\nDollar street 10: MLCommons Dollar Street Dataset is a great alternative to CIFAR-10 that is not ethically problematic. Unlike CIFAR-10, all the images in Dollar street 10 are under public domain data, licensed for academic, commercial and non-commercial usage, or under CC-BY and CC-BY-SA 4. Like CIFAR-10, the images are of 10 categories of everyday, real-life objects, but was created in effort to visually capture socioeconomic diversity of these objects around the globe in order to avoid bias in machine learning models."
  },
  {
    "objectID": "Toolbox/Data/CIFAR.html#about-this-resource",
    "href": "Toolbox/Data/CIFAR.html#about-this-resource",
    "title": "CIFAR Dataset",
    "section": "",
    "text": "CIFAR-10 (Canadian Institute for Advanced Research) is a widely used dataset consisting of 60,000 color images at a resolution of 32×32 pixels, spanning 10 distinct classes with 6,000 images per class. To be exact, the classes are airplanes, automobiles, cats, deer, dogs, horses, ships, and trucks. It is a labeled subset of the 80 Million Tiny Images dataset, originally created by researchers at MIT and NYU, with image annotations provided by paid student annotators. Despite its simplicity, CIFAR-10 covers a diverse set of real-world objects, making it a standard benchmark for evaluating and training computer vision models.\nCIFAR-100 is a more fine-grained variant of CIFAR-10, containing the same images but categorized into 100 classes instead of 10. These 100 classes are grouped into 20 superclasses, with 600 images per class. This finer categorization makes CIFAR-100 a more challenging dataset, requiring models to distinguish between more nuanced variations of objects within the same broad categories. For example, in CIFAR-10, all dogs fall under a single “dog” category. In CIFAR-100, this is broken down into multiple specific dog breeds such as beagle, dalmatian, and golden retriever. Similarly, the “automobile” category in CIFAR-10 is split into more detailed vehicle types in CIFAR-100, such as pickup truck, race car, and tractor. This increased level of detail makes CIFAR-100 useful for training models that need to recognize finer-grained object differences.\n\n\n\nSmall size: Each image is only 32x32 pixels, and with only 60,000 images in total, the dataset is computationally manageable.\nDiversity: The dataset cover a broad range of real-world objects, mainly consisting to animals and transportation vehicles. With this diveristy, CIFAR-10 ensures that models generalize well across different types of objects rather than specializing in a narrow domain.\nBalance: Each class contains an equal number of images, preventing class imbalance issues.\n\n\n\n\n\nImage classification benchmarking: CIFAR has historically been a widely used benchmark dataset for evaluating the performance of image classification models, including convolutional neural networks (CNNs), vision transformers (ViTs), and other deep learning architectures.\nTraining, testing, and experimentation: CIFAR is commonly used in supervised learning tasks such as training CNNs for object recognition, evaluating feature extraction techniques (e.g., PCA), and testing optimization methods.\n\nTransfer learning: Pre-trained models on CIFAR are often used as a starting point for fine-tuning CNNs when working with limited training data.\n\nData augmentation: CIFAR images can be augmented using transformations such as random rotations, flips, cropping, and color adjustments to artificially expand the dataset and improve model generalization. More advanced augmentation methods like MixUp and CutMix can further enhance classification accuracy.\n\n\n\n\nCIFAR’s parent dataset has been widely criticized for its content and collection method. 80 Million Tiny Images was created by scraping images off the internet without knowledge or consent of any of the owners of the photos. Also, in 2020, 80 Million Tiny Images was found to contain a range of racist, sexist, and other offensive labels, and for that reason was taken offline by MIT ans NYU in 2020, who also requested other researchers and users refrain from using copies of the dataset.\n\n\n\n\nDollar street 10: MLCommons Dollar Street Dataset is a great alternative to CIFAR-10 that is not ethically problematic. Unlike CIFAR-10, all the images in Dollar street 10 are under public domain data, licensed for academic, commercial and non-commercial usage, or under CC-BY and CC-BY-SA 4. Like CIFAR-10, the images are of 10 categories of everyday, real-life objects, but was created in effort to visually capture socioeconomic diversity of these objects around the globe in order to avoid bias in machine learning models."
  },
  {
    "objectID": "Toolbox/Data/CIFAR.html#questions",
    "href": "Toolbox/Data/CIFAR.html#questions",
    "title": "CIFAR Dataset",
    "section": "Questions?",
    "text": "Questions?\nIf you have any lingering questions about this resource, feel free to post them on the ML+X Nexus Q&A on GitHub. We will update this resource as new information or applications arise."
  },
  {
    "objectID": "Toolbox/Data/CIFAR.html#see-also",
    "href": "Toolbox/Data/CIFAR.html#see-also",
    "title": "CIFAR Dataset",
    "section": "See also",
    "text": "See also\n\n\nCIFAR-10 Kaggle: CIFAR-10 Kaggle image prediction competition.\nPapers With Code (CIFAR-10): Additional documentation on CIFAR-10\nPapers With Code (CIFAR-100): CIFAR-100 documentation\nCIFAR Ethical Acknowledgement: News article that detailed the ethical concerns of 80 Million Tiny Images that lead to its shutdown.\nDollar street 10: More details and download link to Dollar street 10."
  },
  {
    "objectID": "Toolbox/Data/INQUIRE.html",
    "href": "Toolbox/Data/INQUIRE.html",
    "title": "INQUIRE",
    "section": "",
    "text": "INQUIRE is a benchmark for evaluating text-to-image retrieval models on expert-level ecological queries. Built on a 5-million-image subset of iNaturalist 2024 (iNat24), it includes 250 natural language prompts developed with domain experts — such as “a sick cassava plant” or “a hermit crab using plastic trash as its shell.” Each query is paired with a comprehensively labeled set of relevant images (over 33,000 in total), drawn from real-world biodiversity data.\nINQUIRE goes beyond traditional retrieval tasks by focusing on scientifically meaningful concepts. Its goal is to push AI systems toward supporting ecological and biodiversity research, helping researchers find specific behaviors, species traits, or environmental conditions in large image corpora.\n\n\nMany AI benchmarks rely on generic internet content. INQUIRE challenges vision-language models to match complex, expert-level queries that require ecological reasoning and domain-specific knowledge.\nQuery categories include:\n\nSpecies identification\n\nVisual appearance\n\nBehavior\n\nContextual or environmental cues\n\nEven top-performing models — including CLIP ViT-H with GPT-4o reranking — fail to achieve mean average precision (mAP@50) above 50%. This reflects the difficulty of bridging general-purpose vision-language modeling with the nuance required for real-world science.\nINQUIRE provides a structured way to evaluate whether retrieval systems are ready to assist with challenges like species monitoring, environmental analysis, and automated ecological observation.\n\n\n\nINQUIRE supports two evaluation tasks:\n\nFullrank: Search across all 5 million iNat24 images using a model like CLIP to retrieve a ranked list of results. You can optionally apply a second-stage reranker (e.g., GPT-4o) to reorder your top-k results. This task measures a model’s ability to surface relevant images from scratch.\nRerank: Work with a fixed set of 100 candidate images per query (retrieved using a baseline CLIP model). Your model’s job is to reorder these candidates so the most relevant matches are at the top. This setup is faster, requires fewer compute resources, and is hosted on Hugging Face.\n\nBoth tasks include labeled relevance judgments and are benchmarked using metrics like mAP@50. See the leaderboards for results across recent models.\n\n\n\nCLIP (Contrastive Language-Image Pretraining) is central to INQUIRE’s design and evaluation:\n\nCLIP embeddings are used in both retrieval and reranking tasks\nMost baseline systems use CLIP or open-source CLIP variants (e.g., OpenCLIP)\nINQUIRE measures how well CLIP-style models generalize to ecological domains\n\nIf you’re new to CLIP, it learns a shared embedding space for text and images via contrastive learning. This allows models to rank images based on their semantic similarity to a text query — a powerful capability, but one that may struggle with fine-grained ecological detail.\n\n\n\nThe INQUIRE demo lets you run your own queries using zero-shot CLIP retrieval over iNat24. However:\n\nIt uses CLIP embeddings trained on web data, not fine-tuned for ecology\nReturned results may include images outside the benchmark’s labeled ground truth\nResults are approximate and may reflect CLIP’s known limitations or biases\n\nThe demo is ideal for exploring what CLIP “sees” — but not for comparing benchmark performance.\n\n\n\n\nHow often do specific animal behaviors (e.g., parenting, foraging, aggression) show up in community-contributed data?\nCan we surface rare or hard-to-label events like parasitism, injury, or environmental degradation?\nWhat types of organisms or behaviors are over- or underrepresented in public biodiversity datasets?\nCan retrieval models help scientists find subtle indicators, like sick plants or tagged animals, that aren’t captured in structured labels?\nHow do current AI models perform when interpreting domain-specific visual queries?\n\n\n\n\n\nBuilding retrieval tools for ecologists and conservation scientists\nDeveloping multimodal models for biodiversity datasets\nTesting zero-shot generalization of vision-language models on real-world visual data\nConstructing image subsets for training, analysis, or monitoring using query-based filtering\nSupporting research workflows that require specific ecological visual evidence"
  },
  {
    "objectID": "Toolbox/Data/INQUIRE.html#about-this-resource",
    "href": "Toolbox/Data/INQUIRE.html#about-this-resource",
    "title": "INQUIRE",
    "section": "",
    "text": "INQUIRE is a benchmark for evaluating text-to-image retrieval models on expert-level ecological queries. Built on a 5-million-image subset of iNaturalist 2024 (iNat24), it includes 250 natural language prompts developed with domain experts — such as “a sick cassava plant” or “a hermit crab using plastic trash as its shell.” Each query is paired with a comprehensively labeled set of relevant images (over 33,000 in total), drawn from real-world biodiversity data.\nINQUIRE goes beyond traditional retrieval tasks by focusing on scientifically meaningful concepts. Its goal is to push AI systems toward supporting ecological and biodiversity research, helping researchers find specific behaviors, species traits, or environmental conditions in large image corpora.\n\n\nMany AI benchmarks rely on generic internet content. INQUIRE challenges vision-language models to match complex, expert-level queries that require ecological reasoning and domain-specific knowledge.\nQuery categories include:\n\nSpecies identification\n\nVisual appearance\n\nBehavior\n\nContextual or environmental cues\n\nEven top-performing models — including CLIP ViT-H with GPT-4o reranking — fail to achieve mean average precision (mAP@50) above 50%. This reflects the difficulty of bridging general-purpose vision-language modeling with the nuance required for real-world science.\nINQUIRE provides a structured way to evaluate whether retrieval systems are ready to assist with challenges like species monitoring, environmental analysis, and automated ecological observation.\n\n\n\nINQUIRE supports two evaluation tasks:\n\nFullrank: Search across all 5 million iNat24 images using a model like CLIP to retrieve a ranked list of results. You can optionally apply a second-stage reranker (e.g., GPT-4o) to reorder your top-k results. This task measures a model’s ability to surface relevant images from scratch.\nRerank: Work with a fixed set of 100 candidate images per query (retrieved using a baseline CLIP model). Your model’s job is to reorder these candidates so the most relevant matches are at the top. This setup is faster, requires fewer compute resources, and is hosted on Hugging Face.\n\nBoth tasks include labeled relevance judgments and are benchmarked using metrics like mAP@50. See the leaderboards for results across recent models.\n\n\n\nCLIP (Contrastive Language-Image Pretraining) is central to INQUIRE’s design and evaluation:\n\nCLIP embeddings are used in both retrieval and reranking tasks\nMost baseline systems use CLIP or open-source CLIP variants (e.g., OpenCLIP)\nINQUIRE measures how well CLIP-style models generalize to ecological domains\n\nIf you’re new to CLIP, it learns a shared embedding space for text and images via contrastive learning. This allows models to rank images based on their semantic similarity to a text query — a powerful capability, but one that may struggle with fine-grained ecological detail.\n\n\n\nThe INQUIRE demo lets you run your own queries using zero-shot CLIP retrieval over iNat24. However:\n\nIt uses CLIP embeddings trained on web data, not fine-tuned for ecology\nReturned results may include images outside the benchmark’s labeled ground truth\nResults are approximate and may reflect CLIP’s known limitations or biases\n\nThe demo is ideal for exploring what CLIP “sees” — but not for comparing benchmark performance.\n\n\n\n\nHow often do specific animal behaviors (e.g., parenting, foraging, aggression) show up in community-contributed data?\nCan we surface rare or hard-to-label events like parasitism, injury, or environmental degradation?\nWhat types of organisms or behaviors are over- or underrepresented in public biodiversity datasets?\nCan retrieval models help scientists find subtle indicators, like sick plants or tagged animals, that aren’t captured in structured labels?\nHow do current AI models perform when interpreting domain-specific visual queries?\n\n\n\n\n\nBuilding retrieval tools for ecologists and conservation scientists\nDeveloping multimodal models for biodiversity datasets\nTesting zero-shot generalization of vision-language models on real-world visual data\nConstructing image subsets for training, analysis, or monitoring using query-based filtering\nSupporting research workflows that require specific ecological visual evidence"
  },
  {
    "objectID": "Toolbox/Data/INQUIRE.html#related-datasets-projects",
    "href": "Toolbox/Data/INQUIRE.html#related-datasets-projects",
    "title": "INQUIRE",
    "section": "Related datasets & projects",
    "text": "Related datasets & projects\n\niNaturalist 2024 (iNat24): Download the iNat24 dataset from the INQUIRE GitHub"
  },
  {
    "objectID": "Toolbox/Data/INQUIRE.html#questions",
    "href": "Toolbox/Data/INQUIRE.html#questions",
    "title": "INQUIRE",
    "section": "Questions?",
    "text": "Questions?\nIf you’re using INQUIRE or want to ask about its structure, performance metrics, or use cases, feel free to post in the ML+X Nexus Q&A forum. We’re always interested in sharing new ways this benchmark is being applied."
  },
  {
    "objectID": "Toolbox/Data/INQUIRE.html#see-also",
    "href": "Toolbox/Data/INQUIRE.html#see-also",
    "title": "INQUIRE",
    "section": "See also",
    "text": "See also\n\nTalk: Automating Scientific Discovery: From Natural World Data to Systematic Literature Reviews: Edward Vendrow, a PhD student at MIT, presents his research on automating scientific discovery using multimodal AI. In this talk, he explores how AI can accelerate research through large-scale ecological image analysis and systematic literature reviews.\nData: iNaturalist: Learn more about the iNaturalist dataset."
  },
  {
    "objectID": "Toolbox/Models/DeepForest.html",
    "href": "Toolbox/Models/DeepForest.html",
    "title": "DeepForest: A Toolkit of Models for Tree and Wildlife Detection in Aerial Imagery",
    "section": "",
    "text": "DeepForest is an open-source Python library for object detection in aerial RGB imagery, designed for ecological applications such as identifying tree crowns (the visible upper portion of trees), tracking wildlife, and monitoring habitat. Rather than a single model, DeepForest provides a toolkit of pretrained deep learning models and utilities for training, evaluation, and geospatial integration.\nDeveloped by Ben Weinstein, Sergio Marconi, Mélaine Aubry-Kientz, and Ethan White, DeepForest is maintained by the Weecology Lab. It supports multiple object types, including trees, birds, and livestock, and is built on top of RetinaNet and PyTorch Lightning.\n\n\n\nPretrained (CNN) models for tree crowns, birds, livestock, and alive/dead tree detection\n\nSupports bounding boxes, polygons, and point annotations\n\nReads common geospatial formats: CSV, shapefiles, GeoPackages, COCO, VOC\n\nCompatible with tile-based prediction and raster workflows\n\nBuilt on RetinaNet with PyTorch Lightning for scalable training\n\n\n\n\nDeepForest builds on progress in object detection and remote sensing for ecological applications:\n\nFaster R-CNN (2015): Introduced region-based object detection\n\nRetinaNet (2017): Backbone architecture for DeepForest; introduced focal loss to improve accuracy on dense, small-object scenes\n\nDeepForest 1.0 (2019): Released with a pretrained tree crown detection model using RGB imagery\n\nDeepForest 1.4–1.5 (2023–2025): Added support for polygons, PyTorch Lightning, and new models for birds, livestock, and tree health"
  },
  {
    "objectID": "Toolbox/Models/DeepForest.html#about-this-resource",
    "href": "Toolbox/Models/DeepForest.html#about-this-resource",
    "title": "DeepForest: A Toolkit of Models for Tree and Wildlife Detection in Aerial Imagery",
    "section": "",
    "text": "DeepForest is an open-source Python library for object detection in aerial RGB imagery, designed for ecological applications such as identifying tree crowns (the visible upper portion of trees), tracking wildlife, and monitoring habitat. Rather than a single model, DeepForest provides a toolkit of pretrained deep learning models and utilities for training, evaluation, and geospatial integration.\nDeveloped by Ben Weinstein, Sergio Marconi, Mélaine Aubry-Kientz, and Ethan White, DeepForest is maintained by the Weecology Lab. It supports multiple object types, including trees, birds, and livestock, and is built on top of RetinaNet and PyTorch Lightning.\n\n\n\nPretrained (CNN) models for tree crowns, birds, livestock, and alive/dead tree detection\n\nSupports bounding boxes, polygons, and point annotations\n\nReads common geospatial formats: CSV, shapefiles, GeoPackages, COCO, VOC\n\nCompatible with tile-based prediction and raster workflows\n\nBuilt on RetinaNet with PyTorch Lightning for scalable training\n\n\n\n\nDeepForest builds on progress in object detection and remote sensing for ecological applications:\n\nFaster R-CNN (2015): Introduced region-based object detection\n\nRetinaNet (2017): Backbone architecture for DeepForest; introduced focal loss to improve accuracy on dense, small-object scenes\n\nDeepForest 1.0 (2019): Released with a pretrained tree crown detection model using RGB imagery\n\nDeepForest 1.4–1.5 (2023–2025): Added support for polygons, PyTorch Lightning, and new models for birds, livestock, and tree health"
  },
  {
    "objectID": "Toolbox/Models/DeepForest.html#model-playground",
    "href": "Toolbox/Models/DeepForest.html#model-playground",
    "title": "DeepForest: A Toolkit of Models for Tree and Wildlife Detection in Aerial Imagery",
    "section": "Model playground",
    "text": "Model playground\nAll pretrained models are currently based on RetinaNet and convolutional neural networks (CNNs); no ViTs or transformer-based models (yet). Pretrained models are available via Hugging Face & GitHub: huggingface.co/weecology\n\nweecology/deepforest-tree: General tree crown detection\n\nweecology/deepforest-bird: Aerial bird detection\n\nweecology/deepforest-livestock: Detects large mammals in farmland\n\nAlive/dead classifier: Distinguishes dead trees based on RGB appearance\n\nTo use these models, you must install the DeepForest Python package:\npip install deepforest\nThe models are downloaded from Hugging Face but are loaded and run using DeepForest’s own API—not through Hugging Face’s transformers or pipeline tools. Data can be loaded from shapefiles, CSVs, GeoPackages, and other formats using DeepForest’s read_file utility, which standardizes annotations. The package supports both full-image and tile-based inference.\nTry the DeepForest demo on Hugging Face to test the model in your browser using single-image uploads.\n\nTransfer learning and fine-tuning\nDeepForest supports fine-tuning and transfer learning through its PyTorch Lightning backbone. You can:\n\nFine-tune pretrained models using your own labeled data\n\nCustomize training behavior by subclassing the DeepForest module\n\nUse custom PyTorch dataloaders for advanced workflows (e.g., WILDS-style datasets)\n\nExtend the package with new architectures if needed\n\nThis makes it easy to adapt DeepForest to new regions, forest types, or species with relatively small amounts of local data.\n\n\nPerformance expectations\n\nTree crown model: F1-scores between 0.73 and 0.95 depending on site and canopy structure (Weinstein et al., 2019, Remote Sensing)\n\nBird model: ~65% recall on new data; ~84% recall with just 1,000 local annotations (Weinstein et al., 2022, Ecological Applications)\n\nAlive/dead classifier: 95.8% accuracy on held-out image crops (Hugging Face model card)\n\nPerformance varies by imagery quality, tree species, and geographic region. Fine-tuning is recommended for most new applications."
  },
  {
    "objectID": "Toolbox/Models/DeepForest.html#questions",
    "href": "Toolbox/Models/DeepForest.html#questions",
    "title": "DeepForest: A Toolkit of Models for Tree and Wildlife Detection in Aerial Imagery",
    "section": "Questions?",
    "text": "Questions?\nIf you have any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Toolbox/Models/DeepForest.html#see-also",
    "href": "Toolbox/Models/DeepForest.html#see-also",
    "title": "DeepForest: A Toolkit of Models for Tree and Wildlife Detection in Aerial Imagery",
    "section": "See also",
    "text": "See also\n\nDocumentation: DeepForest User Guide: Full instructions for loading, training, evaluating, and customizing models\n\nModel: U-Net: A foundational segmentation model used in ecological and medical applications\n\nTalk: Automating Scientific Discovery: Discusses retrieval and large-scale image analysis in ecology; mentions DeepForest among supporting tools."
  },
  {
    "objectID": "Toolbox/Models/UNET.html",
    "href": "Toolbox/Models/UNET.html",
    "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
    "section": "",
    "text": "U-Net is a convolutional neural network architecture designed for biomedical image segmentation. Introduced in 2015 by Ronneberger and colleagues in the paper, “U-Net: Convolutional Networks for Biomedical Image Segmentation”, U-Net’s encoder-decoder architecture, combined with skip connections, allows for high accuracy in pixel-wise classification tasks. It remains one of the most widely used models for segmentation across various domains, from medical imaging to satellite image analysis.\n\n\n\nEncoder-Decoder Architecture: U-Net utilizes a contracting path (encoder) for context and an expansive path (decoder) for localization, making it effective in segmentation tasks.\nSkip Connections: These connections between encoder and decoder layers allow for the preservation of spatial information, leading to more accurate segmentation.\nData Efficiency: U-Net is effective even with relatively small datasets, a common scenario in medical and specialized imaging tasks."
  },
  {
    "objectID": "Toolbox/Models/UNET.html#about-this-resource",
    "href": "Toolbox/Models/UNET.html#about-this-resource",
    "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
    "section": "",
    "text": "U-Net is a convolutional neural network architecture designed for biomedical image segmentation. Introduced in 2015 by Ronneberger and colleagues in the paper, “U-Net: Convolutional Networks for Biomedical Image Segmentation”, U-Net’s encoder-decoder architecture, combined with skip connections, allows for high accuracy in pixel-wise classification tasks. It remains one of the most widely used models for segmentation across various domains, from medical imaging to satellite image analysis.\n\n\n\nEncoder-Decoder Architecture: U-Net utilizes a contracting path (encoder) for context and an expansive path (decoder) for localization, making it effective in segmentation tasks.\nSkip Connections: These connections between encoder and decoder layers allow for the preservation of spatial information, leading to more accurate segmentation.\nData Efficiency: U-Net is effective even with relatively small datasets, a common scenario in medical and specialized imaging tasks."
  },
  {
    "objectID": "Toolbox/Models/UNET.html#timeline-context",
    "href": "Toolbox/Models/UNET.html#timeline-context",
    "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
    "section": "Timeline context",
    "text": "Timeline context\nU-Net has been pivotal in advancing image segmentation since its introduction in 2015. Here is a timeline placing U-Net in the broader context of computer vision model development.\n\nLeNet (1998): One of the first CNN architectures for digit recognition.\nAlexNet (2012): Significantly improved CNN performance using deep learning and GPUs for large-scale image classification.\nVGGNet (2014): Simplified CNN architecture by using small convolutional filters, deeper layers.\nFully Convolutional Networks (FCN) (2014): Pioneered fully convolutional networks for image segmentation.\nSegNet (2015): Encoder-decoder architecture optimized for road scene segmentation.\nU-Net (2015): Designed for biomedical image segmentation with an encoder-decoder architecture and skip connections.\nResNet (2015): Introduced residual learning to address vanishing gradient problems in deep networks.\nMask R-CNN (2017): Extended Faster R-CNN for pixel-level segmentation tasks.\nVision Transformer (ViT) (2020): Applied transformer models for image classification tasks.\nSwin Transformer (2021): Hierarchical transformer for vision tasks with improved efficiency.\nSegment Anything (SAM) (2023): A foundation model for segmentation, offering high generalization across image domains."
  },
  {
    "objectID": "Toolbox/Models/UNET.html#u-net-variants",
    "href": "Toolbox/Models/UNET.html#u-net-variants",
    "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
    "section": "U-Net variants",
    "text": "U-Net variants\n\nAttention U-Net: Introduces attention mechanisms to U-Net for more accurate segmentation.\n3D U-Net: Designed for 3D medical imaging tasks such as volumetric segmentation.\nResUnet: Combines U-Net with residual connections for enhanced performance in complex tasks.\nnnU-Net: A self-configuring, state-of-the-art variant for deep learning-based biomedical image segmentation. nnU-Net adapts automatically to a given dataset, optimizing network topology, preprocessing, and postprocessing. Widely used in biomedical challenges and competitions, it serves as both a strong baseline and a development framework for researchers."
  },
  {
    "objectID": "Toolbox/Models/UNET.html#model-playground",
    "href": "Toolbox/Models/UNET.html#model-playground",
    "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
    "section": "Model playground",
    "text": "Model playground\n\nTutorials and Getting Started Notebooks\n\nnnU-Net: Scroll down on the nnU-Net GitHub README for documentation on installing, finetuning, and more.\n\n\n\nHigh-level tips for effective use\n\nPre-trained Encoders: Consider using pre-trained encoders from models like ResNet or EfficientNet to improve performance.\nRegularization Techniques: Apply dropout, early stopping, or weight decay to prevent overfitting, especially on small datasets.\nData Augmentation: Employ data augmentation techniques when working with small datasets to improve model generalization.\nOptimizing Loss Function: Use specialized loss functions such as Dice coefficient or Intersection over Union (IoU) for pixel-wise optimization.\nArchitectural Adjustments: Depending on your dataset size, experiment with deeper or shallower architectures to balance overfitting and underfitting risks.\n\n\n\nRelated datasets & Kaggle challenges\n\nMedical Decathlon Dataset: A popular benchmark dataset for biomedical image segmentation.\nUW Madison GI Tract Image Segmentation:Track healthy organs in medical scans to improve cancer treatment.\nISIC Skin Cancer Segmentation: Dataset and challenges for skin lesion segmentation."
  },
  {
    "objectID": "Toolbox/Models/UNET.html#questions",
    "href": "Toolbox/Models/UNET.html#questions",
    "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
    "section": "Questions?",
    "text": "Questions?\nIf you have any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Toolbox/Models/UNET.html#see-also",
    "href": "Toolbox/Models/UNET.html#see-also",
    "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
    "section": "See also",
    "text": "See also\n\nPlaylists: ML4MI Seminar. Biomedial applications of ML (especially computer vision) at UW-Madison.\nVideo: Vision, Language, and Vision-Language Modeling in Radiology: In this ML4MI seminar, Tyler Bradshaw highlights the history and current use of vision (e.g., UNET), language, and vision-language models in medical imaging.\nModel hub: MONAI - Medical Open Network for AI. An open-source, community-supported framework for deep learning in healthcare imaging\nWorkshop: Introduction to Deep Learning with PyTorch. Learn how to use PyTorch to build and train deep learning models."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ML+X Nexus: Crowdsourced ML and AI Resources",
    "section": "",
    "text": "More than just a repository of content from across the internet, ML+X Nexus is a curated, community-driven platform that captures the collective knowledge and experiences of ML+X (and the broader UW campus). It serves as a growing knowledge hub—preserving past discussions, highlighting widely used tools and datasets, and evolving alongside our community’s needs.\n\nWhat kinds of resources are hosted on Nexus?\nAny content (original or external) that can help make the practice of ML/AI more connected, accessible, efficient, and reproducible is welcome on the Nexus platform! This includes, but is not limited to…\n\n🧠 Educational materials: Explore a library of educational materials (workshops, books, videos, etc.) covering a wide range of ML-related topics, tools, and workflows, from foundational concepts to advanced techniques.\n🛠 Models, code, and more: Learn about popular models, tools, and datasets that you can leverage for your next ML project.\n🧬 Applications & stories: Discover a curated collection of blogs, papers, and talks which dive into real-world ML applications and lessons learned by practitioners.\n\nDisclaimer: The crowdsourced resources on this website are not endorsed by the UW-Madison and have not been vetted by the Division of Information Technology.\n\n\nShare your knowledge on Nexus!\nThis site is powered by contributions from ML/AI practitioners. If you’ve found a tool, tutorial, dataset, or strategy that’s helped you in your ML/AI work, consider adding it to Nexus so others can benefit too. Learn more on our How to contribute. page.\n\n\nExplore resources\nTo narrow down your search, select one of the general category groupings from the left sidebar (e.g., Learn, Applications, Toolbox). You can also select one of the category tags on the right sidebar to filter resources. Visit the Category glossary if you are unsure about the meaning of any of these tags.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Fact-Based QA with RAG: Romeo and Juliet\n\n\n\nNotebooks\n\nRAG\n\nRetrieval\n\nNLP\n\nLLM\n\nEmbeddings\n\nText analysis\n\nDeep learning\n\nPrompt engineering\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2025-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative AI From Multiple Perspectives\n\n\n\nVideos\n\nML+X\n\nLLM\n\nHealthcare\n\nEHR\n\nExplainability\n\nDeep learning\n\nEducation\n\nRetrieval\n\nRAG\n\nNLP\n\n\n\n\n\n\n\nBenjamin Lengerich, Kaiser Pister\n\n\n2025-05-06\n\n\n\n\n\n\n\n\n\n\n\n\n\nPytesseract: OCR with Tesseract (LSTM) in Python\n\n\n\nLibraries\n\nOCR\n\nNLP\n\nComputer vision\n\nText extraction\n\nMultilingual\n\nDeep learning\n\nLSTM\n\n\n\n\n\n\n\nChris Endemann\n\n\n2025-04-05\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeepForest: A Toolkit of Models for Tree and Wildlife Detection in Aerial Imagery\n\n\n\nModels\n\nModel exploration\n\nObject detection\n\nRemote sensing\n\nEcology\n\nForest monitoring\n\nGeospatial data\n\nComputer vision\n\nDeep learning\n\nCNN\n\n\n\n\n\n\n\nChris Endemann\n\n\n2025-04-04\n\n\n\n\n\n\n\n\n\n\n\n\n\niNaturalist (iNat)\n\n\n\nData\n\nImage data\n\nBiology\n\nEcology\n\nComputer vision\n\nMultimodal learning\n\nBenchmarking\n\nCitizen science\n\nCLIP\n\nZero-shot learning\n\n\n\n\n\n\n\nChris Endemann\n\n\n2025-04-03\n\n\n\n\n\n\n\n\n\n\n\n\n\nHarnessing the Power of Foundation Models for Healthcare and Life Sciences\n\n\n\nVideos\n\nML+X\n\nHealthcare\n\nMedical imaging\n\nFoundation models\n\nMultimodal learning\n\nDeep learning\n\nEmbeddings\n\nImage data\n\nComputer vision\n\nRetrieval\n\nZero-shot learning\n\nLLM\n\nMicrosoft Copilot\n\nAzure\n\n\n\n\n\n\n\nJameson Merkow\n\n\n2025-04-01\n\n\n\n\n\n\n\n\n\n\n\n\n\nINQUIRE\n\n\n\nData\n\nMultimodal data\n\nComputer vision\n\nRetrieval\n\nZero-shot learning\n\nBenchmarking\n\nMultimodal learning\n\nViT\n\nLLM\n\nBiology\n\nEcology\n\nImage data\n\nHugging Face\n\n\n\n\n\n\n\nChris Endemann\n\n\n2025-03-26\n\n\n\n\n\n\n\n\n\n\n\n\n\nAutomating Scientific Discovery: From Natural World Data to Systematic Literature Reviews\n\n\n\nVideos\n\nML+X\n\nMultimodal learning\n\nBenchmarking\n\nRetrieval\n\nComputer vision\n\nZero-shot learning\n\nContrastive learning\n\nDeep learning\n\nCLIP\n\nBiology\n\nEcology\n\nImage data\n\nLLM\n\nHugging Face\n\n\n\n\n\n\n\nEdward Vendrow\n\n\n2025-03-04\n\n\n\n\n\n\n\n\n\n\n\n\n\nCIFAR Dataset\n\n\n\nData\n\nImage data\n\nComputer vision\n\nImage classification\n\nCNN\n\nCIFAR\n\n\n\n\n\n\n\nAidan O’Brien\n\n\n2025-03-03\n\n\n\n\n\n\n\n\n\n\n\n\n\nLearning Through Comparison: Use Cases of Contrastive Learning\n\n\n\nVideos\n\nML+X\n\nUW-Madison\n\nDeep learning\n\nContrastive learning\n\nClustering\n\nOOD detection\n\nMultimodal learning\n\nTrustworthy AI\n\nRepresentation learning\n\nCIFAR\n\n\n\n\n\n\n\nYin Li, Chris Endemann\n\n\n2025-02-10\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlphaFold and Protein Language Models\n\n\n\nVideos\n\nComBEE\n\nUW-Madison\n\nBiology\n\nProtein engineering\n\nLLM\n\nDeep learning\n\nAlphafold\n\nTransformer\n\nProtein language models\n\n\n\n\n\n\n\nHannah Wayment-Steele\n\n\n2025-01-28\n\n\n\n\n\n\n\n\n\n\n\n\n\nUW Generative AI Services & Policies\n\n\n\nGenAI\n\nMicrosoft Copilot\n\nGemini\n\nWebex\n\nZoom\n\nUW-Madison\n\nLLM\n\nFoundation models\n\n\n\n\n\n\n\nChris Endemann\n\n\n2025-01-07\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Statistical Learning\n\n\n\nBooks\n\nStatistical learning\n\nClassical ML\n\nRegression\n\nClassification\n\nDeep learning\n\nDecision trees\n\nUnsupervised Learning\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2025-01-06\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne Useful Thing\n\n\n\nBlogs\n\nIndustry applications\n\nLLM\n\nEthical AI\n\nGenAI\n\n\n\n\n\n\n\nAlan Ng\n\n\n2024-12-12\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotebookLM: A GenAI Summarization Tool\n\n\n\nGenAI\n\nNLP\n\nSummarization\n\nGemini\n\nLLM\n\nFoundation models\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-12-09\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrustworthy AI - Explainability, Bias, Fairness, and Safety\n\n\n\nWorkshops\n\nTrustworthy AI\n\nEthical AI\n\nExplainability\n\nFairness\n\nBias\n\nOOD detection\n\nAIF360\n\nPyTorch-OOD\n\nAnomaly detection\n\nGradCAM\n\nTabular\n\nComputer vision\n\nNLP\n\nText analysis\n\nCode-along\n\nCarpentries\n\nDeep learning\n\nModel sharing\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-12-03\n\n\n\n\n\n\n\n\n\n\n\n\n\nAI Fairness 360 (AIF360)\n\n\n\nLibraries\n\nAIF360\n\nFairness\n\nEthical AI\n\nTrustworthy AI\n\nBias\n\nSklearn\n\nTabular\n\nComputer vision\n\nNLP\n\nText analysis\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-12-03\n\n\n\n\n\n\n\n\n\n\n\n\n\nPyTorch-OOD\n\n\n\nLibraries\n\nPyTorch-OOD\n\nOOD detection\n\nPyTorch\n\nAnomaly detection\n\nNovelty detection\n\nOpen-set recognition\n\nTrustworthy AI\n\nEthical AI\n\nDeep learning\n\nComputer vision\n\nNLP\n\nTabular\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-12-03\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to AWS SageMaker for Predictive ML/AI\n\n\n\nWorkshops\n\nCode-along\n\nCarpentries\n\nCompute\n\nAWS\n\nGPU\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-11-07\n\n\n\n\n\n\n\n\n\n\n\n\n\nAI for Music and the Humanities\n\n\n\nVideos\n\nML+X\n\nUW-Madison\n\nDeep learning\n\nGenAI\n\nConformer\n\nHumanities\n\nAudio search\n\nMusic\n\nCSI\n\nSignal processing\n\nSpectral analysis\n\nLLM\n\n\n\n\nWhat Tune Is That: A Humanities Application of Deep Learning — Alan Ng \nFake Artists, Fake Listeners: AI and the Music Industries — Jeremy Morris, PhD\n\n\n\n\nAlan Ng, Jeremy Morris\n\n\n2024-11-05\n\n\n\n\n\n\n\n\n\n\n\n\n\nKornia\n\n\n\nLibraries\n\nModel exploration\n\nComputer vision\n\nDeep learning\n\nPyTorch\n\nImage preprocessing\n\nImage segmentation\n\nViT\n\nSAM\n\nLoFTR\n\nRT-DETR\n\n\n\n\n\n\n\nRadi Akbar\n\n\n2024-10-29\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeaf Vein Dataset (LVD2021)\n\n\n\nData\n\nImage data\n\nPlant phenotyping\n\nImage segmentation\n\nComputer vision\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-10-18\n\n\n\n\n\n\n\n\n\n\n\n\n\nXGBoost: Tree-Based Gradient Boosting for Tabular Data\n\n\n\nModels\n\nBoosting\n\nXGBoost\n\nDecision trees\n\nTabular\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-10-17\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject Gutenberg: Text & Audio Books\n\n\n\nData\n\nText data\n\nAudio data\n\nMultimodal data\n\nNLP\n\nText analysis\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-10-14\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Electronic Health Record Data to Predict Deterioriation in Hospitalized Children\n\n\n\nVideos\n\nML4MI\n\nUW-Madison\n\nHealthcare\n\nNLP\n\nText analysis\n\nEHR\n\nBoosting\n\nDecision trees\n\n\n\n\n\n\n\nAnoop Mayampurath\n\n\n2024-10-14\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring the Titanic Dataset\n\n\n\nNotebooks\n\nEDA\n\nTabular\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-10-07\n\n\n\n\n\n\n\n\n\n\n\n\n\nU-Net: Convolutional Networks for Biomedical Image Segmentation\n\n\n\nModels\n\nDeep learning\n\nMedical imaging\n\nImage segmentation\n\nCNN\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-09-16\n\n\n\n\n\n\n\n\n\n\n\n\n\nVision, Language, and Vision-Language Modeling in Radiology\n\n\n\nVideos\n\nML4MI\n\nUW-Madison\n\nMedical imaging\n\nHealthcare\n\nVLM\n\nViT\n\nUNET\n\nLLaVA\n\nComputer vision\n\nCNN\n\nLLM\n\nDeep learning\n\nMultimodal learning\n\n\n\n\n\n\n\nTyler Bradshaw\n\n\n2024-09-16\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat Tune Is That? A Humanities Application of Deep Learning\n\n\n\nAudio search\n\nBlogs\n\nDeep learning\n\nConformer\n\nTransformer\n\nCNN\n\nHumanities\n\nAudio data\n\nMusic\n\nPyTorch\n\nCSI\n\nTime-series\n\nSignal processing\n\nSpectral analysis\n\n\n\n\n\n\n\nAlan Ng\n\n\n2024-09-11\n\n\n\n\n\n\n\n\n\n\n\n\n\nMONAI: Medical Open Network for AI\n\n\n\nLibraries\n\nDeep learning\n\nPyTorch\n\nMedical imaging\n\nModel exploration\n\n\n\n\n\n\n\nAlan McMillan\n\n\n2024-08-14\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrokking\n\n\n\nVideos\n\nDeep learning\n\nEmpirical patterns\n\nGrokking\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Python\n\n\n\nWorkshops\n\nPython\n\nCarpentries\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Machine Learning with Sklearn\n\n\n\nWorkshops\n\nLibraries\n\nClassical ML\n\nBoosting\n\nDecision trees\n\nSVM\n\nClustering\n\nRegression\n\nSklearn\n\nCode-along\n\nCarpentries\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-17\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Deep Learning with Keras\n\n\n\nWorkshops\n\nLibraries\n\nDeep learning\n\nKeras\n\nCode-along\n\nCarpentries\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-16\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Deep Learning with PyTorch\n\n\n\nWorkshops\n\nLibraries\n\nDeep learning\n\nPyTorch\n\nUdacity\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-15\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Deep Learning\n\n\n\nBooks\n\nDeep learning\n\nPyTorch\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-14\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Text Analysis / NLP\n\n\n\nWorkshops\n\nDeep learning\n\nHugging Face\n\nText analysis\n\nNLP\n\nLLM\n\nCode-along\n\nCarpentries\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-13\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview of Reproducibility Lecture\n\n\n\nVideos\n\nReproducibility\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\n\nVersion Control with GitHub Desktop\n\n\n\nGuides\n\nReproducibility\n\nGit/GitHub\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-11\n\n\n\n\n\n\n\n\n\n\n\n\n\nVersion Control with Git and GitHub (Carpentries)\n\n\n\nWorkshops\n\nVideos\n\nReproducibility\n\nGit/GitHub\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-11\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Out-of-Distribution Detection\n\n\n\nVideos\n\nICCV\n\nOOD detection\n\nTrustworthy AI\n\n\n\n\n\n\n\nSharon Li\n\n\n2024-07-11\n\n\n\n\n\n\n\n\n\n\n\n\n\nCenter for High Throughput Computing (CHTC)\n\n\n\nCompute\n\nGPU\n\nCHTC\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-06-25\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Contribute?\n\n\n\nGuides\n\nContribute\n\n\n\n\n\n\n\nML+X\n\n\n2024-06-24\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Biophysics-based Protein Language Model for Protein Engineering\n\n\n\nVideos\n\nCross Labs AI\n\nUW-Madison\n\nTransfer learning\n\nBiology\n\nBiophysics\n\nProtein language models\n\nFoundation models\n\nLLM\n\nDeep learning\n\nProtein engineering\n\nSimulations\n\n\n\nWe introduce Mutational Effect Transfer Learning (METL), a specialized protein language model that bridges the gap between traditional biophysics-based and machine learning…\n\n\n\nSam Gelman\n\n\n2024-06-18\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Large Language Models for Meteorological Fact Finding\n\n\n\nVideos\n\nIT Prof\n\nUW-Madison\n\nLLM\n\nMeteorology\n\n\n\nThis talk demonstrates harnessing the power of AI to open new avenues in data analysis, including for meteorological fact-finding. Discover how cutting-edge large language…\n\n\n\nZekai Otles\n\n\n2024-05-30\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrustworthy LLMs & Ethical AI\n\n\n\nVideos\n\nML+X\n\nUW-Madison\n\nNLP\n\nText analysis\n\nLLM\n\nDeep learning\n\nTrustworthy AI\n\nDeTox\n\nEthical AI\n\nTopic modeling\n\n\n\n\nDeTox: Denoised Toxic Embeddings for Editing Model Toxicity — Rheeya Uppaal\nA Project on AI Ethics — Mariab A. Knowles\n\n\n\n\nRheeya Uppaal, Mariah A. Knowles\n\n\n2024-05-11\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Model Sharing in the Age of Foundation Models\n\n\n\nVideos\n\nML+X\n\nUW-Madison\n\nMultimodal learning\n\nFoundation models\n\nModel sharing\n\nHugging Face\n\nLLM\n\nLMM\n\nLLaVA\n\nDeep learning\n\n\n\n\nModel sharing and reproducible ML — Chris Endemann\nLLaVA-NeXT and model sharing — Haotian Liu, PhD\n\n\n\n\nChris Endemann, Haotian Liu\n\n\n2024-03-12\n\n\n\n\n\n\n\n\n\n\n\n\n\nNavigating Gravitational Waves with AI Insights\n\n\n\nVideos\n\nML+X\n\nUW-Madison\n\nPhysics\n\nSimulations\n\nSpectral analysis\n\nSignal processing\n\n\n\n\nWelcome and small group discussions — Chris Endemann\nClassifying gravitational wave modes from core-collapse supernovae — Bella Finkel\n\n\n\n\nChris Endemann, Bella Finkel\n\n\n2024-02-13\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Science Communication and Drug Synergy Analysis using GPT\n\n\n\nVideos\n\nML+X\n\nUW-Madison\n\nScience communication\n\nHealthcare\n\nDrug synergy\n\nLLM\n\nText mining\n\nNLP\n\n\n\n\n\n\n\nBen Rush, Jack Freeman\n\n\n2023-12-12\n\n\n\n\n\n\n\n\n\n\n\n\n\nLabelBench: A Framework for Benchmarking Label-Efficient Learning\n\n\n\nVideos\n\nMLOPT\n\nUW-Madison\n\nActive learning\n\nLabel-efficient learning\n\nActive learning\n\nSemi-supervised\n\nViT\n\nBenchmarking\n\n\n\n\n\n\n\nJifan Zhang\n\n\n2023-10-27\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime-Series Analysis\n\n\n\nVideos\n\nML+X\n\nUW-Madison\n\nTime-series\n\nGenomics\n\nBiology\n\nHealthcare\n\n\n\n\n\n\n\nPeng Jiang, Sourav Pal\n\n\n2023-10-10\n\n\n\n\n\n\n\n\n\n\n\n\n\nBiomedical AI Research and Applications\n\n\n\nVideos\n\nUW-Madison\n\nExploring AI@UW\n\nBiology\n\nHealthcare\n\nLLM\n\nComputer vision\n\nMultimodal learning\n\n\n\n\n\n\n\nAnthony Gitter, Junjie Hu, Yin Li, Anoop Mayampurath, Vikas Singh\n\n\n2023-09-22\n\n\n\n\n\n\n\n\n\n\n\n\n\nRacism in AI\n\n\n\nVideos\n\nUW-Madison\n\nExploring AI@UW\n\nEthical AI\n\nBias\n\nFairness\n\n\n\n\n\n\n\nLori Kido Lopez\n\n\n2023-09-22\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultimodal Learning\n\n\n\nVideos\n\nML+X\n\nUW-Madison\n\nMultimodal learning\n\nDeep learning\n\nComputer vision\n\nHealthcare\n\nGenomics\n\n\n\n\n\n\n\nDaifeng Wang, Zachary Huemann, Pedro Morgado\n\n\n2023-09-19\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Generative AI: An Introduction to Large Language Models and Diffusion Models\n\n\n\nVideos\n\nML4MI\n\nUW-Madison\n\nLLM\n\nVLM\n\nLLaVA\n\nLLM\n\nGenAI\n\nDiffusion\n\nDeep learning\n\nMultimodal learning\n\n\n\n\n\n\n\nKangwook Lee\n\n\n2023-09-11\n\n\n\n\n\n\n\n\n\n\n\n\n\nAI and Medical Imaging\n\n\n\nVideos\n\nExploring AI@UW\n\nUW-Madison\n\nHealthcare\n\nMedical imaging\n\nImage classification\n\nComputer vision\n\nDeep learning\n\n\n\n\n\n\n\nLaurel Belman, Todd Shechter, Alan McMillan\n\n\n2023-08-11\n\n\n\n\n\n\n\n\n\n\n\n\n\nAI in Action: Intelligent Systems and Business Operations\n\n\n\nVideos\n\nUW-Madison\n\nExploring AI@UW\n\nBusiness\n\nBias\n\nEthical AI\n\n\n\n\n\n\n\nLaura Albert\n\n\n2023-07-28\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnifying Audio-Visual Machine Perception – Tasks & Architectures\n\n\n\nVideos\n\nML4MI\n\nUW-Madison\n\nHealthcare\n\nMultimodal learning\n\nContrastive learning\n\nPerception\n\nEarly-fusion\n\nAutoencoder\n\n\n\n\n\n\n\nPedro Morgado\n\n\n2023-07-12\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "Contributor-templates/template_application-video.html",
    "href": "Contributor-templates/template_application-video.html",
    "title": "My Amazing ML Talk",
    "section": "",
    "text": "Provide a short description of the talk and why you think it may be useful to others (4-8 sentences). Example: In this talk from the Machine Learning for Medical Imaging (ML4MI) community, Anoop Mayampurath (PhD) discusses the use of electronic health record (EHR) data and machine learning to predict clinical deterioration in hospitalized children. The presentation explores how traditional methods like the Pediatric Early Warning System (PEWS) fall short and introduces a novel model, pCART, which significantly improves outcomes by enabling early and accurate detection of at-risk patients. pCART (Pediatric Cardiac Arrest Risk Tool) is a gradient boosted tree model designed to identify clinical deterioration in hospitalized children, particularly those at risk of requiring ICU transfers. Unlike traditional methods like the Pediatric Early Warning System (PEWS), which rely on static, age-dependent cutoffs and subjective assessments, pCART utilizes advanced analytics and continuous tracking to provide a more accurate and actionable risk assessment.\n\n. If video is not on Youtube (e.g., Kaltura instead), use the following language: “A netID is required to view ML4MI videos: View YYYY-MM-DD recording.”"
  },
  {
    "objectID": "Contributor-templates/template_application-video.html#see-also",
    "href": "Contributor-templates/template_application-video.html#see-also",
    "title": "My Amazing ML Talk",
    "section": "See also",
    "text": "See also\n\n\nRelated Resource 1: Brief description of related resource 1.\nRelated Resource 2: Brief description of related resource 2.\nRelated Resource 3: Brief description of related resource 3 \nML4MI: Explore other talks from the ML4MI group at UW-Madison."
  },
  {
    "objectID": "Contributor-templates/template_learn-video.html",
    "href": "Contributor-templates/template_learn-video.html",
    "title": "Title/Topic of Resource",
    "section": "",
    "text": "Brief description of the resource, including a text embedded link in the first 1-2 sentences. Explain what the resource covers and its relevance. Mention any specific features, strengths, or weaknesses. This section should help potential users understand the value of the resource and what they can expect to learn or achieve by using it.\n\n\n\n\nPrerequisite Resource 1\nPrerequisite Resource 2"
  },
  {
    "objectID": "Contributor-templates/template_learn-video.html#about-this-resource",
    "href": "Contributor-templates/template_learn-video.html#about-this-resource",
    "title": "Title/Topic of Resource",
    "section": "",
    "text": "Brief description of the resource, including a text embedded link in the first 1-2 sentences. Explain what the resource covers and its relevance. Mention any specific features, strengths, or weaknesses. This section should help potential users understand the value of the resource and what they can expect to learn or achieve by using it.\n\n\n\n\nPrerequisite Resource 1\nPrerequisite Resource 2"
  },
  {
    "objectID": "Contributor-templates/template_learn-video.html#questions",
    "href": "Contributor-templates/template_learn-video.html#questions",
    "title": "Title/Topic of Resource",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Contributor-templates/template_learn-video.html#see-also",
    "href": "Contributor-templates/template_learn-video.html#see-also",
    "title": "Title/Topic of Resource",
    "section": "See also",
    "text": "See also\n\n\nRelated Resource 1: Brief description of related resource 1.\nRelated Resource 2: Brief description of related resource 2.\nRelated Resource 3: Brief description of related resource 3."
  },
  {
    "objectID": "Contributor-templates/template_toolbox-model.html",
    "href": "Contributor-templates/template_toolbox-model.html",
    "title": "Model Name",
    "section": "",
    "text": "[Model Name] is a [brief model description: purpose and key characteristics]. Introduced in [Year] by [Author(s)/Organization] in the paper “Model Paper Title,” [Model Name] has become a widely used model for [specific task(s), e.g., image classification, text generation]. Its [key architecture/approach: transformer, decision trees, etc.] allows it to excel in [key domains/tasks]. It is now used across domains, from [example application 1] to [example application 2].\n\n\n\nArchitecture: Description of model’s architecture, e.g., CNN, Transformer, Gradient Boosting\n\nFor vision models: Can describe use of convolutional layers, attention mechanisms, etc.\nFor NLP models: Can describe tokenization, transformer-based layers, etc.\nFor tabular models: Can describe decision trees, boosting mechanisms, etc.\n\nFeature 2: Describe another feature, e.g., skip connections, attention, gradient boosting\nData Efficiency: [Mention if the model is effective with small datasets or if it requires large-scale data"
  },
  {
    "objectID": "Contributor-templates/template_toolbox-model.html#about-this-resource",
    "href": "Contributor-templates/template_toolbox-model.html#about-this-resource",
    "title": "Model Name",
    "section": "",
    "text": "[Model Name] is a [brief model description: purpose and key characteristics]. Introduced in [Year] by [Author(s)/Organization] in the paper “Model Paper Title,” [Model Name] has become a widely used model for [specific task(s), e.g., image classification, text generation]. Its [key architecture/approach: transformer, decision trees, etc.] allows it to excel in [key domains/tasks]. It is now used across domains, from [example application 1] to [example application 2].\n\n\n\nArchitecture: Description of model’s architecture, e.g., CNN, Transformer, Gradient Boosting\n\nFor vision models: Can describe use of convolutional layers, attention mechanisms, etc.\nFor NLP models: Can describe tokenization, transformer-based layers, etc.\nFor tabular models: Can describe decision trees, boosting mechanisms, etc.\n\nFeature 2: Describe another feature, e.g., skip connections, attention, gradient boosting\nData Efficiency: [Mention if the model is effective with small datasets or if it requires large-scale data"
  },
  {
    "objectID": "Contributor-templates/template_toolbox-model.html#timeline-context",
    "href": "Contributor-templates/template_toolbox-model.html#timeline-context",
    "title": "Model Name",
    "section": "Timeline context",
    "text": "Timeline context\n[Model Name] fits into the broader development of models for [task/domain]. Here is a timeline placing [Model Name] in the context of other important models.\n\nModel Predecessor 1 (Year): Short description of relevant model\nModel Predecessor 2 (Year): Short description of relevant model\nModel Name (Year): Short description of the current model and its contribution\nMore Recent Models (Year): Additional models for comparison"
  },
  {
    "objectID": "Contributor-templates/template_toolbox-model.html#model-name-variants",
    "href": "Contributor-templates/template_toolbox-model.html#model-name-variants",
    "title": "Model Name",
    "section": "[Model Name] Variants",
    "text": "[Model Name] Variants\n\nVariant 1 Name: Description of the variant and its improvements/changes over the base model\n\nFor models like XGBoost: Could include different hyperparameter settings or regularization options.\nFor vision models: Could include variants with attention mechanisms or depth changes.\nFor NLP models: Could include pretrained vs fine-tuned versions.\n\nVariant 2 Name: Description of another variant\nPopular Version: E.g., nnU-Net for U-Net, or larger CLIP models"
  },
  {
    "objectID": "Contributor-templates/template_toolbox-model.html#model-playground",
    "href": "Contributor-templates/template_toolbox-model.html#model-playground",
    "title": "Model Name",
    "section": "Model playground",
    "text": "Model playground\n\nTutorials and Getting Started Notebooks\n\nModel-specific tutorial/notebook: Link to official tutorials, Colab notebooks, or GitHub repositories for getting started with the model.\n\n\n\nHigh-level tips for effective use\n\nPre-trained Models: Guidance on using pre-trained models, if applicable\nRegularization Techniques: Suggestions for preventing overfitting, especially with smaller datasets\n\nFor models like XGBoost: Feature regularization, early stopping\nFor deep learning: Dropout, weight decay\n\nData Augmentation: Best practices for augmentation\n\nFor vision models: Flipping, rotation, color jitter\nFor NLP: Data augmentation techniques like backtranslation\n\nLoss Function: Optimizing loss functions for the specific task\nArchitectural Adjustments: Guidance on modifying the architecture based on dataset size, task complexity\n\n\n\nRelated datasets & challenges\n\nRelevant Dataset 1: Provide a relevant dataset for training or benchmarking the model\n\nVision: COCO, ImageNet\nNLP: Common Crawl, Hugging Face datasets\nTabular: Kaggle, UCI ML Repository\n\nRelated Challenge 1: Link to a Kaggle challenge or relevant benchmark competition"
  },
  {
    "objectID": "Contributor-templates/template_toolbox-model.html#questions",
    "href": "Contributor-templates/template_toolbox-model.html#questions",
    "title": "Model Name",
    "section": "Questions?",
    "text": "Questions?\nIf you have any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Contributor-templates/template_toolbox-model.html#see-also",
    "href": "Contributor-templates/template_toolbox-model.html#see-also",
    "title": "Model Name",
    "section": "See also",
    "text": "See also\n\n\nRelated Resource 1: Brief description of related resource 1.\nRelated Resource 2: Brief description of related resource 2.\nRelated Resource 3: Brief description of related resource 3"
  },
  {
    "objectID": "Contributor-templates/template_learn-book.html",
    "href": "Contributor-templates/template_learn-book.html",
    "title": "Title/Topic of Resource",
    "section": "",
    "text": "Brief description of the resource, including a text embedded link in the first 1-2 sentences. Explain what the resource covers and its relevance. Mention any specific features, strengths, or weaknesses. This section should help potential users understand the value of the resource and what they can expect to learn or achieve by using it.\n\n\n\n\n\n\n\nPrerequisite Resource 1\nPrerequisite Resource 2"
  },
  {
    "objectID": "Contributor-templates/template_learn-book.html#about-this-resource",
    "href": "Contributor-templates/template_learn-book.html#about-this-resource",
    "title": "Title/Topic of Resource",
    "section": "",
    "text": "Brief description of the resource, including a text embedded link in the first 1-2 sentences. Explain what the resource covers and its relevance. Mention any specific features, strengths, or weaknesses. This section should help potential users understand the value of the resource and what they can expect to learn or achieve by using it.\n\n\n\n\n\n\n\nPrerequisite Resource 1\nPrerequisite Resource 2"
  },
  {
    "objectID": "Contributor-templates/template_learn-book.html#questions",
    "href": "Contributor-templates/template_learn-book.html#questions",
    "title": "Title/Topic of Resource",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Contributor-templates/template_learn-book.html#see-also",
    "href": "Contributor-templates/template_learn-book.html#see-also",
    "title": "Title/Topic of Resource",
    "section": "See also",
    "text": "See also\n\n\nRelated Resource 1: Brief description of related resource 1.\nRelated Resource 2: Brief description of related resource 2.\nRelated Resource 3: Brief description of related resource 3."
  },
  {
    "objectID": "Contributor-templates/template_learn.html",
    "href": "Contributor-templates/template_learn.html",
    "title": "Title/Topic of Resource",
    "section": "",
    "text": "Brief description of the resource, including a text embedded link in the first 1-2 sentences. Explain what the resource covers and its relevance. Mention any specific features, strengths, or weaknesses. This section should help potential users understand the value of the resource and what they can expect to learn or achieve by using it.\n\n\n\n\nPrerequisite Resource 1\nPrerequisite Resource 2"
  },
  {
    "objectID": "Contributor-templates/template_learn.html#about-this-resource",
    "href": "Contributor-templates/template_learn.html#about-this-resource",
    "title": "Title/Topic of Resource",
    "section": "",
    "text": "Brief description of the resource, including a text embedded link in the first 1-2 sentences. Explain what the resource covers and its relevance. Mention any specific features, strengths, or weaknesses. This section should help potential users understand the value of the resource and what they can expect to learn or achieve by using it.\n\n\n\n\nPrerequisite Resource 1\nPrerequisite Resource 2"
  },
  {
    "objectID": "Contributor-templates/template_learn.html#questions",
    "href": "Contributor-templates/template_learn.html#questions",
    "title": "Title/Topic of Resource",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Contributor-templates/template_learn.html#see-also",
    "href": "Contributor-templates/template_learn.html#see-also",
    "title": "Title/Topic of Resource",
    "section": "See also",
    "text": "See also\n\n\nRelated Resource 1: Brief description of related resource 1.\nRelated Resource 2: Brief description of related resource 2.\nRelated Resource 3: Brief description of related resource 3."
  },
  {
    "objectID": "insights/plot_insights.html",
    "href": "insights/plot_insights.html",
    "title": "Nexus: Crowdsourced ML Resources",
    "section": "",
    "text": "import os\nimport json\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport pandas as pd\n\n# Directory containing the JSON files\ndirectory = \"./\"\n\n# Dictionary to store the most recent data for each date\nlatest_data = {}\n\n# Loop through all files in the directory\nfor filename in sorted(os.listdir(directory)):\n    if filename.startswith(\"views_summary_\") and filename.endswith(\".json\"):\n        file_path = os.path.join(directory, filename)\n        with open(file_path, 'r') as file:\n            data = json.load(file)\n            for daily_entry in data['daily_breakdown']:\n                date = daily_entry['date']\n                count = daily_entry['count']\n                uniques = daily_entry['uniques']\n                # Update with more recent data if necessary\n                if date not in latest_data or latest_data[date]['file'] &lt; filename:\n                    latest_data[date] = {\n                        'count': count,\n                        'uniques': uniques,\n                        'file': filename\n                    }\n\n# Extract the consolidated data\nall_dates = []\nall_counts = []\nall_uniques = []\n\nfor date, values in latest_data.items():\n    all_dates.append(date)\n    all_counts.append(values['count'])\n    all_uniques.append(values['uniques'])\n\n# Convert data into a DataFrame for easier handling\ndf = pd.DataFrame({\n    \"date\": [datetime.strptime(date, \"%Y-%m-%dT%H:%M:%SZ\") for date in all_dates],\n    \"count\": all_counts,\n    \"uniques\": all_uniques\n})\n\n# Sort by date to ensure the plot is in chronological order\ndf = df.sort_values(\"date\")\n\n# Add a rolling average calculation\nrolling_window = 7  # Adjustable rolling window size\ndf[\"uniques_rolling\"] = df[\"uniques\"].rolling(window=rolling_window).mean()\n\n# Plot the data\nplt.figure(figsize=(10, 6))\nplt.plot(df[\"date\"], df[\"uniques\"], label=\"Uniques\", marker='x', linestyle='-', alpha=0.7)\nplt.plot(df[\"date\"], df[\"uniques_rolling\"], label=f\"{rolling_window}-Day Rolling Average\", linewidth=2)\nplt.xlabel(\"Date\")\nplt.ylabel(\"Uniques\")\nplt.title(\"Uniques Over Time with Rolling Average\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "Learn/Notebooks/index.html",
    "href": "Learn/Notebooks/index.html",
    "title": "Notebooks",
    "section": "",
    "text": "This section features short, runnable notebooks designed to help you explore specific ML/AI concepts, tools, and workflows. Many can be launched directly via Google Colab using the “Open in Colab” button included at the top of each page.\nThese notebooks are meant to be lightweight, interactive learning materials. Some may eventually evolve into full workshops using the Carpentries workbench template, but each stands on its own as a handy introduction to key ML/AI ideas.\nHave a notebook that helped you or your team learn something new? We welcome community contributions! Submit your notebook to help others learn and explore ML/AI more effectively.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Fact-Based QA with RAG: Romeo and Juliet\n\n\n\nNotebooks\n\nRAG\n\nRetrieval\n\nNLP\n\nLLM\n\nEmbeddings\n\nText analysis\n\nDeep learning\n\nPrompt engineering\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2025-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring the Titanic Dataset\n\n\n\nNotebooks\n\nEDA\n\nTabular\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-10-07\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Learn",
      "Notebooks"
    ]
  },
  {
    "objectID": "Learn/Guides/Github-desktop.html",
    "href": "Learn/Guides/Github-desktop.html",
    "title": "Version Control with GitHub Desktop",
    "section": "",
    "text": "Navigating the world of version control systems like Git can initially feel daunting, especially for those new to programming or collaborative software development projects. However, with the right tools and guidance, anyone can quickly grasp the essentials and begin leveraging the power of Git for efficient project management and collaboration. While Git commands can be run via a Unix shell, there are alternatives which are more friendly for beginners. In this guide, we’ll explore GitHub Desktop as a convenient and accessible gateway to Git, along with a step-by-step walkthrough on essential Git terminology, setup procedures, tracking changes, collaboration workflows, and even managing Kaggle notebooks seamlessly. Whether you’re embarking on your first coding adventure or seeking to streamline your team’s development process, this guide aims to demystify Git and empower you with practical knowledge to navigate the Git landscape with confidence."
  },
  {
    "objectID": "Learn/Guides/Github-desktop.html#about-this-resource",
    "href": "Learn/Guides/Github-desktop.html#about-this-resource",
    "title": "Version Control with GitHub Desktop",
    "section": "",
    "text": "Navigating the world of version control systems like Git can initially feel daunting, especially for those new to programming or collaborative software development projects. However, with the right tools and guidance, anyone can quickly grasp the essentials and begin leveraging the power of Git for efficient project management and collaboration. While Git commands can be run via a Unix shell, there are alternatives which are more friendly for beginners. In this guide, we’ll explore GitHub Desktop as a convenient and accessible gateway to Git, along with a step-by-step walkthrough on essential Git terminology, setup procedures, tracking changes, collaboration workflows, and even managing Kaggle notebooks seamlessly. Whether you’re embarking on your first coding adventure or seeking to streamline your team’s development process, this guide aims to demystify Git and empower you with practical knowledge to navigate the Git landscape with confidence."
  },
  {
    "objectID": "Learn/Guides/Github-desktop.html#getting-started",
    "href": "Learn/Guides/Github-desktop.html#getting-started",
    "title": "Version Control with GitHub Desktop",
    "section": "Getting started",
    "text": "Getting started\n\nVersion control\nVersion control is system that records changes to a file or set of files over time so that you can recall specific versions later. It helps in managing changes, keeping track of different versions, and collaborating with multiple people. Version control is an essential tool for reproducible science and software systems that can improve over time.\n\n\nGit\nGit is like the programming language of version control. Just as Python gives you commands to manipulate data, Git gives you commands to manage versions of your code and text files. It helps you “save” your work (commits), jump back to earlier versions (like undo), and experiment on different branches without breaking your main code. Git is distributed, meaning every user has a full copy of the project’s history. That makes it easy to collaborate, even offline, and avoid accidentally overwriting each other’s work. It’s the tool that powers platforms like GitHub, which provide a user-friendly way to share and collaborate on Git-tracked projects. More at git-scm.com\n\n\nGitHub\nGitHub or github.com is a web-based platform that uses Git for version control. It provides a collaborative environment where users can host and review code, manage projects, and build software alongside millions of other developers. GitHub also offers additional features such as issue tracking, project management tools, and continuous integration (CI) workflows, which automate the process of testing and integrating new code to ensure that changes don’t break the project. As the most popular Git-based platform, GitHub has a larger user base and ecosystem, often making it the go-to choice for development collaboration.\n\n\nOther platforms using Git\nWhile GitHub is the most widely used, other platforms also use Git for version control and may be preferred in certain contexts:\nGitLab: Known for its privacy and security features, GitLab is often favored for organization-wide code repositories, especially in enterprise environments that require tighter control over their codebase. While GitHub also offers robust CI tools, GitLab’s features make it a strong option for managing both public and private repositories across large organizations.\nBitbucket: Another Git-based repository hosting service that integrates well with Atlassian products like Jira. While less popular than GitHub, Bitbucket is commonly used by teams that rely on the Atlassian ecosystem and need strong project management integration.\n\n\nGitHub Desktop\nGitHub Desktop is a graphical user interface (GUI) application that simplifies the use of Git and GitHub. It is designed for users who prefer not to use the command line interface, offering a more intuitive and visual approach to version control. With GitHub Desktop, you can easily perform common Git tasks such as committing changes, creating branches, and resolving merge conflicts, all within a user-friendly interface.\n\n\nEssential terminology\nBecoming familiar with version control terminology is half the battle in becoming fluent in Git/GitHub. Study the terms below to become better acquainted and revisit as needed. We’ll refer to these terms often throughout this guide.\n\nRepository == repo: A project that is tracked via git/GitHub\n\nRemote repo: A git project that is stored on GitHub\nLocal repo: A git project that has been downloaded to your local machine\n\nClone: Cloning is the process of making a copy of a remote repo on your local machine. This allows you to work on the project locally and perform tasks like commits, branches, and pulls.\nCommit: A git command that marks the completion of new work to a repo (e.g., add a new script, add a feature, fill out README). You can always recover previous versions of your work by loading up a previous commit.\nPush: A git command that sends local changes (commits) stored in your local repo to the remote repo.\nPull: A git command that allows you to update your local repo based on changes made to the remote repo (e.g., if your colleague pushes to the remote repo)\nBranch: A branch in Git is a parallel line of development that allows you to work on features, bug fixes, or experiments without affecting the main codebase. You can create and switch between branches to isolate your work.\nMain branch: The default branch in a git repository where the final, stable version of the project is maintained. It typically contains production-ready code, and new features or changes from other branches are merged into the main branch after being tested and reviewed. Historically, this branch was called master, but in recent years, there has been a shift to using main as the default name. This change reflects a move towards more inclusive language, as the term “master” can carry connotations of oppression, and many platforms, like GitHub, have adopted main to encourage more thoughtful and neutral terminology in coding environments.\nMerge: Merging is the process of integrating changes from one branch into another. This is typically done to combine the changes made in a feature branch with the main branch.\nPull Request (PR): A pull request is a feature provided by platforms like GitHub, GitLab, and Bitbucket. It’s a way to propose changes (commits) to a project. Others can review the changes, and once approved, they can be merged into the main branch. If it’s easier to remember, you can think of this as a “merge request”, which is actually the terminiology GitLab uses.\nFork: Forking a repository means creating a copy of someone else’s project in your GitHub account. This allows you to make changes independently and propose those changes back to the original project via pull requests. If everyone on your team has write-access to the repo, it’s best to use new branches instead of forks for pull requests.\nGitignore: A .gitignore file is used to specify which files and directories should be excluded from version control. It’s essential for preventing unnecessary or sensitive files (contains like API keys) from being included in the repository."
  },
  {
    "objectID": "Learn/Guides/Github-desktop.html#setup",
    "href": "Learn/Guides/Github-desktop.html#setup",
    "title": "Version Control with GitHub Desktop",
    "section": "Setup",
    "text": "Setup\nIn this section, we will walk you through setting up a GitHub repository for a collaborative software project. As an example case, imagine you and your team are preparing for a Kaggle competition and need a streamlined way to manage your code, track changes, and collaborate efficiently. By following the steps below, you’ll learn how to create a new repository, add collaborators, set up secure access, and clone the repository to your local machine, ensuring everyone on your team is ready to contribute seamlessly.\n\nInstall GitHub Desktop\n\nVisit https://desktop.github.com/ to install\n\nCreate new repository or “repo”\n\nVisit https://github.com/ and sign in to your GitHub account (or create an account)\nClick the green “new” button to create a new repo\nProvide a name for the project, e.g., “my_kaggle_project”\nGive a description: “Git repo for collaborating on Kaggle project for MLM24.”\nSet to private if you’re worried about having your work scooped. Otherwise set to public.\nAdd a README file: best practice is to include a README file that explains how to use your code/repo\nChoose a license: https://choosealicense.com/. MIT license is usually best for open-source projects.\n\nAdd collaborator(s)\n\nFrom your repo homepage on GitHub, click the settings tab\nClick on the “Collaborators” menu option shown in the left panel\nClick “Add people” and enter your collaborator’s username or GitHub email address\n\nSetup SSH key: SSH provides a secure way to authenticate and transfer data between your local machine and GitHub. You can also use HTTPS if you prefer. HTTPS avoids having to generate an SSH key, but you may need to enter your GitHub login credentials from time to time.\n\nOpen GitBash (windows) or terminal (Mac) and run the following commands replacing the example email with your GitHub email:\n\nssh-keygen -t ed25519 -C “your_github_email@address.com”\ncat ~/.ssh/id_ed25519.pub\nThe ssh-keygen produces private and public keys, and make sure to copy and paste the output from the command\ncat ~/.ssh/id_ed25519.pub\n\nPaste output (starts like ssh-ed25519) into the new SSH key under GitHub settings (SSH and GPG Keys) and save the key\n\nClone repo\n\nFrom your GitHub repo homepage, click the green “Code” button\nSelect SSH if you setup an SSH key or select HTTPS if you don’t have one setup. Copy the URL shown.\nOpen GitHub Desktop\nClick File → Clone repository → URL\nPaste the repo URL and pay attention to the destination folder path so you can access this folder later\nClick “Clone”"
  },
  {
    "objectID": "Learn/Guides/Github-desktop.html#tracking-changes",
    "href": "Learn/Guides/Github-desktop.html#tracking-changes",
    "title": "Version Control with GitHub Desktop",
    "section": "Tracking changes",
    "text": "Tracking changes\nNow that you have set up your collaborative Kaggle hackathon repository, it’s time to start working on your project and track the changes you make. In this section, we will guide you through the process of adding files to your local repository, viewing and committing changes, and pushing those changes to the remote repository on GitHub. By understanding how to track changes effectively, you and your team can ensure that all contributions are recorded, reviewed, and integrated smoothly into the project.\n\nAdd a blank text file to your local repo\n\nRight-click repo name in GitHub Desktop → show in explorer (show in Finder and go to the directory on Mac)\nCreate a new text file and add to local repo folder\nAdd a line of text to the file, e.g., “hello world” and save the file\n\nView local changes\n\nIn GitHub desktop, you can view this change under the “Changes” tab. Notice that we see the new file and added text under this tab.\n\nCommit the new file\n\nCommits mark a checkpoint in the progress you have made to your repo. Provide a short summary message and optionally provide more information in the “Description” box.\n\nView remote changes (or lack thereof)\n\nVisit GitHub and notice that the change is not yet reflected on GitHub\n\nPush the change to GitHub \nView remote changes\n\nVisit GitHub again and notice the change has now been transferred to GitHub. Your collaborators can now access your changes through the remote repo (the repo stored on GitHub)"
  },
  {
    "objectID": "Learn/Guides/Github-desktop.html#ignoring-.ipynb-files",
    "href": "Learn/Guides/Github-desktop.html#ignoring-.ipynb-files",
    "title": "Version Control with GitHub Desktop",
    "section": "Ignoring .ipynb files",
    "text": "Ignoring .ipynb files\nAs you collaborate on your Kaggle hackathon project, you may encounter challenges with tracking changes in Jupyter notebooks (.ipynb files) due to their complex JSON format. These files can include a lot of metadata that makes version control difficult and cluttered. In this section, we’ll show you how to use Jupytext to convert your Jupyter notebooks into a more manageable format and configure your repository to ignore .ipynb files. This approach will help you maintain a cleaner version history and focus on the actual code changes, making collaboration more efficient.\n\nAdd jupyter lab file to repo\n\nOpen anaconda prompt and cd into your local repo folder\nrun “jupyter lab” command to start a new jupyter lab instance\ncreate a new notebook, e.g., preprocess_data.ipynb\nadd a line of code, e.g., print(‘hello world’)\nsave the notebook and open GitHub desktop\n\nIn GitHub desktop, notice the changes being tracked are wildly confusing. \n\nJupyter files are stored in JSON format which includes a lot of metadata unrelated to the changes you made to your file. The solution? Use Jupytext!\n\nInstall jupytext\n\npip install jupytext\njupytext –set-formats ipynb,py *.ipynb # convert .ipynb files to .py\njupytext –set-formats py,ipynb *.py\n\nalternatively to convert just one specific file: jupytext –set-formats ipynb,py file_name.ipynb\n\n\ngit ignore .ipynb files\n\nright click one of the .ipynb files in GitHub Desktop\nignore all files of this type\n\ncommit changes\npush and view changes on GitHub"
  },
  {
    "objectID": "Learn/Guides/Github-desktop.html#pulling-updates-from-github",
    "href": "Learn/Guides/Github-desktop.html#pulling-updates-from-github",
    "title": "Version Control with GitHub Desktop",
    "section": "Pulling updates from GitHub",
    "text": "Pulling updates from GitHub\nAs your team collaborates on the Kaggle hackathon project, it’s essential to stay up-to-date with the latest changes made by your teammates. In this section, we’ll explain how to pull updates from the remote repository on GitHub to your local machine. This process ensures that you always have the most recent version of the project and can integrate your work with the contributions of others seamlessly. By regularly pulling updates, you can avoid conflicts and ensure smooth collaboration throughout the project.\n\nPretend you are a collaborator and visit GitHub to find your repo\nAdd a new file to the remote repo (the version stored on GitHub): Add file → create new file.\nCommit the file to the repo\nOpen your local repo folder and notice we don’t have this new file yet\nIn GitHub Desktop, click “Fetch origin” by “Pull origin”\n\nFetch origin will run and inform you of any changes made to the remote copy of the repo (the one stored on GitHub)\nIf changes have been made since you last pulled, you’ll see the Fetch button turn into a “Pull” option. Click this option to retrieve any updates from GitHub and pull them into the local version of your repo.\n\nCheck your local repo folder to verify the new file has been pulled from GitHub onto your machine"
  },
  {
    "objectID": "Learn/Guides/Github-desktop.html#reverting-to-a-previous-commit",
    "href": "Learn/Guides/Github-desktop.html#reverting-to-a-previous-commit",
    "title": "Version Control with GitHub Desktop",
    "section": "Reverting to a previous commit",
    "text": "Reverting to a previous commit\nDuring the course of your Kaggle hackathon project, there may be times when you need to revert to a previous version of your code. This could be due to a bug, an unwanted change, or simply the need to return to a stable state. In this section, we’ll guide you through the process of reverting to a previous commit using GitHub Desktop. Understanding how to revert to an earlier commit ensures that you can quickly and safely undo changes, helping your team maintain a stable and functional codebase throughout the competition.\n\nFind the Commit to Revert To\n\nOpen GitHub Desktop and navigate to the repository you are working on.\nClick on the “History” tab to view the commit history of your repository.\nScroll through the list of commits and locate the commit you want to revert to. Click on the specific commit to select it.\n\nCreate a New Branch from the Selected Commit\n\nWith the desired commit selected, click on the “Branch” menu at the top of GitHub Desktop.\nSelect “New Branch” from the dropdown menu.\nIn the dialog box that appears, name your new branch (e.g., “revert-to-commit”) and ensure that it is based on the selected commit. Click “Create Branch” to proceed.\n\nMake Necessary Changes in the New Branch\n\nSwitch to the newly created branch by clicking on the “Current Branch” dropdown menu and selecting your new branch.\nMake any necessary changes in this branch to resolve issues or implement desired modifications.\nUse your code editor or IDE to make and save these changes.\n\nCommit the Changes to the New Branch\n\nReturn to GitHub Desktop and navigate to the “Changes” tab.\nReview the changes you made and provide a commit message summarizing them.\nClick the “Commit to ” button to commit these changes to the new branch.\n\nPush the Changes to GitHub\n\nClick the “Publish branch” button in GitHub Desktop to push your changes to the remote repository on GitHub.\nWait for the push process to complete.\n\nCreate a Pull Request (Optional)\n\nIf you want to merge these changes back into the main branch, go to your repository on GitHub.\nClick on the “Pull requests” tab and then click the “New pull request” button.\nSelect your new branch as the source and the main branch as the destination. Review the changes and click “Create pull request.”\n\nReview and Merge (If Using a Pull Request)\n\nReviewers can now examine the pull request on GitHub. They can leave comments, request changes, or approve the pull request.\nOnce the changes are approved, the pull request can be merged into the main branch by clicking the “Merge pull request” button on GitHub."
  },
  {
    "objectID": "Learn/Guides/Github-desktop.html#using-pull-requests-to-review-each-others-work",
    "href": "Learn/Guides/Github-desktop.html#using-pull-requests-to-review-each-others-work",
    "title": "Version Control with GitHub Desktop",
    "section": "Using “pull requests” to review each other’s’ work",
    "text": "Using “pull requests” to review each other’s’ work\nPull requests are an essential feature of GitHub that facilitate collaborative development by allowing team members to propose changes to a codebase. They provide a structured way for team members to review, discuss, and approve changes before they are merged into the main branch. In this section, we will explore how to use pull requests effectively to ensure that your team’s work is consistently high-quality and integrated smoothly. By following best practices for creating, reviewing, and merging pull requests, you can maintain a clean and stable codebase while fostering a collaborative and transparent development process. These instructions are clear and structured well, but a few refinements can enhance clarity and flow. Here’s an improved version:\n\nCreate a New Branch:\n\nOpen GitHub Desktop and select your repository.\nClick the “Current Branch” dropdown.\nSelect “New Branch” and give it a descriptive name (e.g., “feature-branch” or “collaborator-feature”).\nChoose the base branch, typically the default branch like main or master, and click “Create Branch.”\n\nMake Changes in the New Branch:\n\nSwitch to the newly created branch by selecting it from the “Current Branch” dropdown.\nCollaborators can now make changes in this new branch. They can create, edit, or delete files as needed.\n\nCommit and Push Changes:\n\nAfter making changes, go to the “Changes” tab in GitHub Desktop.\nReview the changes, provide a meaningful commit message, and click “Commit to ”.\nClick “Push origin” to push the changes to the remote repository on GitHub.\n\nPreview Pull Request:\n\nIn GitHub Desktop, click on “Branch” in the menu bar.\nSelect “Create Pull Request” to open a preview. This will show which branch is being merged into the main code base.\n\nCreate Pull Request:\n\nAfter confirming that the preview is correct, click “Create Pull Request”.\nGitHub will open in your web browser. Fill out the details for the pull request, including a title and description.\nAssign reviewers (e.g., you and other collaborators) to review the changes, then click “Create Pull Request.”\n\nReview and Submit the Pull Request on GitHub:\n\nCollaborators should review the pull request on the GitHub website.\nThey can add comments, suggestions, or request changes directly in the pull request interface.\n\nReview the Pull Request in GitHub Desktop:\n\nReturn to GitHub Desktop to see the newly created pull request listed in the “Current Branch” dropdown.\nClick on the pull request to view the changes, comments, and review the code.\nRespond to any feedback or comments in the GitHub Desktop interface.\n\nAccept or Request Changes:\n\nAfter reviewing the code, you and other collaborators can either accept the pull request if it’s ready to merge or request changes if there are issues to address.\nLeave comments, suggestions, and feedback in the pull request.\n\nCollaborators Make Changes:\n\nIf changes are requested, collaborators can make the necessary adjustments in their branch and push the updates.\nThe pull request will automatically update with the new commits.\n\nClose the Pull Request:\n\nOnce the pull request is approved and the changes have been successfully reviewed, merge the pull request into the main branch.\nAfter merging, you can delete the branch to keep the repository clean."
  },
  {
    "objectID": "Learn/Guides/Github-desktop.html#questions",
    "href": "Learn/Guides/Github-desktop.html#questions",
    "title": "Version Control with GitHub Desktop",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Learn/Guides/Github-desktop.html#see-also",
    "href": "Learn/Guides/Github-desktop.html#see-also",
    "title": "Version Control with GitHub Desktop",
    "section": "See also",
    "text": "See also\n\nWorkshop: Intro to Version Control with Git: If you’re curious to learn how to use Git via shell commands (or just want to become more fluent with Git), check out this YouTube playlist from the Data Science Hub!\nVideo: Reproducibility Overview Lecture: Without reproducibility, software systems become unreliable and difficult to improve upon over time. To learn more about best practices when conducting reproducible computational work, check out this talk! Reproducibility is essential not only in research, but for all software developers as well."
  },
  {
    "objectID": "Learn/Guides/How-to-contribute.html",
    "href": "Learn/Guides/How-to-contribute.html",
    "title": "How to Contribute?",
    "section": "",
    "text": "We want Nexus to serve also as a place where members of the community can share their knowledge. This guide answers the question, how to contribute to Nexus?",
    "crumbs": [
      "How to contribute?"
    ]
  },
  {
    "objectID": "Learn/Guides/How-to-contribute.html#what-kinds-of-resources-are-hosted-on-nexus",
    "href": "Learn/Guides/How-to-contribute.html#what-kinds-of-resources-are-hosted-on-nexus",
    "title": "How to Contribute?",
    "section": "What kinds of resources are hosted on Nexus?",
    "text": "What kinds of resources are hosted on Nexus?\nAny content (original or external) that can help make the practice of ML/AI more connected, accessible, efficient, and reproducible is welcome on the Nexus platform! This includes, but is not limited to…\n\n🧠 Educational materials: Explore a library of educational materials (workshops, books, videos, etc.) covering a wide range of ML-related topics, tools, and workflows, from foundational concepts to advanced techniques.\n🛠 Models, code, and more: Learn about popular models, tools, and datasets that you can leverage for your next ML project.\n🧬 Applications & stories: Discover a curated collection of blogs, papers, and talks which dive into real-world ML applications and lessons learned by practitioners.\n\nDisclaimer: The crowdsourced resources on this website are not endorsed by the UW-Madison and have not been vetted by the Division of Information Technology.",
    "crumbs": [
      "How to contribute?"
    ]
  },
  {
    "objectID": "Learn/Guides/How-to-contribute.html#need-inspiration-for-a-good-topic-to-post-about",
    "href": "Learn/Guides/How-to-contribute.html#need-inspiration-for-a-good-topic-to-post-about",
    "title": "How to Contribute?",
    "section": "Need inspiration for a good topic to post about?",
    "text": "Need inspiration for a good topic to post about?\nAn ever-expanding list of requested resources can be found on the Issues page (on GitHub). Search for open issues that have the “Resource” label to check out some of our top priorities. If you’d like to tackle a given issue, please comment on the issue to let others know.",
    "crumbs": [
      "How to contribute?"
    ]
  },
  {
    "objectID": "Learn/Guides/How-to-contribute.html#what-makes-a-good-post",
    "href": "Learn/Guides/How-to-contribute.html#what-makes-a-good-post",
    "title": "How to Contribute?",
    "section": "What makes a good post?",
    "text": "What makes a good post?\nCreating a useful and engaging post for the ML+X Nexus involves a few key elements to ensure it is beneficial for the community. Here are some general guidelines to follow:\n\nClear and concise title\nThe title should accurately reflect the content and main focus of the post. It should be engaging and specific, allowing readers to quickly understand what they can expect.\n\n\nDetailed description\nProvide a comprehensive overview of the resource or topic. This should include:\n\nPurpose and scope: Clearly state what the resource covers and its main objectives. Explain why the content is valuable and how it can help practitioners.\nKey features: Highlight the unique aspects or strengths of the resource. This could include practical examples, interactive elements, or real-world applications.\nStrengths and weaknesses: Provide a balanced view by discussing both the strengths and any potential limitations of the resource. This helps users make informed decisions about whether the resource is right for them.\n\n\n\nPrerequisites\nList any necessary background knowledge or skills required to fully benefit from the resource. This helps set expectations and ensures that users are adequately prepared. Include links to additional resources or tutorials that can help users gain the required knowledge.\n\n\nEstimated time to complete\nOffer an estimate of the time commitment needed to complete the resource. This helps users plan their learning activities and manage their time effectively.\n\n\nAccessibility and usability\nEnsure the resource is easy to access and use. We want the majority of resources on Nexus to be free and open source (with possibly a few rare exceptions for tools/resources in high-demand). Provide clear instructions on how to navigate and utilize the content. If the resource is hosted externally, include a direct link and any necessary login or access information.\n\n\nAdditional related resources\nInclude links to related materials or further readings that can enhance the user’s understanding and provide more in-depth knowledge on the topic. This can include books, articles, other workshops, or case studies. When possible, link to any relevant materials which are already hosted on the Nexus platform.\n\n\nExamples of good posts\nPlease see below for a list of resources that meet our platform’s standards. You can use these examples in conjunction with the template files provided in the next section to create your post.\n\nExternal content\n\nLearn: Workshop\nLearn: Book\nLearn: Video\nApplication: Video\nToolbox: Data\n\n\n\nOriginal content\n\nLearn: Guide\nLearn: Notebook\nApplication: Blog",
    "crumbs": [
      "How to contribute?"
    ]
  },
  {
    "objectID": "Learn/Guides/How-to-contribute.html#how-to-make-a-new-post-with-github",
    "href": "Learn/Guides/How-to-contribute.html#how-to-make-a-new-post-with-github",
    "title": "How to Contribute?",
    "section": "How to make a new post with GitHub?",
    "text": "How to make a new post with GitHub?\n\nGitHub collaboration model\nWe follow GitHub’s collaboration model, so the general idea to make a post or edit a document is the same. The high-level steps include:\n\nCreate an issue announcing your plan to add a resource — see ML+X Nexus Issues\nFork the ML+X-Nexus repository\nClone the forked repository onto your local machine\nCreate a new branch\nWrite the post, commit and push the changes\nMake a pull request\n\nIf you don’t know how to use Git / GitHub already, it can be a little intimidating at first. A friendlier alternative could be to download GitHub desktop and add your post using the instructions provided below. If you’d like to learn more about GitHub Desktop, check out the Version Control with GitHub Desktop guide on Nexus. If you need additional help (and work in a research lab at UW-Madison), you may also seek help at the Data Science Hub’s office hours (“Coding Meetup”).\n\n1. Get Started with GitHub Desktop\n\nGo to the GitHub Desktop website and download the application for your operating system. Install GitHub Desktop by following the on-screen instructions.\nOpen GitHub Desktop and sign in with your GitHub account. If you don’t have one, you will need to create your GitHub account first.\n\n\n\n2. Create an Issue on Nexus GitHub\nBefore you start writing your content, create an issue on the Nexus GitHub to announce your intended addition. This helps the Nexus development team keep track of new contributions and provides an opportunity for feedback.\n\nGo to the Nexus GitHub Issues page\nClick on the New Issue button.\nTitle the issue with the name of your resource, and add a “Resource” label/tag (found on right side panel of issue post).\nDescribe why you think this resource should be included on the Nexus platform.\nWait for feedback: Wait for one of the Nexus developers to provide feedback or comments on your issue before proceeding.\n\n\n\n3. Fork the Repository\n\nGo to the ML+X-Nexus repository on GitHub.\nClick the Fork button at the top-right corner of the page. This will create a copy of the repository under your GitHub account.\n\n\n\n4. Clone the Repository to Your System to Your Local System\n\nFrom your new forked version of the repo on GitHub, click the green Code button to copy the HTTPS URL of the fork\nIn GitHub Desktop, click on File &gt; Clone Repository.\nPaste the URL and click Clone\n\n\n\n5. Create a New Branch\n\nIn GitHub Desktop, click on Branch &gt; New Branch.\nName your new branch descriptively based on the resource type and name (e.g., workshop-introDL, video-NeurIPS2024, etc.).\n\n\n\n6. Write Your Post\n\nOpen your favorite text editor or IDE (e.g., Visual Studio Code, Sublime Text).\nWrite your post in the appropriate format. Follow the guidelines in the next section for writing a good post, making use the template file provided.\n\n\n\n7. Commit and Push Changes\n\nIn GitHub Desktop, you should see your changes listed under Changes.\nWrite a descriptive commit message (e.g., Add new resource: workshop-introDL).\nClick Commit to your-branch-name.\nClick on Repository &gt; Push to push your changes to GitHub.\n\n\n\n8. Make a Pull Request\n\nGo to your forked repository on GitHub.\nClick on the Compare & pull request button.\nEnsure you are merging into the “main” branch of the ML+X-Nexus repository.\nWrite a descriptive title and comment for your pull request.\nClick Create pull request.\n\n\n\n9. Wait for Review\n\nOne of the Nexus developers will review your pull request. They may provide feedback or request changes.\nAddress any feedback and push additional commits as needed.\n\n\n\n\nHow to write the post? Start with a template!\nIf applicable, start with one of the relevant template files linked below. There are comments in each template that will help you make the appropriate edits for your resource. You can also check out how other posts have been formatted by clicking “Improve this page” from a given post’s webpage. This will bring you directly to the qmd file for that post.\n\nEducational - video: lecture, demo, recorded workshop, etc.\nEducational - general: workshop materials, book, guide, etc.\nApplication - video\nApplication - blog\nToolbox - data\nToolbox - model\n\n\n\nWhere to locate your post?\nWe want the site to be constantly evolving with the community, and our intention is to keep the contributions to the site as free as possible. However, we added some sections to structure the site a little bit:\n\n├── Applications\n│   ├── Blogs\n│   ├── Videos\n│   ├── Papers\n│   ├── Playlists\n├── Learn\n│   ├── Blogs\n│   ├── Books\n│   ├── Guides\n│   ├── Notebooks\n│   ├── Videos\n│   ├── Workshops\n├── Toolbox\n│   ├── Compute\n│   ├── Data \n│   ├── GenAI \n│   ├── Libraries \n│   ├── Models \nNote: Some subfolders may not exist yet (e.g., Code, Data) since no one has contributed a resource from one of those categories yet, and git doesn’t allow empty folders. Feel free to start one of the missing folders, if applicable. If your resource doesn’t belong to one of the categories listed above, you may add a new one. We’ll discuss this new category when we review your initial “Issue” announcing the resource addition.",
    "crumbs": [
      "How to contribute?"
    ]
  },
  {
    "objectID": "Learn/Guides/How-to-contribute.html#how-to-improve-an-existing-post",
    "href": "Learn/Guides/How-to-contribute.html#how-to-improve-an-existing-post",
    "title": "How to Contribute?",
    "section": "How to improve an existing post?",
    "text": "How to improve an existing post?\nWant to add a code-long exercise to an existing post, add your perspective, or correct a typo? Anyone is welcome and encouraged to suggest improvements to existing materials hosted on Nexus! The most straightforward way to do this is to click “Improve this page” from the post’s webpage on Nexus to suggest your edits. The below steps will walk you through this process.\n\nSteps to improve a post on Nexus via GitHub (no software installation needed):\n\nClick “Improve this page”: This will redirect you to the GitHub repository where the content of the post is stored.\nEdit the file directly on GitHub:\n\nOn the GitHub page, click the pencil icon (✏️) at the top right of the file to edit the content directly in your browser. No need to install any git software!\nMake your changes in the text editor. You can add a code-long exercise, share your perspective, or correct any typos.\n\nCommit your changes:\n\nAfter you have made your edits, scroll down to the “Commit changes” section.\nWrite a brief description of what you changed.\nChoose the option to “Create a new branch for this commit and start a pull request.”\n\nCreate a pull request:\n\nClick the “Propose changes” button.\nYou will be taken to a new page where you can review your changes.\nClick “Create pull request” to submit your changes for review.\n\n\nCongratulations! You have suggested an improvement to a Nexus post. The repository maintainers will review your pull request, and if everything looks good, your changes will be merged into the post.\nNote: While you can make these edits directly on GitHub without any software installation, we recommend all ML practitioners learn git! Check out our git category search resources.",
    "crumbs": [
      "How to contribute?"
    ]
  },
  {
    "objectID": "Learn/Guides/How-to-contribute.html#questions",
    "href": "Learn/Guides/How-to-contribute.html#questions",
    "title": "How to Contribute?",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions on how to contribute, please feel free to post to the Nexus Q&A on GitHub. We will improve this guide based on additional questions/comments we receive.",
    "crumbs": [
      "How to contribute?"
    ]
  },
  {
    "objectID": "Learn/Videos/Grokking.html",
    "href": "Learn/Videos/Grokking.html",
    "title": "Grokking",
    "section": "",
    "text": "The verb, “to grok”, was originally coined by Robert A. Heinlein in his 1961 science fiction novel “Stranger in a Strange Land,” where it meant to understand something so thoroughly that it becomes a part of oneself. In the context of machine learning, “grokking” refers to the phenomenon whereby a model, after extensive training, suddenly shifts from merely memorizing data to achieving a deep and intuitive understanding, allowing it to generalize effectively to new, unseen data. Observing this phenomenon requires a significantly large increase in training iterations, often far beyond the usual training duration expected for a model to reach acceptable performance. This prolonged training period initially shows no improvement in generalization, making the eventual transition to grokking both surprising and significant.\nThe grokking phenomenon was recently investigated in a notable paper from OpenAI (Power et al., 2021). The video you’ll be watching will explain OpenAI’s findings on this transition to generalization, highlighting the dramatic increase in iterations needed and providing a deeper understanding of the process and its implications for developing more robust and reliable machine learning models."
  },
  {
    "objectID": "Learn/Videos/Grokking.html#about-this-resource",
    "href": "Learn/Videos/Grokking.html#about-this-resource",
    "title": "Grokking",
    "section": "",
    "text": "The verb, “to grok”, was originally coined by Robert A. Heinlein in his 1961 science fiction novel “Stranger in a Strange Land,” where it meant to understand something so thoroughly that it becomes a part of oneself. In the context of machine learning, “grokking” refers to the phenomenon whereby a model, after extensive training, suddenly shifts from merely memorizing data to achieving a deep and intuitive understanding, allowing it to generalize effectively to new, unseen data. Observing this phenomenon requires a significantly large increase in training iterations, often far beyond the usual training duration expected for a model to reach acceptable performance. This prolonged training period initially shows no improvement in generalization, making the eventual transition to grokking both surprising and significant.\nThe grokking phenomenon was recently investigated in a notable paper from OpenAI (Power et al., 2021). The video you’ll be watching will explain OpenAI’s findings on this transition to generalization, highlighting the dramatic increase in iterations needed and providing a deeper understanding of the process and its implications for developing more robust and reliable machine learning models."
  },
  {
    "objectID": "Learn/Videos/Grokking.html#questions",
    "href": "Learn/Videos/Grokking.html#questions",
    "title": "Grokking",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Learn/Videos/Reproducibility-overview.html",
    "href": "Learn/Videos/Reproducibility-overview.html",
    "title": "Overview of Reproducibility Lecture",
    "section": "",
    "text": "Dr. Sarah Stevens’ lecture highlights the critical importance of reproducibility in computational and data science projects. She also shares best practices to ensure reproducible results including:\n\nHow to organize your project\nGood names for files/folders\nDocumenting your work and README files\nHow to organize data in spreadsheets and use data dictionaries\nAutomation with scripts\nVersion control\nLicensing\nCreating backups"
  },
  {
    "objectID": "Learn/Videos/Reproducibility-overview.html#about-this-resource",
    "href": "Learn/Videos/Reproducibility-overview.html#about-this-resource",
    "title": "Overview of Reproducibility Lecture",
    "section": "",
    "text": "Dr. Sarah Stevens’ lecture highlights the critical importance of reproducibility in computational and data science projects. She also shares best practices to ensure reproducible results including:\n\nHow to organize your project\nGood names for files/folders\nDocumenting your work and README files\nHow to organize data in spreadsheets and use data dictionaries\nAutomation with scripts\nVersion control\nLicensing\nCreating backups"
  },
  {
    "objectID": "Learn/Videos/Reproducibility-overview.html#questions",
    "href": "Learn/Videos/Reproducibility-overview.html#questions",
    "title": "Overview of Reproducibility Lecture",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Learn/Videos/Reproducibility-overview.html#see-also",
    "href": "Learn/Videos/Reproducibility-overview.html#see-also",
    "title": "Overview of Reproducibility Lecture",
    "section": "See also",
    "text": "See also\n\nGuide: Version Control with GitHub Desktop: GitHub Desktop is a graphical user interface (GUI) application that simplifies the use of Git and GitHub. It is designed for users who prefer not to use the command line interface, offering a more intuitive and visual approach to version control. With GitHub Desktop, you can easily perform common Git tasks such as committing changes, creating branches, and resolving merge conflicts, all within a user-friendly interface.\nWorkshop: Intro to Version Control with Git: If you’re curious to learn how to use Git via shell commands (or just want to become more fluent with Git), check out this YouTube playlist from the Data Science Hub!"
  },
  {
    "objectID": "Learn/index.html",
    "href": "Learn/index.html",
    "title": "Learn",
    "section": "",
    "text": "Explore a library of educational materials (workshops, guides, books, videos, etc.) covering a wide range of ML-related topics, tools, and workflows. From foundational concepts to advanced techniques, these materials offer clear explanations, practical examples, and actionable insights to help you navigate the complexities of ML with confidence.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Fact-Based QA with RAG: Romeo and Juliet\n\n\n\nNotebooks\n\nRAG\n\nRetrieval\n\nNLP\n\nLLM\n\nEmbeddings\n\nText analysis\n\nDeep learning\n\nPrompt engineering\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2025-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\nLearning Through Comparison: Use Cases of Contrastive Learning\n\n\n\nVideos\n\nML+X\n\nUW-Madison\n\nDeep learning\n\nContrastive learning\n\nClustering\n\nOOD detection\n\nMultimodal learning\n\nTrustworthy AI\n\nRepresentation learning\n\nCIFAR\n\n\n\n\n\n\n\nYin Li, Chris Endemann\n\n\n2025-02-10\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Statistical Learning\n\n\n\nBooks\n\nStatistical learning\n\nClassical ML\n\nRegression\n\nClassification\n\nDeep learning\n\nDecision trees\n\nUnsupervised Learning\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2025-01-06\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne Useful Thing\n\n\n\nBlogs\n\nIndustry applications\n\nLLM\n\nEthical AI\n\nGenAI\n\n\n\n\n\n\n\nAlan Ng\n\n\n2024-12-12\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrustworthy AI - Explainability, Bias, Fairness, and Safety\n\n\n\nWorkshops\n\nTrustworthy AI\n\nEthical AI\n\nExplainability\n\nFairness\n\nBias\n\nOOD detection\n\nAIF360\n\nPyTorch-OOD\n\nAnomaly detection\n\nGradCAM\n\nTabular\n\nComputer vision\n\nNLP\n\nText analysis\n\nCode-along\n\nCarpentries\n\nDeep learning\n\nModel sharing\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-12-03\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to AWS SageMaker for Predictive ML/AI\n\n\n\nWorkshops\n\nCode-along\n\nCarpentries\n\nCompute\n\nAWS\n\nGPU\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-11-07\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring the Titanic Dataset\n\n\n\nNotebooks\n\nEDA\n\nTabular\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-10-07\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrokking\n\n\n\nVideos\n\nDeep learning\n\nEmpirical patterns\n\nGrokking\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Python\n\n\n\nWorkshops\n\nPython\n\nCarpentries\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Machine Learning with Sklearn\n\n\n\nWorkshops\n\nLibraries\n\nClassical ML\n\nBoosting\n\nDecision trees\n\nSVM\n\nClustering\n\nRegression\n\nSklearn\n\nCode-along\n\nCarpentries\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-17\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Deep Learning with Keras\n\n\n\nWorkshops\n\nLibraries\n\nDeep learning\n\nKeras\n\nCode-along\n\nCarpentries\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-16\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Deep Learning with PyTorch\n\n\n\nWorkshops\n\nLibraries\n\nDeep learning\n\nPyTorch\n\nUdacity\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-15\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Deep Learning\n\n\n\nBooks\n\nDeep learning\n\nPyTorch\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-14\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Text Analysis / NLP\n\n\n\nWorkshops\n\nDeep learning\n\nHugging Face\n\nText analysis\n\nNLP\n\nLLM\n\nCode-along\n\nCarpentries\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-13\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview of Reproducibility Lecture\n\n\n\nVideos\n\nReproducibility\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\n\nVersion Control with GitHub Desktop\n\n\n\nGuides\n\nReproducibility\n\nGit/GitHub\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-11\n\n\n\n\n\n\n\n\n\n\n\n\n\nVersion Control with Git and GitHub (Carpentries)\n\n\n\nWorkshops\n\nVideos\n\nReproducibility\n\nGit/GitHub\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-11\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Out-of-Distribution Detection\n\n\n\nVideos\n\nICCV\n\nOOD detection\n\nTrustworthy AI\n\n\n\n\n\n\n\nSharon Li\n\n\n2024-07-11\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Contribute?\n\n\n\nGuides\n\nContribute\n\n\n\n\n\n\n\nML+X\n\n\n2024-06-24\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Learn"
    ]
  },
  {
    "objectID": "Learn/Blogs/index.html",
    "href": "Learn/Blogs/index.html",
    "title": "Blogs",
    "section": "",
    "text": "Explore websites that features regular blogs on AI and ML related topics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne Useful Thing\n\n1 min\n\n\nBlogs\n\nIndustry applications\n\nLLM\n\nEthical AI\n\nGenAI\n\n\n\n\n\n\n\nAlan Ng\n\n\n2024-12-12\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Learn",
      "Blogs"
    ]
  },
  {
    "objectID": "Learn/Books/Intro-Deeplearning_SimonJDPrince.html",
    "href": "Learn/Books/Intro-Deeplearning_SimonJDPrince.html",
    "title": "Understanding Deep Learning",
    "section": "",
    "text": "Nowadays, nearly anyone can implement a deep learning model in a just a few lines of code. What separates the novices from the experts, however, is the ability to understand (or at least predict!) how these models work in different circumstances.\nSimon J.D. Prince’s free textbook, Understanding Deep Learning, provides a modern overview of deep learning, including newer topics like double descent and transformer models, and provides colab notebooks to explore deep learning concepts and implementations. The book uses PyTorch as its framework of choice.\n\n\n\nThe title of this book is “Understanding Deep Learning” to distinguish it from volumes that cover coding and other practical aspects. This text is primarily about the ideas that underlie deep learning. The first part of the book introduces deep learning models and discusses how to train them, measure their performance, and improve this performance. The next part considers architectures that are specialized to images, text, and graph data. These chapters require only introductory linear algebra, calculus, and probability and should be accessible to any second-year undergraduate in a quantitative discipline. Subsequent parts of the book tackle generative models and reinforcement learning. These chapters require more knowledge of probability and calculus and target more advanced students. The title is also partly a joke — no-one really understands deep learning at the time of writing. Modern deep networks learn piecewise linear functions with more regions than there are atoms in the universe and can be trained with fewer data examples than model parameters. It is neither obvious that we should be able to fit these functions reliably nor that they should generalize well to new data. The penultimate chapter addresses these and other aspects that are not yet fully understood. Regardless, deep learning will change the world for better or worse. The final chapter discusses AI ethics and concludes with an appeal for practitioners to consider the moral implications of their work.\n\n\n\nLearners are expected to have the following knowledge:\n\nLinear algebra: linear algebra is the language of machine learning\nCalculus: recommended to understand gradient descent\nProbability theory: needed for reinforcement learning\nPyTorch: recommended for following along with Colab notebooks\n\n\n\n\nTBD: Use the Improve this page functionality to add your own estimate!"
  },
  {
    "objectID": "Learn/Books/Intro-Deeplearning_SimonJDPrince.html#about-this-resource",
    "href": "Learn/Books/Intro-Deeplearning_SimonJDPrince.html#about-this-resource",
    "title": "Understanding Deep Learning",
    "section": "",
    "text": "Nowadays, nearly anyone can implement a deep learning model in a just a few lines of code. What separates the novices from the experts, however, is the ability to understand (or at least predict!) how these models work in different circumstances.\nSimon J.D. Prince’s free textbook, Understanding Deep Learning, provides a modern overview of deep learning, including newer topics like double descent and transformer models, and provides colab notebooks to explore deep learning concepts and implementations. The book uses PyTorch as its framework of choice.\n\n\n\nThe title of this book is “Understanding Deep Learning” to distinguish it from volumes that cover coding and other practical aspects. This text is primarily about the ideas that underlie deep learning. The first part of the book introduces deep learning models and discusses how to train them, measure their performance, and improve this performance. The next part considers architectures that are specialized to images, text, and graph data. These chapters require only introductory linear algebra, calculus, and probability and should be accessible to any second-year undergraduate in a quantitative discipline. Subsequent parts of the book tackle generative models and reinforcement learning. These chapters require more knowledge of probability and calculus and target more advanced students. The title is also partly a joke — no-one really understands deep learning at the time of writing. Modern deep networks learn piecewise linear functions with more regions than there are atoms in the universe and can be trained with fewer data examples than model parameters. It is neither obvious that we should be able to fit these functions reliably nor that they should generalize well to new data. The penultimate chapter addresses these and other aspects that are not yet fully understood. Regardless, deep learning will change the world for better or worse. The final chapter discusses AI ethics and concludes with an appeal for practitioners to consider the moral implications of their work.\n\n\n\nLearners are expected to have the following knowledge:\n\nLinear algebra: linear algebra is the language of machine learning\nCalculus: recommended to understand gradient descent\nProbability theory: needed for reinforcement learning\nPyTorch: recommended for following along with Colab notebooks\n\n\n\n\nTBD: Use the Improve this page functionality to add your own estimate!"
  },
  {
    "objectID": "Learn/Books/Intro-Deeplearning_SimonJDPrince.html#questions",
    "href": "Learn/Books/Intro-Deeplearning_SimonJDPrince.html#questions",
    "title": "Understanding Deep Learning",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Learn/Books/Intro-Deeplearning_SimonJDPrince.html#see-also",
    "href": "Learn/Books/Intro-Deeplearning_SimonJDPrince.html#see-also",
    "title": "Understanding Deep Learning",
    "section": "See also",
    "text": "See also\n\nWorkshop: Intro to Deep Learning with PyTorch: Learn how to use the PyTorch deep learning framework\nWorkshop: Intro to Deep Learning with Keras: Learn how to use the Keras deep learning framework"
  },
  {
    "objectID": "Learn/Workshops/Intro-Python_Gapminder.html",
    "href": "Learn/Workshops/Intro-Python_Gapminder.html",
    "title": "Intro to Python",
    "section": "",
    "text": "The Plotting and Programming in Python workshop provides an introduction to programming in Python 3 for people with little or no previous programming experience. It uses plotting as its motivating example to learn Python fundamentals, including functions, conditional logic, loops, and popular packages (e.g., Pandas).\n\n\nNo previous experience with programming necessary.\n\n\n\nThis workshop takes approximately 8 hours to complete.\n\n\n\nThe Carpentries is a global organization of researchers who volunteer their time and effort to create workshops that teach software engineering and data analysis skills to other researchers. UW-Madison has its own local Carpentries community which is actively engaged in developing new ML/AI workshops. To be notified of upcoming workshops offered by the Carpentries, make sure to subscribe to the Data Science @ UW Newsletter.\n\n\n\nAll Carpentries lessons are published as open source educational materials. You are welcome and encouraged to visit the lesson materials to work through them on your own. If you are involved with a research lab at UW-Madison campus, you may attend Coding Meetup (Tue/Thur, 2:30-4:30pm) to get help working through the materials."
  },
  {
    "objectID": "Learn/Workshops/Intro-Python_Gapminder.html#about-this-resource",
    "href": "Learn/Workshops/Intro-Python_Gapminder.html#about-this-resource",
    "title": "Intro to Python",
    "section": "",
    "text": "The Plotting and Programming in Python workshop provides an introduction to programming in Python 3 for people with little or no previous programming experience. It uses plotting as its motivating example to learn Python fundamentals, including functions, conditional logic, loops, and popular packages (e.g., Pandas).\n\n\nNo previous experience with programming necessary.\n\n\n\nThis workshop takes approximately 8 hours to complete.\n\n\n\nThe Carpentries is a global organization of researchers who volunteer their time and effort to create workshops that teach software engineering and data analysis skills to other researchers. UW-Madison has its own local Carpentries community which is actively engaged in developing new ML/AI workshops. To be notified of upcoming workshops offered by the Carpentries, make sure to subscribe to the Data Science @ UW Newsletter.\n\n\n\nAll Carpentries lessons are published as open source educational materials. You are welcome and encouraged to visit the lesson materials to work through them on your own. If you are involved with a research lab at UW-Madison campus, you may attend Coding Meetup (Tue/Thur, 2:30-4:30pm) to get help working through the materials."
  },
  {
    "objectID": "Learn/Workshops/Intro-Python_Gapminder.html#questions",
    "href": "Learn/Workshops/Intro-Python_Gapminder.html#questions",
    "title": "Intro to Python",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Learn/Workshops/Intro-Python_Gapminder.html#see-also",
    "href": "Learn/Workshops/Intro-Python_Gapminder.html#see-also",
    "title": "Intro to Python",
    "section": "See also",
    "text": "See also\n\nWorkshop: Intro to Machine Learning with Sklearn: Once you master Python fundamentals, start using the scikit-learn package to begin exploring “classical” ML methods (e.g., regression, clustering, decision trees)."
  },
  {
    "objectID": "Learn/Workshops/Intro-Deeplearning_PyTorch.html",
    "href": "Learn/Workshops/Intro-Deeplearning_PyTorch.html",
    "title": "Intro to Deep Learning with PyTorch",
    "section": "",
    "text": "The Intro to Deep Learning with PyTorch workshop from Udacity will walk you through introductory deep learning concepts as well as how to build a neural networks in PyTorch. PyTorch is one of the most popular deep learning frameworks. Known for its speed and more “Pythonic” feel, it is frequently the go-to choice for most researchers. The biggest downside of PyTorch, compared to a high-level framework like Keras, is that it is quite verbose. That is, you’ll need to write a couple hundred lines of code to train and evaluate your neural network. Keras is a great alternative for those who are just getting started with neural networks or those that don’t need to train many models, as you can train/evaluate in just a dozen or so lines of code.\n\n\nLearners are expected to have the following knowledge:\n\nBasic Python programming skills and familiarity with the Pandas package. If you need a refresher, these Introductory Python lesson materials are available for independent study.\n\n\n\n\nTBD: Use the Improve this page functionality to add your own estimate!"
  },
  {
    "objectID": "Learn/Workshops/Intro-Deeplearning_PyTorch.html#about-this-resource",
    "href": "Learn/Workshops/Intro-Deeplearning_PyTorch.html#about-this-resource",
    "title": "Intro to Deep Learning with PyTorch",
    "section": "",
    "text": "The Intro to Deep Learning with PyTorch workshop from Udacity will walk you through introductory deep learning concepts as well as how to build a neural networks in PyTorch. PyTorch is one of the most popular deep learning frameworks. Known for its speed and more “Pythonic” feel, it is frequently the go-to choice for most researchers. The biggest downside of PyTorch, compared to a high-level framework like Keras, is that it is quite verbose. That is, you’ll need to write a couple hundred lines of code to train and evaluate your neural network. Keras is a great alternative for those who are just getting started with neural networks or those that don’t need to train many models, as you can train/evaluate in just a dozen or so lines of code.\n\n\nLearners are expected to have the following knowledge:\n\nBasic Python programming skills and familiarity with the Pandas package. If you need a refresher, these Introductory Python lesson materials are available for independent study.\n\n\n\n\nTBD: Use the Improve this page functionality to add your own estimate!"
  },
  {
    "objectID": "Learn/Workshops/Intro-Deeplearning_PyTorch.html#questions",
    "href": "Learn/Workshops/Intro-Deeplearning_PyTorch.html#questions",
    "title": "Intro to Deep Learning with PyTorch",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Learn/Workshops/Intro-Deeplearning_PyTorch.html#see-also",
    "href": "Learn/Workshops/Intro-Deeplearning_PyTorch.html#see-also",
    "title": "Intro to Deep Learning with PyTorch",
    "section": "See also",
    "text": "See also\n\nWorkshop: Intro to Deep Learning with Keras: Explore Keras as an alternative deep learning framework\nBook: Understanding Deep Learning - Simon J.D. Prince: This free textbook is a good modern overview of deep learning, and provides colab notebooks to explore deep learning concepts and implementations. The book uses PyTorch as its framework of choice. You may find additional details in this book that the workshop only briefly touches on."
  },
  {
    "objectID": "Learn/Workshops/Intro-Deeplearning_Keras.html",
    "href": "Learn/Workshops/Intro-Deeplearning_Keras.html",
    "title": "Intro to Deep Learning with Keras",
    "section": "",
    "text": "The Intro to Deep Learning with Keras workshop will walk you through introductory deep learning concepts as well as how to build a neural networks in Keras. Keras is high-level wrapper framework (uses PyTorch or Tensorflow in the backend) which allows you to train and evaluate neural networks in just a few lines of code. It may take slightly longer to train a Keras model (compared to PyTorch and Tensorflow), but the difference in performance is often negligible for those that only need to train a few models. The ability to quickly build and test models is the primary selling point of Keras.\n\n\nLearners are expected to have the following knowledge:\n\nBasic Python programming skills and familiarity with the Pandas package. If you need a refresher, these Introductory Python lesson materials are available for independent study.\nBasic knowledge on machine learning, including the following concepts: Data cleaning, train & test split, type of problems (regression, classification), overfitting & underfitting, metrics (accuracy, recall, etc.).The Intro to Machine Learning with Sklearn lesson materials are a good option for those that need a refresher on machine learning fundamentals.\n\n\n\n\nThis workshop takes approximately 15 hours to complete.\n\n\n\nThe Carpentries is a global organization of researchers who volunteer their time and effort to create workshops that teach software engineering and data analysis skills to other researchers. UW-Madison has its own local Carpentries community which is actively engaged in developing new ML/AI workshops. To be notified of upcoming workshops offered by the Carpentries, make sure to subscribe to the Data Science @ UW Newsletter. The Intro Deep Learning workshop is typically taught in May each year.\n\n\n\nAll Carpentries lessons are published as open source educational materials. You are welcome and encouraged to visit the lesson materials to work through them on your own. If you are involved with a research lab at UW-Madison campus, you may attend Coding Meetup (Tue/Thur, 2:30-4:30pm) to get help working through the materials."
  },
  {
    "objectID": "Learn/Workshops/Intro-Deeplearning_Keras.html#about-this-resource",
    "href": "Learn/Workshops/Intro-Deeplearning_Keras.html#about-this-resource",
    "title": "Intro to Deep Learning with Keras",
    "section": "",
    "text": "The Intro to Deep Learning with Keras workshop will walk you through introductory deep learning concepts as well as how to build a neural networks in Keras. Keras is high-level wrapper framework (uses PyTorch or Tensorflow in the backend) which allows you to train and evaluate neural networks in just a few lines of code. It may take slightly longer to train a Keras model (compared to PyTorch and Tensorflow), but the difference in performance is often negligible for those that only need to train a few models. The ability to quickly build and test models is the primary selling point of Keras.\n\n\nLearners are expected to have the following knowledge:\n\nBasic Python programming skills and familiarity with the Pandas package. If you need a refresher, these Introductory Python lesson materials are available for independent study.\nBasic knowledge on machine learning, including the following concepts: Data cleaning, train & test split, type of problems (regression, classification), overfitting & underfitting, metrics (accuracy, recall, etc.).The Intro to Machine Learning with Sklearn lesson materials are a good option for those that need a refresher on machine learning fundamentals.\n\n\n\n\nThis workshop takes approximately 15 hours to complete.\n\n\n\nThe Carpentries is a global organization of researchers who volunteer their time and effort to create workshops that teach software engineering and data analysis skills to other researchers. UW-Madison has its own local Carpentries community which is actively engaged in developing new ML/AI workshops. To be notified of upcoming workshops offered by the Carpentries, make sure to subscribe to the Data Science @ UW Newsletter. The Intro Deep Learning workshop is typically taught in May each year.\n\n\n\nAll Carpentries lessons are published as open source educational materials. You are welcome and encouraged to visit the lesson materials to work through them on your own. If you are involved with a research lab at UW-Madison campus, you may attend Coding Meetup (Tue/Thur, 2:30-4:30pm) to get help working through the materials."
  },
  {
    "objectID": "Learn/Workshops/Intro-Deeplearning_Keras.html#questions",
    "href": "Learn/Workshops/Intro-Deeplearning_Keras.html#questions",
    "title": "Intro to Deep Learning with Keras",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Learn/Workshops/Intro-Deeplearning_Keras.html#see-also",
    "href": "Learn/Workshops/Intro-Deeplearning_Keras.html#see-also",
    "title": "Intro to Deep Learning with Keras",
    "section": "See also",
    "text": "See also\n\nWorkshop: Intro to Deep Learning with PyTorch: Explore PyTorch as an alternative deep learning framework.\nBook: Understanding Deep Learning - Simon J.D. Prince: This free textbook is a good modern overview of deep learning, and provides colab notebooks to explore deep learning concepts and implementations. The book uses PyTorch as its framework of choice. You may find additional details in this book that the workshop only briefly touches on."
  },
  {
    "objectID": "Learn/Workshops/Intro-ML_Sklearn.html",
    "href": "Learn/Workshops/Intro-ML_Sklearn.html",
    "title": "Intro to Machine Learning with Sklearn",
    "section": "",
    "text": "The Intro to Machine Learning with Sklearn workshop will walk you through introductory machine learning concepts as well as how to implement common ML methods (e.g., regression, clustering, classication) using the popular scikit-learn (“sklearn”) package. Sklearn makes it possible to quickly fit and evaluate many models in just a few lines of code. It also comes with convenient functions needed for nearly all ML pipelines (e.g., train/test split, gridsearchcv). Note: Don’t use Sklearn for neural networks (it is the slowest option!). Instead, explore Keras or PyTorch.\n\n\nLearners are expected to have the following knowledge:\n\nBasic Python programming skills and familiarity with the Pandas package. If you need a refresher, these Introductory Python lesson materials are available for independent study.\n\n\n\n\nThis workshop takes approximately 8 hours to complete.\n\n\n\nThe Carpentries is a global organization of researchers who volunteer their time and effort to create workshops that teach software engineering and data analysis skills to other researchers. UW-Madison has its own local Carpentries community which is actively engaged in developing new ML/AI workshops. To be notified of upcoming workshops offered by the Carpentries, make sure to subscribe to the Data Science @ UW Newsletter.\n\n\n\nAll Carpentries lessons are published as open source educational materials. You are welcome and encouraged to visit the lesson materials to work through them on your own. If you are involved with a research lab at UW-Madison campus, you may attend Coding Meetup (Tue/Thur, 2:30-4:30pm) to get help working through the materials."
  },
  {
    "objectID": "Learn/Workshops/Intro-ML_Sklearn.html#about-this-resource",
    "href": "Learn/Workshops/Intro-ML_Sklearn.html#about-this-resource",
    "title": "Intro to Machine Learning with Sklearn",
    "section": "",
    "text": "The Intro to Machine Learning with Sklearn workshop will walk you through introductory machine learning concepts as well as how to implement common ML methods (e.g., regression, clustering, classication) using the popular scikit-learn (“sklearn”) package. Sklearn makes it possible to quickly fit and evaluate many models in just a few lines of code. It also comes with convenient functions needed for nearly all ML pipelines (e.g., train/test split, gridsearchcv). Note: Don’t use Sklearn for neural networks (it is the slowest option!). Instead, explore Keras or PyTorch.\n\n\nLearners are expected to have the following knowledge:\n\nBasic Python programming skills and familiarity with the Pandas package. If you need a refresher, these Introductory Python lesson materials are available for independent study.\n\n\n\n\nThis workshop takes approximately 8 hours to complete.\n\n\n\nThe Carpentries is a global organization of researchers who volunteer their time and effort to create workshops that teach software engineering and data analysis skills to other researchers. UW-Madison has its own local Carpentries community which is actively engaged in developing new ML/AI workshops. To be notified of upcoming workshops offered by the Carpentries, make sure to subscribe to the Data Science @ UW Newsletter.\n\n\n\nAll Carpentries lessons are published as open source educational materials. You are welcome and encouraged to visit the lesson materials to work through them on your own. If you are involved with a research lab at UW-Madison campus, you may attend Coding Meetup (Tue/Thur, 2:30-4:30pm) to get help working through the materials."
  },
  {
    "objectID": "Learn/Workshops/Intro-ML_Sklearn.html#questions",
    "href": "Learn/Workshops/Intro-ML_Sklearn.html#questions",
    "title": "Intro to Machine Learning with Sklearn",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Learn/Workshops/Intro-ML_Sklearn.html#see-also",
    "href": "Learn/Workshops/Intro-ML_Sklearn.html#see-also",
    "title": "Intro to Machine Learning with Sklearn",
    "section": "See also",
    "text": "See also\n\nWorkshop: Intro to Deep Learning with Keras: Once you master sklearn, start using Keras to build neural networks quickly.\nWorkshop: Intro to Deep Learning with PyTorch: Explore PyTorch as an alternative deep learning framework (faster but more verbose than Keras)\nBook: Understanding Deep Learning - Simon J.D. Prince: This free textbook is a good modern overview of deep learning, and provides colab notebooks to explore deep learning concepts and implementations. The book uses PyTorch as its framework of choice. You may find additional details in this book that the workshop only briefly touches on."
  },
  {
    "objectID": "Applications/Videos/Exploring-AI-at-UW/2023-09-15_ToddSchechter_Biomedical-AI-Research-and-Applications.html",
    "href": "Applications/Videos/Exploring-AI-at-UW/2023-09-15_ToddSchechter_Biomedical-AI-Research-and-Applications.html",
    "title": "Biomedical AI Research and Applications",
    "section": "",
    "text": "This session features lightning talks about biomedical applications of artificial intelligence from UW–Madison researchers working to develop algorithms and models to help us better understand the human body, improve communication and create treatments that improve and save lives.\nThis is the 12th installment in the Exploring AI@UW seires. You can learn more about the series, catch up on past seminars, and see what we have planned next on our “Exploring Artificial Intelligence @ UW–Madison” page.\n\nA netID is required to view this recording: View 2023-09-22 recording."
  },
  {
    "objectID": "Applications/Videos/Exploring-AI-at-UW/2023-09-15_ToddSchechter_Biomedical-AI-Research-and-Applications.html#see-also",
    "href": "Applications/Videos/Exploring-AI-at-UW/2023-09-15_ToddSchechter_Biomedical-AI-Research-and-Applications.html#see-also",
    "title": "Biomedical AI Research and Applications",
    "section": "See also",
    "text": "See also\n\nExploring Artificial Intelligence @ UW-Madison: Explore other talks from the Exploring AI @ UW series."
  },
  {
    "objectID": "Applications/Videos/Exploring-AI-at-UW/2023-07-28_LauraElbert_AI-In-Action-Intelligent-Systems-and-Business-Operations.html",
    "href": "Applications/Videos/Exploring-AI-at-UW/2023-07-28_LauraElbert_AI-In-Action-Intelligent-Systems-and-Business-Operations.html",
    "title": "AI in Action: Intelligent Systems and Business Operations",
    "section": "",
    "text": "Laura Albert, professor and the David Gustafson Department Chair of Industrial & Systems Engineering at the UW–Madison College of Engineering discusses how artificial intelligence is changing the American business landscape. This is the 6th installment in our webinar series about AI.\nThis is the 6th installment in our webinar series about AI. You can learn more about the series, catch up on past seminars, and see what we have planned next on our “Exploring Artificial Intelligence @ UW–Madison” page.\n\nA netID is required to view this recording: View 2023-07-28 recording."
  },
  {
    "objectID": "Applications/Videos/Exploring-AI-at-UW/2023-07-28_LauraElbert_AI-In-Action-Intelligent-Systems-and-Business-Operations.html#see-also",
    "href": "Applications/Videos/Exploring-AI-at-UW/2023-07-28_LauraElbert_AI-In-Action-Intelligent-Systems-and-Business-Operations.html#see-also",
    "title": "AI in Action: Intelligent Systems and Business Operations",
    "section": "See also",
    "text": "See also\n\nExploring Artificial Intelligence @ UW-Madison: Explore other talks from the Exploring AI @ UW series."
  },
  {
    "objectID": "Applications/Videos/Exploring-AI-at-UW/2023-09-1_LaurelBelman_AI-And-Medical-Imaging.html",
    "href": "Applications/Videos/Exploring-AI-at-UW/2023-09-1_LaurelBelman_AI-And-Medical-Imaging.html",
    "title": "AI and Medical Imaging",
    "section": "",
    "text": "Dr. Alan McMillan discusses the growing role of AI and machine learning in healthcare, particularly in radiology and medical imaging. He outlines how AI is being applied in tasks such as classification, regression, and segmentation, helping to enhance efficiency, diagnostic accuracy, and clinical decision-making. However, he also emphasizes current challenges, including ensuring HIPAA compliance, managing legal liability, and addressing algorithmic bias and poor robustness. The conversation also explores technical considerations like research reporting guidelines and interpretability of deep neural networks, as well as current radiology research on campus at UW-Madison utilizing AI. .\nA netID is required to view ML4MI videos: View 2023-08-11 recording."
  },
  {
    "objectID": "Applications/Videos/Exploring-AI-at-UW/2023-09-1_LaurelBelman_AI-And-Medical-Imaging.html#see-also",
    "href": "Applications/Videos/Exploring-AI-at-UW/2023-09-1_LaurelBelman_AI-And-Medical-Imaging.html#see-also",
    "title": "AI and Medical Imaging",
    "section": "See also",
    "text": "See also\n\nExploring Artificial Intelligence @ UW-Madison: Explore other talks from the Exploring AI @ UW series.\nML4MI: Learn more about AI’s application in medical imaging."
  },
  {
    "objectID": "Applications/Videos/ML4MI/index.html",
    "href": "Applications/Videos/ML4MI/index.html",
    "title": "ML4MI",
    "section": "",
    "text": "Explore a library of recordings (netID required) from the Machine Learning for Medical Imaging (ML4MI) community! Additional ML4MI talks can also be found on the UW-Madison Kaltura Mediaspace.\nFrom the ML4MI webpage:\n\nA regular seminar series began in February 2018, and includes 1) seminars describing technical developments in ML with potential biomedical applications, 2) seminars by local or external Radiology researchers, describing problems that may benefit from ML approaches and ongoing projects involving ML techniques, and 3) seminars by biomedical researchers (not in Radiology), describing pioneering experiences applying ML in their fields of study. The seminar location will alternate between ECB/WID and SMPH/WIMR. These seminars will also provide an opportunity for UW researchers to become familiar with researchers “on the other side of campus.”\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Electronic Health Record Data to Predict Deterioriation in Hospitalized Children\n\n\n\nVideos\n\nML4MI\n\nUW-Madison\n\nHealthcare\n\nNLP\n\nText analysis\n\nEHR\n\nBoosting\n\nDecision trees\n\n\n\n\n\n\n\n\n\n2024-10-14\n\n\nAnoop Mayampurath\n\n\n\n\n\n\n\n\n\n\n\n\nVision, Language, and Vision-Language Modeling in Radiology\n\n\n\nVideos\n\nML4MI\n\nUW-Madison\n\nMedical imaging\n\nHealthcare\n\nVLM\n\nViT\n\nUNET\n\nLLaVA\n\nComputer vision\n\nCNN\n\nLLM\n\nDeep learning\n\nMultimodal learning\n\n\n\n\n\n\n\n\n\n2024-09-16\n\n\nTyler Bradshaw\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Generative AI: An Introduction to Large Language Models and Diffusion Models\n\n\n\nVideos\n\nML4MI\n\nUW-Madison\n\nLLM\n\nVLM\n\nLLaVA\n\nLLM\n\nGenAI\n\nDiffusion\n\nDeep learning\n\nMultimodal learning\n\n\n\n\n\n\n\n\n\n2023-09-11\n\n\nKangwook Lee\n\n\n\n\n\n\n\n\n\n\n\n\nUnifying Audio-Visual Machine Perception – Tasks & Architectures\n\n\n\nVideos\n\nML4MI\n\nUW-Madison\n\nHealthcare\n\nMultimodal learning\n\nContrastive learning\n\nPerception\n\nEarly-fusion\n\nAutoencoder\n\n\n\n\n\n\n\n\n\n2023-07-12\n\n\nPedro Morgado\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Applications",
      "Videos",
      "ML4MI"
    ]
  },
  {
    "objectID": "Applications/Videos/ML4MI/2023-09-11_Exploring-Generative-AI-An-Intro-to-LLMs-and-Diffusion-Models_Kangwook-Lee.html",
    "href": "Applications/Videos/ML4MI/2023-09-11_Exploring-Generative-AI-An-Intro-to-LLMs-and-Diffusion-Models_Kangwook-Lee.html",
    "title": "Exploring Generative AI: An Introduction to Large Language Models and Diffusion Models",
    "section": "",
    "text": "In this talk from the Machine Learning for Medical Imaging (ML4MI) community, Kangwook Lee, PhD, provides a comprehensive introduction to generative AI through the lens of Large Language Models (LLMs) and diffusion-based models.\nDr. Lee begins by tracing the historical context leading to the development of modern LLMs, such as GPT, and explores how these models serve as universal interfaces to general intelligence through next-word prediction. By presenting key examples, including applications in software development and medical imaging, he demonstrates their transformative potential across diverse domains.\nThe talk also delves into Language-Interfaced Fine-Tuning (LIFT), a method for adapting LLMs to non-language machine learning tasks, and highlights Visual Instruction Tuning (VIT) work by Haotian Liu, showing how these advancements push the boundaries of multimodal learning.\nIn the second part of the presentation, Dr. Lee introduces diffusion-based generative models. He explains core concepts behind these models, emphasizing their unique approach to data synthesis and generative tasks. Additionally, topics such as LoRA (Low-Rank Adaptation) are briefly covered, showcasing methods to make training and fine-tuning efficient for large-scale models.\nA netID is required to view ML4MI videos: View 2023-09 ML4MI recording."
  },
  {
    "objectID": "Applications/Videos/ML4MI/2023-09-11_Exploring-Generative-AI-An-Intro-to-LLMs-and-Diffusion-Models_Kangwook-Lee.html#see-also",
    "href": "Applications/Videos/ML4MI/2023-09-11_Exploring-Generative-AI-An-Intro-to-LLMs-and-Diffusion-Models_Kangwook-Lee.html#see-also",
    "title": "Exploring Generative AI: An Introduction to Large Language Models and Diffusion Models",
    "section": "See also",
    "text": "See also\n\nApplication - Video: Exploring Model Sharing in the Age of Foundation Models: Learn more about the LLaVA model."
  },
  {
    "objectID": "Applications/Videos/SILO/index.html",
    "href": "Applications/Videos/SILO/index.html",
    "title": "SILO",
    "section": "",
    "text": "The Systems, Information, Learning and Optimization (SILO) research group at the University of Wisconsin-Madison hosts a weekly seminar that covers a variety of topics related to machine learning, optimization, and information theory. SILO breaks down the research “silos” created by academic department boundaries by providing time and space for researchers to present their work, interact, and find common threads.\nA non exhaustive list of recorded SILO seminars (feel free to add others!) can be found here on Nexus. Visit the SILO - Past Talks page to explore additional talks of interest.\n\nJoin the next live SILO!\n\nWhere: Orchard View Room (rm. 3280), Discovery Building & via Zoom.\nWhen: Check the SILO schedule and join the Google group to receive a calendar invite (with Zoom link) and other updates.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorld Knowledge in the Time of Large Models\n\n\n\nVideos\n\nSILO\n\nUW-Madison\n\nVLM\n\nLLM\n\nLMM\n\nMultimodal learning\n\nFoundation models\n\n\n\nThis talk will discuss the massive shift that has come about in the vision and ML community as a result of the large pre-trained language and language and vision models such as Flamingo, GPT-4, and other models.\n\n\n\n\n\n2023-11-22\n\n\nKenneth Marino\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Applications",
      "Videos",
      "SILO"
    ]
  },
  {
    "objectID": "Applications/Videos/Other/CrossLabsAI-CrossRoads45-METL-Biophysics-based-Protein-Language-Model.html",
    "href": "Applications/Videos/Other/CrossLabsAI-CrossRoads45-METL-Biophysics-based-Protein-Language-Model.html",
    "title": "A Biophysics-based Protein Language Model for Protein Engineering",
    "section": "",
    "text": "Summary from Cross Labs AI:\n\nJust as words combine to form sentences that convey meaning in human languages, the specific arrangement of amino acids in proteins can be viewed as an information-rich language describing molecular structure and behavior.\nProtein language models harness advances in natural language processing to decode intricate patterns and relationships within protein sequences. These models learn meaningful, low-dimensional representations that capture the semantic organization of protein space and have broad utility in protein engineering. However, while protein language models are powerful, they do not take advantage of the extensive knowledge of protein biophysics and molecular mechanisms acquired over the last century. Thus, they are largely unaware of the underlying physical principles governing protein function.\nWe introduce Mutational Effect Transfer Learning (METL), a specialized protein language model that bridges the gap between traditional biophysics-based and machine learning approaches by incorporating synthetic data from molecular simulations. We pretrain a transformer on millions of molecular simulations to capture the relationship between protein sequence, structure, energetics, and stability. We then finetune the neural network to harness these fundamental biophysical signals and apply them when predicting protein functional scores from experimental assays. METL excels in protein engineering tasks like generalizing from small training sets and extrapolating to new sequence positions. We demonstrate METL’s ability to design functional green fluorescent protein variants when trained on only 64 experimental examples.\n\n\n\nLinks & code\n\nAbout the speaker: Sam Gelamn, PhD → samgelman.com\nCheck out the preprint\nAll code is available under the MIT license. A collection of METL software repositories is provided to reproduce the results of this manuscript and run METL on new data:\n\ngithub.com/gitter-lab/metl for pretraining and finetuning METL PLMs (archived at doi:10.5281/zenodo.10819483)\ngithub.com/gitter-lab/metl-sim for generating biophysical attributes with Rosetta (archived at doi:10.5281/zenodo.10819523)\ngithub.com/gitter-lab/metl-pretrained for making predictions with pretrained METL PLMs (archived at doi:10.5281/zenodo.10819499)\ngithub.com/gitter-lab/metl-pub for additional code and data to reproduce these results (archived at doi:10.5281/zenodo.10819536)\n\n\n\n\n\nJump to section\n\n[3:01] Intro\n[5:03] Proteins as nature’s molecular machines\n[6:58] Proteins defined by a sequence of amino acids\n[10:34] Challenge: Vastness of sequence space\n[11:18] Navigating sequence space\n[11:52] Challenge: Small changes can have a large impact\n[12:51] Protein language models (PLMs)\n[15:57] Incorporating biophysics\n[17:33] Mutational Effect Transfer Learning (METL)\n[19:50] Simulating protein structures with Rosetta\n[21:10] Local and global strategies for simulations\n[24:26] Train transformer encoder to predict Rosetta scores\n[25:38] Finetune to predict experimental fitness score\n[27:10] Evaluation baselines (evolutionary models): METL, ESM, and EVE\n[28:26] Generalizing from small datasets\n[31:08] Extrapolating beyond train set\n[33:50] Simulating specific functions\n[35:46] How much simulated/experimental data is needed?\n[38:01] Engineering GFP variants with METL\n[41:40] Q&A"
  },
  {
    "objectID": "Applications/Videos/Other/index.html",
    "href": "Applications/Videos/Other/index.html",
    "title": "Other",
    "section": "",
    "text": "Explore talks from other groups besides ML+X, ML4MI, and SILO; on campus and beyond.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlphaFold and Protein Language Models\n\n\n\nVideos\n\nComBEE\n\nUW-Madison\n\nBiology\n\nProtein engineering\n\nLLM\n\nDeep learning\n\nAlphafold\n\nTransformer\n\nProtein language models\n\n\n\n\n\n\n\n\n\n2025-01-28\n\n\nHannah Wayment-Steele\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Out-of-Distribution Detection\n\n\n\nVideos\n\nICCV\n\nOOD detection\n\nTrustworthy AI\n\n\n\n\n\n\n\n\n\n2024-07-11\n\n\nSharon Li\n\n\n\n\n\n\n\n\n\n\n\n\nA Biophysics-based Protein Language Model for Protein Engineering\n\n\n\nVideos\n\nCross Labs AI\n\nUW-Madison\n\nTransfer learning\n\nBiology\n\nBiophysics\n\nProtein language models\n\nFoundation models\n\nLLM\n\nDeep learning\n\nProtein engineering\n\nSimulations\n\n\n\nWe introduce Mutational Effect Transfer Learning (METL), a specialized protein language model that bridges the gap between traditional biophysics-based and machine learning approaches by incorporating synthetic data from molecular simulations.\n\n\n\n\n\n2024-06-18\n\n\nSam Gelman\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Large Language Models for Meteorological Fact Finding\n\n\n\nVideos\n\nIT Prof\n\nUW-Madison\n\nLLM\n\nMeteorology\n\n\n\nThis talk demonstrates harnessing the power of AI to open new avenues in data analysis, including for meteorological fact-finding. Discover how cutting-edge large language models (LLMs) like OpenAI’s ChatGPT 3.5 and 4.0 hold are poised to help the field of meteorological data analysis.\n\n\n\n\n\n2024-05-30\n\n\nZekai Otles\n\n\n\n\n\n\n\n\n\n\n\n\nLabelBench: A Framework for Benchmarking Label-Efficient Learning\n\n\n\nVideos\n\nMLOPT\n\nUW-Madison\n\nActive learning\n\nLabel-efficient learning\n\nActive learning\n\nSemi-supervised\n\nViT\n\nBenchmarking\n\n\n\n\n\n\n\n\n\n2023-10-27\n\n\nJifan Zhang\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Applications",
      "Videos",
      "Other"
    ]
  },
  {
    "objectID": "Applications/Videos/Other/2024-02-23_LabelBench_Jifan-Zhang.html",
    "href": "Applications/Videos/Other/2024-02-23_LabelBench_Jifan-Zhang.html",
    "title": "LabelBench: A Framework for Benchmarking Label-Efficient Learning",
    "section": "",
    "text": "Labeled data are critical to modern machine learning applications, but obtaining labels can be expensive. To mitigate this cost, machine learning methods, such as transfer learning, semi-supervised learning and active learning, aim to be label-efficient: achieving high predictive performance from relatively few labeled examples. While obtaining the best label-efficiency in practice often requires combinations of these techniques, existing benchmark and evaluation frameworks do not capture a concerted combination of all such techniques. This paper addresses this deficiency by introducing LabelBench, a new computationally-efficient framework for joint evaluation of multiple label-efficient learning techniques. As an application of LabelBench, we introduce a novel benchmark of state-of-the-art active learning methods in combination with semi-supervised learning for fine-tuning pretrained vision transformers. Our benchmark demonstrates better label-efficiencies than previously reported in active learning. LabelBench’s modular codebase is open-sourced for the broader community to contribute label-efficient learning methods and benchmarks.\n\nSpeaker: Jifan Zhang (https://jifanz.github.io/)is a PhD student at UW-Madison working with Prof. Robert Nowak. His work focuses on label-efficient learning and its modern application to large-scale deep learning systems. He is also generally interested in human-in-the-loop learning and cross-disciplinary research that apply these methods in the real world.\nProject GitHub: https://github.com/EfficientTraining/LabelBench"
  },
  {
    "objectID": "Applications/Videos/Other/2024-02-23_LabelBench_Jifan-Zhang.html#see-also",
    "href": "Applications/Videos/Other/2024-02-23_LabelBench_Jifan-Zhang.html#see-also",
    "title": "LabelBench: A Framework for Benchmarking Label-Efficient Learning",
    "section": "See also",
    "text": "See also\n\nLearn: video: Theoretical Foundations of Active Machine Learning: Learn more about the fundamentals of active learning, presented by Prof. Rob Nowak."
  },
  {
    "objectID": "Applications/Videos/Forums/mlx_2025-05-06.html",
    "href": "Applications/Videos/Forums/mlx_2025-05-06.html",
    "title": "Generative AI From Multiple Perspectives",
    "section": "",
    "text": "This forum brings together two UW–Madison presenters exploring how generative AI can serve as a collaborative tool in high-stakes environments—whether surfacing clinical anomalies in health data or supporting deeper engagement in the classroom. Both talks emphasize that LLMs are most effective when paired with interpretable structures, such as statistical models or retrieval pipelines, and used in ways that prioritize transparency, safety, and human oversight.\n\n\nBenjamin Lengerich (Statistics & Computer Sciences) shares a framework for automating surprise detection in electronic health records (EHR) using generalized additive models (GAMs). These interpretable models decompose clinical features into univariate effects, which are then passed to LLMs to identify unexpected trends. Examples include counter-causal mortality patterns linked to biomarkers affected by treatment decisions. His team compares LLM-detected anomalies with human clinicians, proposing a hybrid system to support safer medical modeling. Several examples draw from both proprietary hospital data and the public MIMIC-IV dataset.\n\n\n\nKaiser Pister (Computer Sciences) describes how he built a retrieval-augmented chatbot for his programming languages course using lecture transcripts and Whisper-generated summaries. Students used the tool to ask questions, retrieve course content, and generate examples. He outlines a new direction where students “teach” an LLM a concept as a way to reinforce learning—turning the act of prompting into a pedagogical exercise."
  },
  {
    "objectID": "Applications/Videos/Forums/mlx_2025-05-06.html#about-this-resource",
    "href": "Applications/Videos/Forums/mlx_2025-05-06.html#about-this-resource",
    "title": "Generative AI From Multiple Perspectives",
    "section": "",
    "text": "This forum brings together two UW–Madison presenters exploring how generative AI can serve as a collaborative tool in high-stakes environments—whether surfacing clinical anomalies in health data or supporting deeper engagement in the classroom. Both talks emphasize that LLMs are most effective when paired with interpretable structures, such as statistical models or retrieval pipelines, and used in ways that prioritize transparency, safety, and human oversight.\n\n\nBenjamin Lengerich (Statistics & Computer Sciences) shares a framework for automating surprise detection in electronic health records (EHR) using generalized additive models (GAMs). These interpretable models decompose clinical features into univariate effects, which are then passed to LLMs to identify unexpected trends. Examples include counter-causal mortality patterns linked to biomarkers affected by treatment decisions. His team compares LLM-detected anomalies with human clinicians, proposing a hybrid system to support safer medical modeling. Several examples draw from both proprietary hospital data and the public MIMIC-IV dataset.\n\n\n\nKaiser Pister (Computer Sciences) describes how he built a retrieval-augmented chatbot for his programming languages course using lecture transcripts and Whisper-generated summaries. Students used the tool to ask questions, retrieve course content, and generate examples. He outlines a new direction where students “teach” an LLM a concept as a way to reinforce learning—turning the act of prompting into a pedagogical exercise."
  },
  {
    "objectID": "Applications/Videos/Forums/mlx_2025-05-06.html#see-also",
    "href": "Applications/Videos/Forums/mlx_2025-05-06.html#see-also",
    "title": "Generative AI From Multiple Perspectives",
    "section": "See also",
    "text": "See also\n\nMIMIC-IV dataset\n\nWhisper speech recognition model\n\nVideo: Automating Scientific Discovery: From Natural World Data to Systematic Literature Reviews"
  },
  {
    "objectID": "Applications/Videos/Forums/index.html",
    "href": "Applications/Videos/Forums/index.html",
    "title": "ML+X Forums",
    "section": "",
    "text": "Explore our library of ML+X forum recordings below! Each monthly ML+X forum highlights two ML applications that share a theme followed by communal discussions and project feedback.\n\nJoin the next live forum!\n\nWhere: Orchard View Room (rm. 3280), Discovery Building & via Zoom.\nWhen: Typically monthly on Tuesdays, 12-1pm CT. Join the Google group to receive a calendar invite (with Zoom link) and other updates. Please email us if you have any trouble joining.\n\n\n\nShare your work!\nWe encourage anyone who is using ML in their work to present at one of the ML+X forums. If interested, please fill out this brief presenter sign-up form.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative AI From Multiple Perspectives\n\n\n\nVideos\n\nML+X\n\nLLM\n\nHealthcare\n\nEHR\n\nExplainability\n\nDeep learning\n\nEducation\n\nRetrieval\n\nRAG\n\nNLP\n\n\n\n\n\n\n\n\n\n2025-05-06\n\n\nBenjamin Lengerich, Kaiser Pister\n\n\n\n\n\n\n\n\n\n\n\n\nHarnessing the Power of Foundation Models for Healthcare and Life Sciences\n\n\n\nVideos\n\nML+X\n\nHealthcare\n\nMedical imaging\n\nFoundation models\n\nMultimodal learning\n\nDeep learning\n\nEmbeddings\n\nImage data\n\nComputer vision\n\nRetrieval\n\nZero-shot learning\n\nLLM\n\nMicrosoft Copilot\n\nAzure\n\n\n\n\n\n\n\n\n\n2025-04-01\n\n\nJameson Merkow\n\n\n\n\n\n\n\n\n\n\n\n\nAutomating Scientific Discovery: From Natural World Data to Systematic Literature Reviews\n\n\n\nVideos\n\nML+X\n\nMultimodal learning\n\nBenchmarking\n\nRetrieval\n\nComputer vision\n\nZero-shot learning\n\nContrastive learning\n\nDeep learning\n\nCLIP\n\nBiology\n\nEcology\n\nImage data\n\nLLM\n\nHugging Face\n\n\n\n\n\n\n\n\n\n2025-03-04\n\n\nEdward Vendrow\n\n\n\n\n\n\n\n\n\n\n\n\nLearning Through Comparison: Use Cases of Contrastive Learning\n\n\n\nVideos\n\nML+X\n\nUW-Madison\n\nDeep learning\n\nContrastive learning\n\nClustering\n\nOOD detection\n\nMultimodal learning\n\nTrustworthy AI\n\nRepresentation learning\n\nCIFAR\n\n\n\n\n\n\n\n\n\n2025-02-10\n\n\nYin Li, Chris Endemann\n\n\n\n\n\n\n\n\n\n\n\n\nAI for Music and the Humanities\n\n\n\nVideos\n\nML+X\n\nUW-Madison\n\nDeep learning\n\nGenAI\n\nConformer\n\nHumanities\n\nAudio search\n\nMusic\n\nCSI\n\nSignal processing\n\nSpectral analysis\n\nLLM\n\n\n\n\nWhat Tune Is That: A Humanities Application of Deep Learning — Alan Ng \nFake Artists, Fake Listeners: AI and the Music Industries — Jeremy Morris, PhD\n\n\n\n\n\n\n2024-11-05\n\n\nAlan Ng, Jeremy Morris\n\n\n\n\n\n\n\n\n\n\n\n\nTrustworthy LLMs & Ethical AI\n\n\n\nVideos\n\nML+X\n\nUW-Madison\n\nNLP\n\nText analysis\n\nLLM\n\nDeep learning\n\nTrustworthy AI\n\nDeTox\n\nEthical AI\n\nTopic modeling\n\n\n\n\nDeTox: Denoised Toxic Embeddings for Editing Model Toxicity — Rheeya Uppaal\nA Project on AI Ethics — Mariab A. Knowles\n\n\n\n\n\n\n2024-05-11\n\n\nRheeya Uppaal, Mariah A. Knowles\n\n\n\n\n\n\n\n\n\n\n\n\nAdvancing Healthcare and Agriculture through Computer Vision\n\n\n\nVideos\n\nML+X\n\nUW-Madison\n\nComputer vision\n\nUltrasound\n\nMedical imaging\n\nAthletics\n\nAgriculture\n\nLSTM\n\nCNN-LSTM\n\nCNN\n\nDeep learning\n\n\n\n\nAn ultrasound-based method to measure knee kinematics enabled by deep learning — Matthew Blomquist\nPlant breeding in the age of computer vision — Will de la Bretonne\n\n\n\n\n\n\n2024-04-09\n\n\nMatthew Blomquist, Will de la Bretonne\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Model Sharing in the Age of Foundation Models\n\n\n\nVideos\n\nML+X\n\nUW-Madison\n\nMultimodal learning\n\nFoundation models\n\nModel sharing\n\nHugging Face\n\nLLM\n\nLMM\n\nLLaVA\n\nDeep learning\n\n\n\n\nModel sharing and reproducible ML — Chris Endemann\nLLaVA-NeXT and model sharing — Haotian Liu, PhD\n\n\n\n\n\n\n2024-03-12\n\n\nChris Endemann, Haotian Liu\n\n\n\n\n\n\n\n\n\n\n\n\nNavigating Gravitational Waves with AI Insights\n\n\n\nVideos\n\nML+X\n\nUW-Madison\n\nPhysics\n\nSimulations\n\nSpectral analysis\n\nSignal processing\n\n\n\n\nWelcome and small group discussions — Chris Endemann\nClassifying gravitational wave modes from core-collapse supernovae — Bella Finkel\n\n\n\n\n\n\n2024-02-13\n\n\nChris Endemann, Bella Finkel\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Science Communication and Drug Synergy Analysis using GPT\n\n\n\nVideos\n\nML+X\n\nUW-Madison\n\nScience communication\n\nHealthcare\n\nDrug synergy\n\nLLM\n\nText mining\n\nNLP\n\n\n\n\n\n\n\n\n\n2023-12-12\n\n\nBen Rush, Jack Freeman\n\n\n\n\n\n\n\n\n\n\n\n\nLLMS in Genomics and Health Coaching\n\n\n\nVideos\n\nML+X\n\nUW-Madison\n\nHealthcare\n\nClustering\n\nDeep learning\n\nLLM\n\nGenomics\n\n\n\n\nClustering of genomic sequences of mycoviruses using deep learning — Rohan Sontahlia\nSpurring self-improvement and intrinsic motivation using LLMs and reinforcement learning — Michael Roytman\n\n\n\n\n\n\n2023-11-07\n\n\nRohan Sontahlia, Michael Roytman\n\n\n\n\n\n\n\n\n\n\n\n\nTime-Series Analysis\n\n\n\nVideos\n\nML+X\n\nUW-Madison\n\nTime-series\n\nGenomics\n\nBiology\n\nHealthcare\n\n\n\n\n\n\n\n\n\n2023-10-10\n\n\nPeng Jiang, Sourav Pal\n\n\n\n\n\n\n\n\n\n\n\n\nMultimodal Learning\n\n\n\nVideos\n\nML+X\n\nUW-Madison\n\nMultimodal learning\n\nDeep learning\n\nComputer vision\n\nHealthcare\n\nGenomics\n\n\n\n\n\n\n\n\n\n2023-09-19\n\n\nDaifeng Wang, Zachary Huemann, Pedro Morgado\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Applications",
      "Videos",
      "ML+X"
    ]
  },
  {
    "objectID": "Applications/Videos/Forums/mlx_2025-03-04.html",
    "href": "Applications/Videos/Forums/mlx_2025-03-04.html",
    "title": "Automating Scientific Discovery: From Natural World Data to Systematic Literature Reviews",
    "section": "",
    "text": "Edward Vendrow, a PhD student at MIT, presents his research on automating scientific discovery using multimodal AI. In this talk, he explores how AI can accelerate research through large-scale ecological image analysis and systematic literature reviews.\n\n\nThe talk introduces INQUIRE, a benchmark dataset designed to evaluate AI systems on expert-level ecological queries. By applying multimodal AI models, researchers can search through vast datasets like iNaturalist to answer scientific questions. For example, identifying individual animals with unique tags, studying human impact on wildlife, and monitoring environmental changes. INQUIRE enables more accurate and scalable insights by providing labeled ecological queries over millions of images.\n\n\n\nEdward also discusses how AI models such as GPT-4 can reduce the burden of systematic literature reviews, a critical yet time-intensive process in research. Using large language models, researchers can efficiently screen and analyze scientific papers, helping them identify relevant studies with greater accuracy.\n\n\n\nFrom monitoring seasonal variations in avian diets to exploring the effects of climate change on plant flowering phenology, AI-driven tools like INQUIRE are unlocking new research possibilities. Looking ahead, Edward’s work explores the potential of AI systems in autonomous species discovery and broader ecological research."
  },
  {
    "objectID": "Applications/Videos/Forums/mlx_2025-03-04.html#about-this-resource",
    "href": "Applications/Videos/Forums/mlx_2025-03-04.html#about-this-resource",
    "title": "Automating Scientific Discovery: From Natural World Data to Systematic Literature Reviews",
    "section": "",
    "text": "Edward Vendrow, a PhD student at MIT, presents his research on automating scientific discovery using multimodal AI. In this talk, he explores how AI can accelerate research through large-scale ecological image analysis and systematic literature reviews.\n\n\nThe talk introduces INQUIRE, a benchmark dataset designed to evaluate AI systems on expert-level ecological queries. By applying multimodal AI models, researchers can search through vast datasets like iNaturalist to answer scientific questions. For example, identifying individual animals with unique tags, studying human impact on wildlife, and monitoring environmental changes. INQUIRE enables more accurate and scalable insights by providing labeled ecological queries over millions of images.\n\n\n\nEdward also discusses how AI models such as GPT-4 can reduce the burden of systematic literature reviews, a critical yet time-intensive process in research. Using large language models, researchers can efficiently screen and analyze scientific papers, helping them identify relevant studies with greater accuracy.\n\n\n\nFrom monitoring seasonal variations in avian diets to exploring the effects of climate change on plant flowering phenology, AI-driven tools like INQUIRE are unlocking new research possibilities. Looking ahead, Edward’s work explores the potential of AI systems in autonomous species discovery and broader ecological research."
  },
  {
    "objectID": "Applications/Videos/Forums/mlx_2025-03-04.html#see-also",
    "href": "Applications/Videos/Forums/mlx_2025-03-04.html#see-also",
    "title": "Automating Scientific Discovery: From Natural World Data to Systematic Literature Reviews",
    "section": "See also",
    "text": "See also\n\nData: INQUIRE Benchmark: Evaluate multimodal AI models on ecological image retrieval tasks.\nData: iNaturalist: Learn more about the iNaturalist dataset.\nTalk: Learning Through Comparison: Use Cases of Contrastive Learning: Learn about the fundamentals and use-cases of contrastive learning, including multimodal learning."
  },
  {
    "objectID": "Applications/Videos/Forums/mlx_2023-12-12.html",
    "href": "Applications/Videos/Forums/mlx_2023-12-12.html",
    "title": "Exploring Science Communication and Drug Synergy Analysis using GPT",
    "section": "",
    "text": "GPT for Science Communication: User-Interface and Developer Pipeline Approaches — Ben Rush, PhD\nAdvancing Biomedical Research with GPT-4: A Novel Approach to Drug Synergy Analysis using Text Mining and Classification — Jack Freeman"
  },
  {
    "objectID": "Applications/index.html",
    "href": "Applications/index.html",
    "title": "Applications",
    "section": "",
    "text": "Discover a curated collection of blogs, papers, and talks which dive into ML/AI applications and lessons learned by practitioners.\nNote: A netID is required to view the ML4MI and the Exploring AI @ UW video series.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative AI From Multiple Perspectives\n\n\n\nVideos\n\nML+X\n\nLLM\n\nHealthcare\n\nEHR\n\nExplainability\n\nDeep learning\n\nEducation\n\nRetrieval\n\nRAG\n\nNLP\n\n\n\n\n\n\n\n\n\n2025-05-06\n\n\nBenjamin Lengerich, Kaiser Pister\n\n\n\n\n\n\n\n\n\n\n\n\nHarnessing the Power of Foundation Models for Healthcare and Life Sciences\n\n\n\nVideos\n\nML+X\n\nHealthcare\n\nMedical imaging\n\nFoundation models\n\nMultimodal learning\n\nDeep learning\n\nEmbeddings\n\nImage data\n\nComputer vision\n\nRetrieval\n\nZero-shot learning\n\nLLM\n\nMicrosoft Copilot\n\nAzure\n\n\n\n\n\n\n\n\n\n2025-04-01\n\n\nJameson Merkow\n\n\n\n\n\n\n\n\n\n\n\n\nAutomating Scientific Discovery: From Natural World Data to Systematic Literature Reviews\n\n\n\nVideos\n\nML+X\n\nMultimodal learning\n\nBenchmarking\n\nRetrieval\n\nComputer vision\n\nZero-shot learning\n\nContrastive learning\n\nDeep learning\n\nCLIP\n\nBiology\n\nEcology\n\nImage data\n\nLLM\n\nHugging Face\n\n\n\n\n\n\n\n\n\n2025-03-04\n\n\nEdward Vendrow\n\n\n\n\n\n\n\n\n\n\n\n\nLearning Through Comparison: Use Cases of Contrastive Learning\n\n\n\nVideos\n\nML+X\n\nUW-Madison\n\nDeep learning\n\nContrastive learning\n\nClustering\n\nOOD detection\n\nMultimodal learning\n\nTrustworthy AI\n\nRepresentation learning\n\nCIFAR\n\n\n\n\n\n\n\n\n\n2025-02-10\n\n\nYin Li, Chris Endemann\n\n\n\n\n\n\n\n\n\n\n\n\nAlphaFold and Protein Language Models\n\n\n\nVideos\n\nComBEE\n\nUW-Madison\n\nBiology\n\nProtein engineering\n\nLLM\n\nDeep learning\n\nAlphafold\n\nTransformer\n\nProtein language models\n\n\n\n\n\n\n\n\n\n2025-01-28\n\n\nHannah Wayment-Steele\n\n\n\n\n\n\n\n\n\n\n\n\nAI for Music and the Humanities\n\n\n\nVideos\n\nML+X\n\nUW-Madison\n\nDeep learning\n\nGenAI\n\nConformer\n\nHumanities\n\nAudio search\n\nMusic\n\nCSI\n\nSignal processing\n\nSpectral analysis\n\nLLM\n\n\n\n\nWhat Tune Is That: A Humanities Application of Deep Learning — Alan Ng \nFake Artists, Fake Listeners: AI and the Music Industries — Jeremy Morris, PhD\n\n\n\n\n\n\n2024-11-05\n\n\nAlan Ng, Jeremy Morris\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Electronic Health Record Data to Predict Deterioriation in Hospitalized Children\n\n\n\nVideos\n\nML4MI\n\nUW-Madison\n\nHealthcare\n\nNLP\n\nText analysis\n\nEHR\n\nBoosting\n\nDecision trees\n\n\n\n\n\n\n\n\n\n2024-10-14\n\n\nAnoop Mayampurath\n\n\n\n\n\n\n\n\n\n\n\n\nVision, Language, and Vision-Language Modeling in Radiology\n\n\n\nVideos\n\nML4MI\n\nUW-Madison\n\nMedical imaging\n\nHealthcare\n\nVLM\n\nViT\n\nUNET\n\nLLaVA\n\nComputer vision\n\nCNN\n\nLLM\n\nDeep learning\n\nMultimodal learning\n\n\n\n\n\n\n\n\n\n2024-09-16\n\n\nTyler Bradshaw\n\n\n\n\n\n\n\n\n\n\n\n\nWhat Tune Is That? A Humanities Application of Deep Learning\n\n\n\nAudio search\n\nBlogs\n\nDeep learning\n\nConformer\n\nTransformer\n\nCNN\n\nHumanities\n\nAudio data\n\nMusic\n\nPyTorch\n\nCSI\n\nTime-series\n\nSignal processing\n\nSpectral analysis\n\n\n\n\n\n\n\n\n\n2024-09-11\n\n\nAlan Ng\n\n\n\n\n\n\n\n\n\n\n\n\nA Biophysics-based Protein Language Model for Protein Engineering\n\n\n\nVideos\n\nCross Labs AI\n\nUW-Madison\n\nTransfer learning\n\nBiology\n\nBiophysics\n\nProtein language models\n\nFoundation models\n\nLLM\n\nDeep learning\n\nProtein engineering\n\nSimulations\n\n\n\nWe introduce Mutational Effect Transfer Learning (METL), a specialized protein language model that bridges the gap between traditional biophysics-based and machine learning approaches by incorporating synthetic data from molecular simulations.\n\n\n\n\n\n2024-06-18\n\n\nSam Gelman\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Large Language Models for Meteorological Fact Finding\n\n\n\nVideos\n\nIT Prof\n\nUW-Madison\n\nLLM\n\nMeteorology\n\n\n\nThis talk demonstrates harnessing the power of AI to open new avenues in data analysis, including for meteorological fact-finding. Discover how cutting-edge large language models (LLMs) like OpenAI’s ChatGPT 3.5 and 4.0 hold are poised to help the field of meteorological data analysis.\n\n\n\n\n\n2024-05-30\n\n\nZekai Otles\n\n\n\n\n\n\n\n\n\n\n\n\nTrustworthy LLMs & Ethical AI\n\n\n\nVideos\n\nML+X\n\nUW-Madison\n\nNLP\n\nText analysis\n\nLLM\n\nDeep learning\n\nTrustworthy AI\n\nDeTox\n\nEthical AI\n\nTopic modeling\n\n\n\n\nDeTox: Denoised Toxic Embeddings for Editing Model Toxicity — Rheeya Uppaal\nA Project on AI Ethics — Mariab A. Knowles\n\n\n\n\n\n\n2024-05-11\n\n\nRheeya Uppaal, Mariah A. Knowles\n\n\n\n\n\n\n\n\n\n\n\n\nAdvancing Healthcare and Agriculture through Computer Vision\n\n\n\nVideos\n\nML+X\n\nUW-Madison\n\nComputer vision\n\nUltrasound\n\nMedical imaging\n\nAthletics\n\nAgriculture\n\nLSTM\n\nCNN-LSTM\n\nCNN\n\nDeep learning\n\n\n\n\nAn ultrasound-based method to measure knee kinematics enabled by deep learning — Matthew Blomquist\nPlant breeding in the age of computer vision — Will de la Bretonne\n\n\n\n\n\n\n2024-04-09\n\n\nMatthew Blomquist, Will de la Bretonne\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Model Sharing in the Age of Foundation Models\n\n\n\nVideos\n\nML+X\n\nUW-Madison\n\nMultimodal learning\n\nFoundation models\n\nModel sharing\n\nHugging Face\n\nLLM\n\nLMM\n\nLLaVA\n\nDeep learning\n\n\n\n\nModel sharing and reproducible ML — Chris Endemann\nLLaVA-NeXT and model sharing — Haotian Liu, PhD\n\n\n\n\n\n\n2024-03-12\n\n\nChris Endemann, Haotian Liu\n\n\n\n\n\n\n\n\n\n\n\n\nNavigating Gravitational Waves with AI Insights\n\n\n\nVideos\n\nML+X\n\nUW-Madison\n\nPhysics\n\nSimulations\n\nSpectral analysis\n\nSignal processing\n\n\n\n\nWelcome and small group discussions — Chris Endemann\nClassifying gravitational wave modes from core-collapse supernovae — Bella Finkel\n\n\n\n\n\n\n2024-02-13\n\n\nChris Endemann, Bella Finkel\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Science Communication and Drug Synergy Analysis using GPT\n\n\n\nVideos\n\nML+X\n\nUW-Madison\n\nScience communication\n\nHealthcare\n\nDrug synergy\n\nLLM\n\nText mining\n\nNLP\n\n\n\n\n\n\n\n\n\n2023-12-12\n\n\nBen Rush, Jack Freeman\n\n\n\n\n\n\n\n\n\n\n\n\nWorld Knowledge in the Time of Large Models\n\n\n\nVideos\n\nSILO\n\nUW-Madison\n\nVLM\n\nLLM\n\nLMM\n\nMultimodal learning\n\nFoundation models\n\n\n\nThis talk will discuss the massive shift that has come about in the vision and ML community as a result of the large pre-trained language and language and vision models such as Flamingo, GPT-4, and other models.\n\n\n\n\n\n2023-11-22\n\n\nKenneth Marino\n\n\n\n\n\n\n\n\n\n\n\n\nLLMS in Genomics and Health Coaching\n\n\n\nVideos\n\nML+X\n\nUW-Madison\n\nHealthcare\n\nClustering\n\nDeep learning\n\nLLM\n\nGenomics\n\n\n\n\nClustering of genomic sequences of mycoviruses using deep learning — Rohan Sontahlia\nSpurring self-improvement and intrinsic motivation using LLMs and reinforcement learning — Michael Roytman\n\n\n\n\n\n\n2023-11-07\n\n\nRohan Sontahlia, Michael Roytman\n\n\n\n\n\n\n\n\n\n\n\n\nLabelBench: A Framework for Benchmarking Label-Efficient Learning\n\n\n\nVideos\n\nMLOPT\n\nUW-Madison\n\nActive learning\n\nLabel-efficient learning\n\nActive learning\n\nSemi-supervised\n\nViT\n\nBenchmarking\n\n\n\n\n\n\n\n\n\n2023-10-27\n\n\nJifan Zhang\n\n\n\n\n\n\n\n\n\n\n\n\nTime-Series Analysis\n\n\n\nVideos\n\nML+X\n\nUW-Madison\n\nTime-series\n\nGenomics\n\nBiology\n\nHealthcare\n\n\n\n\n\n\n\n\n\n2023-10-10\n\n\nPeng Jiang, Sourav Pal\n\n\n\n\n\n\n\n\n\n\n\n\nBiomedical AI Research and Applications\n\n\n\nVideos\n\nUW-Madison\n\nExploring AI@UW\n\nBiology\n\nHealthcare\n\nLLM\n\nComputer vision\n\nMultimodal learning\n\n\n\n\n\n\n\n\n\n2023-09-22\n\n\nAnthony Gitter, Junjie Hu, Yin Li, Anoop Mayampurath, Vikas Singh\n\n\n\n\n\n\n\n\n\n\n\n\nRacism in AI\n\n\n\nVideos\n\nUW-Madison\n\nExploring AI@UW\n\nEthical AI\n\nBias\n\nFairness\n\n\n\n\n\n\n\n\n\n2023-09-22\n\n\nLori Kido Lopez\n\n\n\n\n\n\n\n\n\n\n\n\nMultimodal Learning\n\n\n\nVideos\n\nML+X\n\nUW-Madison\n\nMultimodal learning\n\nDeep learning\n\nComputer vision\n\nHealthcare\n\nGenomics\n\n\n\n\n\n\n\n\n\n2023-09-19\n\n\nDaifeng Wang, Zachary Huemann, Pedro Morgado\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Generative AI: An Introduction to Large Language Models and Diffusion Models\n\n\n\nVideos\n\nML4MI\n\nUW-Madison\n\nLLM\n\nVLM\n\nLLaVA\n\nLLM\n\nGenAI\n\nDiffusion\n\nDeep learning\n\nMultimodal learning\n\n\n\n\n\n\n\n\n\n2023-09-11\n\n\nKangwook Lee\n\n\n\n\n\n\n\n\n\n\n\n\nAI and Medical Imaging\n\n\n\nVideos\n\nExploring AI@UW\n\nUW-Madison\n\nHealthcare\n\nMedical imaging\n\nImage classification\n\nComputer vision\n\nDeep learning\n\n\n\n\n\n\n\n\n\n2023-08-11\n\n\nLaurel Belman, Todd Shechter, Alan McMillan\n\n\n\n\n\n\n\n\n\n\n\n\nAI in Action: Intelligent Systems and Business Operations\n\n\n\nVideos\n\nUW-Madison\n\nExploring AI@UW\n\nBusiness\n\nBias\n\nEthical AI\n\n\n\n\n\n\n\n\n\n2023-07-28\n\n\nLaura Albert\n\n\n\n\n\n\n\n\n\n\n\n\nUnifying Audio-Visual Machine Perception – Tasks & Architectures\n\n\n\nVideos\n\nML4MI\n\nUW-Madison\n\nHealthcare\n\nMultimodal learning\n\nContrastive learning\n\nPerception\n\nEarly-fusion\n\nAutoencoder\n\n\n\n\n\n\n\n\n\n2023-07-12\n\n\nPedro Morgado\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Applications"
    ]
  },
  {
    "objectID": "Applications/Blogs/index.html",
    "href": "Applications/Blogs/index.html",
    "title": "Blogs",
    "section": "",
    "text": "Share your ML story!\nAre you currently immersed in an exciting ML project? We want to hear about it! Share your insights, challenges, and successes by contributing a blog post to Nexus, the ML+X resource sharing platform.\nWhether you’re exploring ML applications in biology, engineering, social sciences, or any other field, your unique perspective is invaluable. Showcase your innovation, research, and creativity to inspire others in the ML+X community.\nGet started by posting a brief summary of your blog idea as an Issue on GitHub! The Nexus development team will follow-up with further instructions.\n\n\nExplore blogs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat Tune Is That? A Humanities Application of Deep Learning\n\n\n\nAudio search\n\nBlogs\n\nDeep learning\n\nConformer\n\nTransformer\n\nCNN\n\nHumanities\n\nAudio data\n\nMusic\n\nPyTorch\n\nCSI\n\nTime-series\n\nSignal processing\n\nSpectral analysis\n\n\n\n\n\n\n\n\n\n2024-09-11\n\n\nAlan Ng\n\n13 min\n\n\n\n\nNo matching items",
    "crumbs": [
      "Applications",
      "Blogs"
    ]
  },
  {
    "objectID": "includes/genAI-disclaimer.html",
    "href": "includes/genAI-disclaimer.html",
    "title": "Nexus: Crowdsourced ML Resources",
    "section": "",
    "text": "UW–Madison faculty, staff, students, and affiliates are required to follow campus policies relevant to AI use. Uses of generative AI that are explicitly prohibited by policy include, but are not limited to, the following:\n\nEntering any sensitive, restricted or otherwise protected institutional data – including hard-coded passwords – into any generative AI tool or service;\nUsing AI-generated code for institutional IT systems or services without review by a human to verify the absence of malicious elements;\nUsing generative AI to violate laws; institutional policies, rules or guidelines; or agreements or contracts."
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "Glossary of Categories",
    "section": "",
    "text": "Glossary of Categories\n\n\n\n\n\n\n\n\nTerm\nMeaning\nDescription\n\n\n\n\nAgriculture\nN/A\nThe application of machine learning in agricultural practices, including crop prediction, pest detection, and yield optimization. Often involves satellite imaging and IoT data.\n\n\nBiophysics\nN/A\nThe study of biological processes through the methods of physics. Machine learning is increasingly applied in areas like protein structure prediction and molecular dynamics simulations.\n\n\nBooks\nN/A\nComprehensive resources for learning and reference. Often written by experts, these provide in-depth coverage of machine learning topics.\n\n\nCarpentries\nN/A\nHands-on, interactive learning sessions focusing on foundational coding and data skills. Facilitates skill acquisition through real-world examples and active learning.\n\n\nClassical ML\nClassical Machine Learning\nRefers to traditional ML algorithms like SVMs, decision trees, and k-means clustering. These methods are well-established, interpretable, and often less resource-intensive than deep learning.\n\n\nClustering\nN/A\nThe process of grouping a set of objects in such a way that objects in the same group (cluster) are more similar to each other than to those in other groups. Common algorithms include k-means and DBSCAN.\n\n\nCNN\nConvolutional Neural Network\nA class of deep learning models particularly well-suited for image processing tasks. Known for their ability to automatically learn spatial hierarchies of features. Common libraries include TensorFlow and Keras.\n\n\nCNN-LSTM\nConvolutional Neural Network - Long Short-Term Memory\nA hybrid model combining CNNs for feature extraction from images or sequences and LSTMs for handling time dependencies. Used in applications like video classification.\n\n\nCode-along\nN/A\nLive coding sessions where learners write code simultaneously with the instructor, which enhances learning through practice and immediate application of concepts.\n\n\nCompute\nN/A\nRefers to computational resources, often in the context of machine learning, such as CPUs, GPUs, and cloud computing. Critical for training models, especially deep learning models.\n\n\nComputer vision\nN/A\nA field of machine learning focused on enabling computers to interpret and make decisions based on visual data. Applications include image recognition, object detection, and facial recognition. Libraries include OpenCV, PyTorch, and TensorFlow.\n\n\nContribute\nN/A\nInvolves contributing to open-source projects, fostering collaboration, improving software quality, and providing community support.\n\n\nCSI\nCover Song Identification\nA task in music information retrieval that involves identifying whether one song is a cover version of another. Machine learning models for CSI often analyze harmonic, melodic, and rhythmic similarities across versions. Popular approaches include feature-based methods and deep learning models.\n\n\nCross Labs AI\nN/A\nRefers to interdisciplinary AI research and development across various labs or research groups, often involving collaboration between different fields such as computer science, biology, and engineering.\n\n\nDeep learning\nN/A\nA subset of machine learning involving neural networks with many layers, used for complex tasks like image recognition and natural language processing. Commonly used libraries include TensorFlow, PyTorch, and Keras.\n\n\nDrug synergy\nN/A\nThe study and identification of drug combinations that produce a greater effect together than individually. Machine learning is used to predict synergistic drug pairs.\n\n\nEmpirical patterns\nN/A\nObserved patterns in data identified through experimentation and analysis. These patterns help inform the development of models and algorithms.\n\n\nForums\nN/A\nOnline platforms where members of the machine learning community can ask questions, share knowledge, and collaborate on projects. Examples include Reddit, Stack Overflow, and specialized forums like Kaggle.\n\n\nFoundation models\nN/A\nLarge-scale pre-trained models that serve as a base for fine-tuning on specific tasks. They underpin many state-of-the-art NLP and computer vision systems. Examples include GPT-3, BERT, and CLIP.\n\n\nGenomics\nN/A\nThe study of genomes, often involving the analysis of DNA sequences. Machine learning aids in tasks like gene prediction, mutation analysis, and personalized medicine. Libraries include Scikit-learn, TensorFlow, and PyTorch.\n\n\nGit/GitHub\nN/A\nVersion control system (Git) and the associated platform (GitHub) for hosting and sharing code. Essential tools for collaboration and project management in software development, including ML projects.\n\n\nGrokking\nN/A\nA phenomenon in machine learning where a model unexpectedly generalizes well after many training iterations, often after initially performing poorly. Highlights the non-linear relationship between training time and model performance.\n\n\nGuides\nN/A\nDetailed instructions or explanations, often in the form of tutorials or documentation, aimed at helping users understand and apply specific concepts or tools.\n\n\nGPU\nGraphics Processing Unit\nA specialized processor that accelerates the creation of images in a frame buffer intended for output to a display. Widely used in deep learning for parallel processing capabilities.\n\n\nHealthcare\nN/A\nThe application of machine learning in the healthcare industry, including areas like medical imaging, diagnostics, and personalized treatment plans. Common challenges include data privacy and interpretability.\n\n\nHugging Face\nN/A\nA popular platform for sharing pre-trained models, datasets, and other machine learning resources, especially in NLP. Provides tools like the Transformers library for easy model deployment.\n\n\nIndustry applications\nN/A\nRefers to the use of machine learning across various industries such as finance, manufacturing, and logistics, for tasks like predictive maintenance, fraud detection, and supply chain optimization.\n\n\nKeras\nN/A\nA high-level neural networks API written in Python, capable of running on top of TensorFlow, Theano, or CNTK. It allows for easy and fast prototyping of deep learning models.\n\n\nKnowledge-based\nN/A\nRefers to models or systems that incorporate domain-specific knowledge, often encoded in rules or ontologies, to improve decision-making or interpretability. Examples include expert systems and knowledge graphs.\n\n\nLLaVA\nLarge Language and Vision Assistant\nA multimodal AI model that combines language and vision understanding, capable of processing and generating both text and images.\n\n\nLLM\nLarge Language Model\nA type of deep learning model that can process and generate human-like text by understanding context from vast amounts of data. Examples include GPT-3 and BERT.\n\n\nLMM\nLarge Multimodal Model\nA class of models that can process and generate content across different modalities such as text, image, and audio. Examples include CLIP and DALL-E.\n\n\nMedical imaging\nN/A\nThe application of machine learning techniques to analyze and interpret medical images. Common tasks include segmentation, classification, and anomaly detection. Libraries include MONAI, PyTorch, and TensorFlow.\n\n\nModel exploration\nN/A\nLibraries that facilitate trying out different model architectures and pretrained models.\n\n\nNLP\nNatural Language Processing\nA field of machine learning focused on the interaction between computers and humans using natural language. Common libraries include NLTK, SpaCy, and Hugging Face Transformers.\n\n\nOOD detection\nOut-of-Distribution Detection\nTechniques for identifying data points that do not belong to the distribution on which a model was trained. Important for building robust and trustworthy models. Common methods include Mahalanobis distance and energy-based models.\n\n\nPyTorch\nN/A\nAn open-source machine learning library based on the Torch library, primarily used for applications such as computer vision and natural language processing. Developed by Facebook’s AI Research lab.\n\n\nPython\nN/A\nA high-level programming language that has become the de facto standard for machine learning and data science due to its readability and vast ecosystem of libraries (e.g., NumPy, Pandas, Scikit-learn, TensorFlow).\n\n\nReproducibility\nN/A\nEnsuring that ML experiments and results can be consistently replicated by others, a key principle in scientific research. Often involves detailed documentation, version control, and the use of containers.\n\n\nSklearn\nScikit-learn\nA machine learning library for Python that provides simple and efficient tools for data mining and data analysis, built on NumPy, SciPy, and Matplotlib.\n\n\nText analysis\nN/A\nThe process of deriving information from text data, often involving techniques from NLP. Used in sentiment analysis, topic modeling, and more. Common libraries include NLTK, SpaCy, and Gensim.\n\n\nTrustworthy ML\nN/A\nApproaches and practices in machine learning that ensure models are reliable, fair, and transparent, especially in critical applications like healthcare or finance.\n\n\nUdacity\nN/A\nAn online learning platform offering courses, including nanodegree programs, on a variety of topics including machine learning and data science. Often includes projects and code-alongs.\n\n\nViT\nVision Transformer\nViT is based on the transformer architecture, originally designed for NLP tasks but adapted for vision tasks. Images are split into patches (usually 16x16), which are flattened and treated as input tokens. These patches are then processed similarly to how words are processed in NLP transformers like BERT.\n\n\n\nVLM | Visual-Language Model | A type of machine learning model that understands and generates content based on both visual and textual inputs, often used in tasks like image captioning and visual question answering. Examples include CLIP and LLaVA. |",
    "crumbs": [
      "Category glossary"
    ]
  },
  {
    "objectID": "includes/common-resources-text.html",
    "href": "includes/common-resources-text.html",
    "title": "Nexus: Crowdsourced ML Resources",
    "section": "",
    "text": "Any content (original or external) that can help make the practice of ML/AI more connected, accessible, efficient, and reproducible is welcome on the Nexus platform! This includes, but is not limited to…\n\n🧠 Educational materials: Explore a library of educational materials (workshops, books, videos, etc.) covering a wide range of ML-related topics, tools, and workflows, from foundational concepts to advanced techniques.\n🛠 Models, code, and more: Learn about popular models, tools, and datasets that you can leverage for your next ML project.\n🧬 Applications & stories: Discover a curated collection of blogs, papers, and talks which dive into real-world ML applications and lessons learned by practitioners.\n\nDisclaimer: The crowdsourced resources on this website are not endorsed by the UW-Madison and have not been vetted by the Division of Information Technology."
  },
  {
    "objectID": "includes/netID-disclaimer.html",
    "href": "includes/netID-disclaimer.html",
    "title": "Nexus: Crowdsourced ML Resources",
    "section": "",
    "text": "Note: A netID is required to view the ML4MI and the Exploring AI @ UW video series."
  },
  {
    "objectID": "Applications/Blogs/blog-music-identification.html",
    "href": "Applications/Blogs/blog-music-identification.html",
    "title": "What Tune Is That? A Humanities Application of Deep Learning",
    "section": "",
    "text": "Deep learning (neural network training) can solve humanities challenges, too! Read about a successful project that trained a model to be able to identify Irish traditional dance tunes. Project GitHub: github.com/alanngnet/CoverHunterMPS"
  },
  {
    "objectID": "Applications/Blogs/blog-music-identification.html#the-challenge",
    "href": "Applications/Blogs/blog-music-identification.html#the-challenge",
    "title": "What Tune Is That? A Humanities Application of Deep Learning",
    "section": "The Challenge",
    "text": "The Challenge\nThe mystery I have been working on since about 1993 is how to systematically describe the Irish traditional dance music repertoire. This particular musical culture might be the healthiest European folk music tradition that has survived unbroken for centuries as an aurally transmitted culture, with something on the order of 10,000 musically distinct “tunes” (as musical works for dance use are called in this culture) and tens of thousands of active participants around the world. My main work is published at irishtune.info as a combination scholarly reference work and practical day-to-day tool for the global community of musicians at all levels.\nOne side benefit of the manual work I’ve been doing for 30 years to carefully describe the contents of about a thousand albums of commercially published Irish traditional music is that I (only somewhat intentionally) created an ideal dataset for training a machine-learning solution that can do what only very few human experts can do after a lifetime of experience and use of large archival resources: Hear any performance and identify what tune it is. This is the core challenge I have tackled here.\nBackground explanation: Folk musicians in aurally transmitted traditions generally do not know the “identity” of tunes they play, especially not in any kind of broadly reliable or generally agreed-upon way other than as often-contradictory informal assertions, each held within a subset of the global community. Welcome to the fuzziness of humanities research! :)\nSome benefits of solving this challenge:\n\nAccelerate my work as a human expert as I expand the coverage of albums represented in irishtune.info, which in turn benefits the global community of musicians and the health of the tradition itself. This goal was accomplished as of July 27, 2024, but refinement continues.\nEmpower the global community of Irish traditional musicians to identify their own tunes and recordings. (I am currently looking for help with this technical challenge.)\nNow that I have solved this for Irish traditional music, how do we enable musicologists and musicians interested in all the other folk musics on planet Earth to create the same kind of solution for those musical cultures?"
  },
  {
    "objectID": "Applications/Blogs/blog-music-identification.html#the-solution",
    "href": "Applications/Blogs/blog-music-identification.html#the-solution",
    "title": "What Tune Is That? A Humanities Application of Deep Learning",
    "section": "The Solution",
    "text": "The Solution\nFor better or worse, commercial music is a big-money industry. That means there are both well-funded organizations involved as well as financial incentives to analyze and automate all sorts of business processes related to pop music. For example, social media platforms have to worry about copyright and licensing issues in any kind of social-media post that includes music in any form. Youtube or Tiktok, for example, need to make sure that your video of yourself singing a Taylor Swift song in the car doesn’t get shared without the proper licensing fees being paid to the artist. And in order to do that, they needed to solve the problem of “What song is that?” in an automated way. So they have been funding CSI (Cover Song Identification) research for years, and that has occasionally shown up in public as open-source code repositories, like this ground-breaking one that appeared in July 2023 from an industry-funded team of researchers in China: github.com/Liu-Feng-deeplearning/CoverHunter\nWhile humanities research would never get the resources to build a solution from scratch to, say, figure out a way to catalog 10,000 hours of audio in an Armenian folk song archive, we can certainly take advantage of big-industry solutions that solve similar problems.\nAnd that’s exactly what I did. I forked that CoverHunter project and the result is published as github.com/alanngnet/CoverHunterMPS.\nIt took about 6 months of:\n\nReverse-engineering the very poorly documented CoverHunter code to understand it enough to proceed.\nTeaching myself just enough about machine learning, neural network training, and just enough new Python skills to proceed.\nFixing bugs and documenting both the Python code itself as well as how to use it. My correspondence with the lead author of CoverHunter confirmed that their industry sponsor required them to remove proprietary aspects of the code, which presumably broke things in the process. Plus they wrote their solution to run on big industry-scale server farms which humanities researchers typically have no way to fund, so I had to revise it to run on a desktop.\nData wrangling - mainly in the form of writing Python scripts - to automate the large-scale tasks of leveraging my own database and audio library to prepare training data in the format needed by the Python application.\nHyperparameter tuning (and adding more hyperparameters). So many long training runs, so many TensorBoard graphs to pore over!\nAdding features to take the CoverHunter solution, which was only built out enough to generate the training metrics needed to claim success as a CSI research breakthough, and turn it into a reliable, easy-to-use solution that answers “What tune is this?” for any arbitrary audio input.\n\nIn case you are interested in a more technical summary of the solution, it involves:\n\nConversion of raw audio data to CQT (Constant-Q Transform) 2-D arrays, representing time and frequency dimensions that can also be treated as a visual picture of the audio, so that visual machine-learning methods and models could also be leveraged. For example, a single-instrument melody appears as a line moving vertically higher for higher notes and lower for lower notes, and longer horizontally when notes are sustained for a longer time. CQT is a well-established method of audio analysis in the larger academic research field of MIR (music information retrieval).\nA lot of artificial data augmentation done both in pre-training preprocessing as well as on-the-fly augmentation done during training. For each real-world audio sample you give to this solution, it will generate 4 other variants in pre-processing, and then during training each of those will in turn get augmented (modified) in different, partly random ways during each training step. So by the end of a full training run, the model may have seen hundreds of artificial variants of each real-world data sample.\nA PyTorch-based implementation of a conformer neural network, a somewhat unusual or newer complex network that combines a CNN (convolutional neural network) for small-timescale learning of specific patterns with a transformer network that enables large-timescale and structurally flexible learning. You need the latter because real-world musicians (unlike, say, music played at the school level) are free to modify the structure of a work without hurting the musical identity of the work, and are free to improvise and vary within the structure, likewise without causing a human listener to fail to recognize the identity of the work.\nLeveraging a “bag of tricks” of various neural-network training optimization techniques that the CoverHunter authors adapted from previous deep-learning research, including taking a lot of source code from various - often unidentified - open-source projects in the field of automated speech recognition. This amalgamation of code from many undocumented sources was a big part of why my first task of even understanding their code was so challenging (I did fix a lot of that along the way).\nOutput of a 128-dimensional tensor (a 1x128-element array of numbers) that uniquely describes an audio performance. The tune-identification solution basically does a relatively simple mathematical comparison of the model-generated tensor for your mystery audio file against the model-generated tensors for all the known, identified audio files in your database, and finds the nearest neighbors for you."
  },
  {
    "objectID": "Applications/Blogs/blog-music-identification.html#how-good-is-it",
    "href": "Applications/Blogs/blog-music-identification.html#how-good-is-it",
    "title": "What Tune Is That? A Humanities Application of Deep Learning",
    "section": "How Good Is It?",
    "text": "How Good Is It?\nHere is a graph of mAP (mean average precision) during a late-August 2024 full training run using the K-fold cross-validation production-training tool in this project. The horizontal axis is a time dimension measured in training epochs. Each different colored line is one of the 5 folds, plus a 6th full-dataset training run.\n94% accuracy at identifying “easy reels” is very, very good! So is 74% accuracy for “hard reels,” which are performances I selected as probably going to be challenging for any ML solution to be able to recognize. After using it in the real world in the last month to identify tunes on new audio recordings, I am confident that this solution performs better than any human expert will ever be able to, especially given that it spits out its best guess in a couple seconds! In this specialized niche, it is already “superhuman AI.”\n\n\n\nGraph of training results for a production training run, measured as mAP (mean average precision) against two test datasets.\n\n\nDetails about the two test datasets, “reels50easy” and “reels50hard,” are published at www.irishtune.info/public/MLdata.htm"
  },
  {
    "objectID": "Applications/Blogs/blog-music-identification.html#how-did-i-learn-how-to-do-this",
    "href": "Applications/Blogs/blog-music-identification.html#how-did-i-learn-how-to-do-this",
    "title": "What Tune Is That? A Humanities Application of Deep Learning",
    "section": "How Did I Learn How to Do This?",
    "text": "How Did I Learn How to Do This?\nBefore this project, I had zero machine-learning experience, only intermediate-level Python experience, and a 30-year gap of disuse since my undergraduate coursework in college-level math.\n\nHuman Helpers\nMy first step in later 2023 - see the next section for the backstory before then - was to reach out to the global community of users of irishtune.info, as well as to the ML+X community here in Madison, to seek collaborators and advisors. That effort netted me several useful connections.\nJosh Liu, an Irish musician in the New England area and a data-science student, was the first to offer help, and I asked him to point me to where the state-of-the-art is at the moment in the field of CSI (Cover Song Identification), especially if any open-source code might be out there. He is the person who found the CoverHunter research paper for me, and shared that although his attempt to use their published code and pre-trained (pop-music trained) model failed for Irish music, he saw it had fundamental promise, especially knowing that I had a uniquely large and high-quality dataset to train it with.\nA couple other Irish musicians who work in the data-science/“AI” industry responded with interest, and ended up giving me some useful advice and encouragement in single video calls early in my struggles with the CoverHunter project. Knowing that an AI professional thinks your own newbie’s pie-in-the-sky idea is worth pursuing is a powerful motivator!\nProbably the most useful help I got was from Samuel Gauthier, an Irish musician in France who is a professional Python developer, even though he had no experience in machine learning. He got hands-on with me directly in my GitHub project, especially cleaning up and improving a lot of the inscrutable and inefficient code that the CoverHunter authors had left as-is during their creative but messy work of combining good ideas from other open-source machine learning projects.\n\n\nAI Chat Assistants\nAI chat-based assistants were absolutely crucial in my ability to do this project, much less in a mere 6 months as a side hobby. I needed them for everything from explaining cryptic bits of Python code to me at just the right level for my background, to teaching me exactly the right bits of data science and machine learning literacy that I needed for this project, to writing first-through-twentieth drafts of a lot of the Python code I needed to add.\nEven before I reached out for human help in late 2023, what sparked the entire project was when I tried out the then-new ChatGPT in early 2023, by asking it the very high-level question of how one might approach my decades-old dream of using machine learning with my database to solve the “What is this tune?” problem. The specificity and doability of its suggestions, and its ability to write the necessary code for me, was mind-blowing! Something that had previously seemed impossible - at least without me somehow earning a degree in data science or finding a wealthy donor to fund a team - looked suddenly within reach of my hobbyist time and skills. ChatGPT got me as far as building a simple CNN model from scratch to at least solve the problem of “is this tune a reel, jig, polka, hornpipe, (etc.). …?” which eventually fizzled out with unsatisfactory training results. But ChatGPT got the whole thing kicked off, so that I was now literate enough and confident enough to seek human helpers … see above for that story.\nDuring early-mid 2024, I did frequent comparisons between getting help from ChatGPT, Google Gemini, and Claude. I found early on that Gemini was helpful at explaining concepts and code, but was a waste of my time for trickier code-writing assistance. ChatGPT and Claude were kind of neck-and-neck with writing code, but as 2024 progressed, Claude began winning my anecdotal “contests.” In general both were good enough for it to clearly be worth my time to use them. With any AI assistant, it generally takes a longer conversation of back-and-forth, with me reading its code, testing it on my computer, and providing feedback to the assistant, to eventually reach my desired solution. And occasionally - really a minority of the time -, they just wasted my time, particularly when I ended up going in circles, with the assistant coming back around to fix the latest problem with something that removed or broke the fixes done earlier in the conversation! I suppose I at least got some good, guided practice at thinking through Python puzzles along the way …\nEspecially powerful was my use of the “Project” feature of Claude Pro that came out earlier this summer, in which I could give it my whole project full of Python code and have it write high-quality Python code that is compatible with and efficiently leverages the rest of the project, without my having to explain the background context. For example, the embedding-generation script (tools/make_embeds.py) was written by Claude Pro from scratch as a single response to a single prompt that worked on the first try. (I did eventually modify it further both by hand and with further Claude assistance.)\n\n\nGoogle CoLab\nGoogle CoLab was uniquely helpful to me very early on when I first attempted to get the CoverHunter code working. Having an industry-standard, pre-configured set of hardware and software available in CoLab - for free! - helped me figure out what problems might be unique to my own computer and software vs. what were basic problems in the CoverHunter code. And I hope it might continue to be useful to others attempting the same learning curve, since any interested passer-by curious to try out my project can do so without having to set up or even have access to any particular hardware and software of their own."
  },
  {
    "objectID": "Applications/Blogs/blog-music-identification.html#value-for-other-purposes",
    "href": "Applications/Blogs/blog-music-identification.html#value-for-other-purposes",
    "title": "What Tune Is That? A Humanities Application of Deep Learning",
    "section": "Value for Other Purposes",
    "text": "Value for Other Purposes\n\nI’m hopeful that my project will turn out to help solve other kinds of musicological mysteries than just tune identification. See github.com/alanngnet/CoverHunterMPS#coverhuntermps for some ideas I have.\nIf you’re looking for a high-level summary of the steps involved in preparing, training, evaluating, and productionizing any deep learning solution, consider the “Usage” section of the project Readme at github.com/alanngnet/CoverHunterMPS/blob/main/README.md\nSteal, borrow, and tweak the source code for your own purposes. There are a lot of generally useful bits in this project for any deep-learning classification solution. But even better would be to contribute improvements to this project to share your innovations more broadly.\nUse the Irish-specific CSI benchmarking datasets downloadable from https://www.irishtune.info/public/MLdata.htm to test how well your own CSI model can handle Irish traditional music.\nFeel free to contact me (alan@alan-ng.net) to tell this story if it can help expand the use of data science in the humanities."
  },
  {
    "objectID": "Applications/Blogs/blog-music-identification.html#questions",
    "href": "Applications/Blogs/blog-music-identification.html#questions",
    "title": "What Tune Is That? A Humanities Application of Deep Learning",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Applications/Blogs/blog-music-identification.html#see-also",
    "href": "Applications/Blogs/blog-music-identification.html#see-also",
    "title": "What Tune Is That? A Humanities Application of Deep Learning",
    "section": "See also",
    "text": "See also\n\nWorkshop: Intro to Deep Learning with PyTorch: Learn how to use the PyTorch deep learning framework"
  },
  {
    "objectID": "Applications/Videos/Forums/mlx_2023-09-19.html",
    "href": "Applications/Videos/Forums/mlx_2023-09-19.html",
    "title": "Multimodal Learning",
    "section": "",
    "text": "Multimodal learning and analysis for understanding single-cell functional genomics in brains and brain diseases — Daifeng Wang, PhD\nTransforming healthcare: AI-enhanced disease quantification with vision-language models — Zachary Huemann\nThe benefits of early fusion: deeply integrated audio-visual representation learning — Pedro Morgado, PhD"
  },
  {
    "objectID": "Applications/Videos/Forums/mlx_2024-03-12.html",
    "href": "Applications/Videos/Forums/mlx_2024-03-12.html",
    "title": "Exploring Model Sharing in the Age of Foundation Models",
    "section": "",
    "text": "Model sharing (via platforms such as Hugging Face) has become more commonplace over the past few years as practitioners increasingly rely on pretrained models and foundation models to find patterns in their data. In this month’s forum, we discuss best practices surrounding model sharing and learn about the recently released LLaVA-NeXT model — a large multimodal model (LMM) which can be used for vision+text related tasks.\n\nModel sharing and reproducible ML - Chris Endemann\nDuring this facilitated discussion, we will share ideas and experiences surrounding best practices in reproducible ML. We will explore questions such as:\n\nWhy should you share your ML model?\nWhat are potential challenges, risks, or ethical concerns associated with model sharing and reproducing ML workflows?\nHow do you market or share your models in a way that ensures proper use?\nWhat pieces must be well-documented to ensure reproducible and responsible model sharing?\n\n\nLLaVA-NeXT and model sharing - Haotian Liu\nWe’ve been open sourcing the LLaVA-series multimodal models in the past year, and our LLaVA-series of work has inspired many creative researches and explorations based on our open-source releases. We’ll give a brief introduction of our LLaVA model and how we’ve been releasing model weights and maintaining our open-source code bases. We’ll also discuss some challenges we face on open-source release, including hosting cost, privacy, commercial license, etc."
  },
  {
    "objectID": "Applications/Videos/Forums/mlx_2025-02-10.html",
    "href": "Applications/Videos/Forums/mlx_2025-02-10.html",
    "title": "Learning Through Comparison: Use Cases of Contrastive Learning",
    "section": "",
    "text": "Contrastive learning is reshaping how models learn, driving widespread progress in feature learning, clustering, out-of-distribution detection, and multimodal applications. Instead of relying on rigid classifications for model training, contrastive learning helps models recognize nuanced patterns and relationships in data, making them more adaptable and better at handling real-world data. In this forum, we explore how contrastive learning works, why it’s effective, and how you can apply it in your own projects.\n\nOverview of Contrastive Learning — Yin Li, PhD 01:34\nContrastive Learning Use Cases in Clustering and Out-Of-Distribution Detection — Chris Endemann 47:52\n\nSlides: View the slides to grab the links referenced throughout this presentation.\nSupplmental Colab notebooks: Coming soon!"
  },
  {
    "objectID": "Applications/Videos/Forums/mlx_2025-02-10.html#about-this-resoure",
    "href": "Applications/Videos/Forums/mlx_2025-02-10.html#about-this-resoure",
    "title": "Learning Through Comparison: Use Cases of Contrastive Learning",
    "section": "",
    "text": "Contrastive learning is reshaping how models learn, driving widespread progress in feature learning, clustering, out-of-distribution detection, and multimodal applications. Instead of relying on rigid classifications for model training, contrastive learning helps models recognize nuanced patterns and relationships in data, making them more adaptable and better at handling real-world data. In this forum, we explore how contrastive learning works, why it’s effective, and how you can apply it in your own projects.\n\nOverview of Contrastive Learning — Yin Li, PhD 01:34\nContrastive Learning Use Cases in Clustering and Out-Of-Distribution Detection — Chris Endemann 47:52\n\nSlides: View the slides to grab the links referenced throughout this presentation.\nSupplmental Colab notebooks: Coming soon!"
  },
  {
    "objectID": "Applications/Videos/Forums/mlx_2025-02-10.html#see-also",
    "href": "Applications/Videos/Forums/mlx_2025-02-10.html#see-also",
    "title": "Learning Through Comparison: Use Cases of Contrastive Learning",
    "section": "See also",
    "text": "See also\n\n\nData: CIFAR: Learn more about the CIFAR-10/CIFAR-100 datasets."
  },
  {
    "objectID": "Applications/Videos/Forums/mlx_2023-10-10.html",
    "href": "Applications/Videos/Forums/mlx_2023-10-10.html",
    "title": "Time-Series Analysis",
    "section": "",
    "text": "Computational Methods for Comparative Time Clocks in Early Development and Tissue Regeneration — Peng Jiang, PhD\nControlled Differential Equations on Long Sequences via Non-standard Wavelets — Sourav Pal"
  },
  {
    "objectID": "Applications/Videos/Forums/mlx_2025-04-01.html",
    "href": "Applications/Videos/Forums/mlx_2025-04-01.html",
    "title": "Harnessing the Power of Foundation Models for Healthcare and Life Sciences",
    "section": "",
    "text": "Jameson Merkow, AI engineer at Microsoft, presents an overview of recent advances in applying foundation models to healthcare and life sciences. This forum explores the use of multimodal embedding models, large language models (LLMs), and Azure-integrated tools to support clinical workflows and biomedical research.\n\n\nJameson introduces a suite of tools available through Microsoft’s AI Foundry and Azure ML:\n\nMedImage Insight (MI2): A powerful multimodal embedding model trained across nine medical imaging modalities.\nMedImage Parse: A segmentation model that responds to natural language prompts across modalities.\nCXR ReportGen: An image-to-text model that outputs structured radiology findings and highlights supporting regions in images.\n\nAll tools are open source and deployable within a secure, tenant-controlled Azure environment.\n\n\n\nJameson covers a range of use cases, including:\n\nImage classification using pretrained embeddings or lightweight adapters.\nOutlier detection for clinical trial QC and data curation.\nSurvival prediction from paired MRI and histopathology data using multimodal fusion.\nImage search across 2D, 3D, and cross-modality benchmarks.\nIntegration with multi-agent systems for orchestrating model calls and generating clinical reports.\nDrift monitoring using multimodal embeddings and patient metadata.\n\n\n\n\nRather than retraining LLMs for each medical task, Microsoft’s approach combines task-specific vision/language models with general-purpose reasoning agents. This modular strategy enables faster deployment, clearer interpretability, and easier fine-tuning across domains."
  },
  {
    "objectID": "Applications/Videos/Forums/mlx_2025-04-01.html#about-this-resource",
    "href": "Applications/Videos/Forums/mlx_2025-04-01.html#about-this-resource",
    "title": "Harnessing the Power of Foundation Models for Healthcare and Life Sciences",
    "section": "",
    "text": "Jameson Merkow, AI engineer at Microsoft, presents an overview of recent advances in applying foundation models to healthcare and life sciences. This forum explores the use of multimodal embedding models, large language models (LLMs), and Azure-integrated tools to support clinical workflows and biomedical research.\n\n\nJameson introduces a suite of tools available through Microsoft’s AI Foundry and Azure ML:\n\nMedImage Insight (MI2): A powerful multimodal embedding model trained across nine medical imaging modalities.\nMedImage Parse: A segmentation model that responds to natural language prompts across modalities.\nCXR ReportGen: An image-to-text model that outputs structured radiology findings and highlights supporting regions in images.\n\nAll tools are open source and deployable within a secure, tenant-controlled Azure environment.\n\n\n\nJameson covers a range of use cases, including:\n\nImage classification using pretrained embeddings or lightweight adapters.\nOutlier detection for clinical trial QC and data curation.\nSurvival prediction from paired MRI and histopathology data using multimodal fusion.\nImage search across 2D, 3D, and cross-modality benchmarks.\nIntegration with multi-agent systems for orchestrating model calls and generating clinical reports.\nDrift monitoring using multimodal embeddings and patient metadata.\n\n\n\n\nRather than retraining LLMs for each medical task, Microsoft’s approach combines task-specific vision/language models with general-purpose reasoning agents. This modular strategy enables faster deployment, clearer interpretability, and easier fine-tuning across domains."
  },
  {
    "objectID": "Applications/Videos/Forums/mlx_2025-04-01.html#see-also",
    "href": "Applications/Videos/Forums/mlx_2025-04-01.html#see-also",
    "title": "Harnessing the Power of Foundation Models for Healthcare and Life Sciences",
    "section": "See also",
    "text": "See also\n\nMedImage Insight (MI2) Blog: Read more about Microsoft’s open medical embedding model.\nChestX-ray14 Dataset: A benchmark for image-to-text and classification tasks.\nTalk: Automating Scientific Discovery: Another ML+X forum presentation on multimodal learning in biology and literature."
  },
  {
    "objectID": "Applications/Videos/index.html",
    "href": "Applications/Videos/index.html",
    "title": "Videos",
    "section": "",
    "text": "Discover a curated collection of talks which dive into AI/ML applications and lessons learned by practitioners.\nNote: A netID is required to view the ML4MI and the Exploring AI @ UW video series.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative AI From Multiple Perspectives\n\n\n\nVideos\n\nML+X\n\nLLM\n\nHealthcare\n\nEHR\n\nExplainability\n\nDeep learning\n\nEducation\n\nRetrieval\n\nRAG\n\nNLP\n\n\n\n\n\n\n\n\n\n2025-05-06\n\n\nBenjamin Lengerich, Kaiser Pister\n\n\n\n\n\n\n\n\n\n\n\n\nHarnessing the Power of Foundation Models for Healthcare and Life Sciences\n\n\n\nVideos\n\nML+X\n\nHealthcare\n\nMedical imaging\n\nFoundation models\n\nMultimodal learning\n\nDeep learning\n\nEmbeddings\n\nImage data\n\nComputer vision\n\nRetrieval\n\nZero-shot learning\n\nLLM\n\nMicrosoft Copilot\n\nAzure\n\n\n\n\n\n\n\n\n\n2025-04-01\n\n\nJameson Merkow\n\n\n\n\n\n\n\n\n\n\n\n\nAutomating Scientific Discovery: From Natural World Data to Systematic Literature Reviews\n\n\n\nVideos\n\nML+X\n\nMultimodal learning\n\nBenchmarking\n\nRetrieval\n\nComputer vision\n\nZero-shot learning\n\nContrastive learning\n\nDeep learning\n\nCLIP\n\nBiology\n\nEcology\n\nImage data\n\nLLM\n\nHugging Face\n\n\n\n\n\n\n\n\n\n2025-03-04\n\n\nEdward Vendrow\n\n\n\n\n\n\n\n\n\n\n\n\nLearning Through Comparison: Use Cases of Contrastive Learning\n\n\n\nVideos\n\nML+X\n\nUW-Madison\n\nDeep learning\n\nContrastive learning\n\nClustering\n\nOOD detection\n\nMultimodal learning\n\nTrustworthy AI\n\nRepresentation learning\n\nCIFAR\n\n\n\n\n\n\n\n\n\n2025-02-10\n\n\nYin Li, Chris Endemann\n\n\n\n\n\n\n\n\n\n\n\n\nAlphaFold and Protein Language Models\n\n\n\nVideos\n\nComBEE\n\nUW-Madison\n\nBiology\n\nProtein engineering\n\nLLM\n\nDeep learning\n\nAlphafold\n\nTransformer\n\nProtein language models\n\n\n\n\n\n\n\n\n\n2025-01-28\n\n\nHannah Wayment-Steele\n\n\n\n\n\n\n\n\n\n\n\n\nAI for Music and the Humanities\n\n\n\nVideos\n\nML+X\n\nUW-Madison\n\nDeep learning\n\nGenAI\n\nConformer\n\nHumanities\n\nAudio search\n\nMusic\n\nCSI\n\nSignal processing\n\nSpectral analysis\n\nLLM\n\n\n\n\nWhat Tune Is That: A Humanities Application of Deep Learning — Alan Ng \nFake Artists, Fake Listeners: AI and the Music Industries — Jeremy Morris, PhD\n\n\n\n\n\n\n2024-11-05\n\n\nAlan Ng, Jeremy Morris\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Electronic Health Record Data to Predict Deterioriation in Hospitalized Children\n\n\n\nVideos\n\nML4MI\n\nUW-Madison\n\nHealthcare\n\nNLP\n\nText analysis\n\nEHR\n\nBoosting\n\nDecision trees\n\n\n\n\n\n\n\n\n\n2024-10-14\n\n\nAnoop Mayampurath\n\n\n\n\n\n\n\n\n\n\n\n\nVision, Language, and Vision-Language Modeling in Radiology\n\n\n\nVideos\n\nML4MI\n\nUW-Madison\n\nMedical imaging\n\nHealthcare\n\nVLM\n\nViT\n\nUNET\n\nLLaVA\n\nComputer vision\n\nCNN\n\nLLM\n\nDeep learning\n\nMultimodal learning\n\n\n\n\n\n\n\n\n\n2024-09-16\n\n\nTyler Bradshaw\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Out-of-Distribution Detection\n\n\n\nVideos\n\nICCV\n\nOOD detection\n\nTrustworthy AI\n\n\n\n\n\n\n\n\n\n2024-07-11\n\n\nSharon Li\n\n\n\n\n\n\n\n\n\n\n\n\nA Biophysics-based Protein Language Model for Protein Engineering\n\n\n\nVideos\n\nCross Labs AI\n\nUW-Madison\n\nTransfer learning\n\nBiology\n\nBiophysics\n\nProtein language models\n\nFoundation models\n\nLLM\n\nDeep learning\n\nProtein engineering\n\nSimulations\n\n\n\nWe introduce Mutational Effect Transfer Learning (METL), a specialized protein language model that bridges the gap between traditional biophysics-based and machine learning approaches by incorporating synthetic data from molecular simulations.\n\n\n\n\n\n2024-06-18\n\n\nSam Gelman\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Large Language Models for Meteorological Fact Finding\n\n\n\nVideos\n\nIT Prof\n\nUW-Madison\n\nLLM\n\nMeteorology\n\n\n\nThis talk demonstrates harnessing the power of AI to open new avenues in data analysis, including for meteorological fact-finding. Discover how cutting-edge large language models (LLMs) like OpenAI’s ChatGPT 3.5 and 4.0 hold are poised to help the field of meteorological data analysis.\n\n\n\n\n\n2024-05-30\n\n\nZekai Otles\n\n\n\n\n\n\n\n\n\n\n\n\nTrustworthy LLMs & Ethical AI\n\n\n\nVideos\n\nML+X\n\nUW-Madison\n\nNLP\n\nText analysis\n\nLLM\n\nDeep learning\n\nTrustworthy AI\n\nDeTox\n\nEthical AI\n\nTopic modeling\n\n\n\n\nDeTox: Denoised Toxic Embeddings for Editing Model Toxicity — Rheeya Uppaal\nA Project on AI Ethics — Mariab A. Knowles\n\n\n\n\n\n\n2024-05-11\n\n\nRheeya Uppaal, Mariah A. Knowles\n\n\n\n\n\n\n\n\n\n\n\n\nAdvancing Healthcare and Agriculture through Computer Vision\n\n\n\nVideos\n\nML+X\n\nUW-Madison\n\nComputer vision\n\nUltrasound\n\nMedical imaging\n\nAthletics\n\nAgriculture\n\nLSTM\n\nCNN-LSTM\n\nCNN\n\nDeep learning\n\n\n\n\nAn ultrasound-based method to measure knee kinematics enabled by deep learning — Matthew Blomquist\nPlant breeding in the age of computer vision — Will de la Bretonne\n\n\n\n\n\n\n2024-04-09\n\n\nMatthew Blomquist, Will de la Bretonne\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Model Sharing in the Age of Foundation Models\n\n\n\nVideos\n\nML+X\n\nUW-Madison\n\nMultimodal learning\n\nFoundation models\n\nModel sharing\n\nHugging Face\n\nLLM\n\nLMM\n\nLLaVA\n\nDeep learning\n\n\n\n\nModel sharing and reproducible ML — Chris Endemann\nLLaVA-NeXT and model sharing — Haotian Liu, PhD\n\n\n\n\n\n\n2024-03-12\n\n\nChris Endemann, Haotian Liu\n\n\n\n\n\n\n\n\n\n\n\n\nNavigating Gravitational Waves with AI Insights\n\n\n\nVideos\n\nML+X\n\nUW-Madison\n\nPhysics\n\nSimulations\n\nSpectral analysis\n\nSignal processing\n\n\n\n\nWelcome and small group discussions — Chris Endemann\nClassifying gravitational wave modes from core-collapse supernovae — Bella Finkel\n\n\n\n\n\n\n2024-02-13\n\n\nChris Endemann, Bella Finkel\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Science Communication and Drug Synergy Analysis using GPT\n\n\n\nVideos\n\nML+X\n\nUW-Madison\n\nScience communication\n\nHealthcare\n\nDrug synergy\n\nLLM\n\nText mining\n\nNLP\n\n\n\n\n\n\n\n\n\n2023-12-12\n\n\nBen Rush, Jack Freeman\n\n\n\n\n\n\n\n\n\n\n\n\nWorld Knowledge in the Time of Large Models\n\n\n\nVideos\n\nSILO\n\nUW-Madison\n\nVLM\n\nLLM\n\nLMM\n\nMultimodal learning\n\nFoundation models\n\n\n\nThis talk will discuss the massive shift that has come about in the vision and ML community as a result of the large pre-trained language and language and vision models such as Flamingo, GPT-4, and other models.\n\n\n\n\n\n2023-11-22\n\n\nKenneth Marino\n\n\n\n\n\n\n\n\n\n\n\n\nLLMS in Genomics and Health Coaching\n\n\n\nVideos\n\nML+X\n\nUW-Madison\n\nHealthcare\n\nClustering\n\nDeep learning\n\nLLM\n\nGenomics\n\n\n\n\nClustering of genomic sequences of mycoviruses using deep learning — Rohan Sontahlia\nSpurring self-improvement and intrinsic motivation using LLMs and reinforcement learning — Michael Roytman\n\n\n\n\n\n\n2023-11-07\n\n\nRohan Sontahlia, Michael Roytman\n\n\n\n\n\n\n\n\n\n\n\n\nLabelBench: A Framework for Benchmarking Label-Efficient Learning\n\n\n\nVideos\n\nMLOPT\n\nUW-Madison\n\nActive learning\n\nLabel-efficient learning\n\nActive learning\n\nSemi-supervised\n\nViT\n\nBenchmarking\n\n\n\n\n\n\n\n\n\n2023-10-27\n\n\nJifan Zhang\n\n\n\n\n\n\n\n\n\n\n\n\nTime-Series Analysis\n\n\n\nVideos\n\nML+X\n\nUW-Madison\n\nTime-series\n\nGenomics\n\nBiology\n\nHealthcare\n\n\n\n\n\n\n\n\n\n2023-10-10\n\n\nPeng Jiang, Sourav Pal\n\n\n\n\n\n\n\n\n\n\n\n\nBiomedical AI Research and Applications\n\n\n\nVideos\n\nUW-Madison\n\nExploring AI@UW\n\nBiology\n\nHealthcare\n\nLLM\n\nComputer vision\n\nMultimodal learning\n\n\n\n\n\n\n\n\n\n2023-09-22\n\n\nAnthony Gitter, Junjie Hu, Yin Li, Anoop Mayampurath, Vikas Singh\n\n\n\n\n\n\n\n\n\n\n\n\nRacism in AI\n\n\n\nVideos\n\nUW-Madison\n\nExploring AI@UW\n\nEthical AI\n\nBias\n\nFairness\n\n\n\n\n\n\n\n\n\n2023-09-22\n\n\nLori Kido Lopez\n\n\n\n\n\n\n\n\n\n\n\n\nMultimodal Learning\n\n\n\nVideos\n\nML+X\n\nUW-Madison\n\nMultimodal learning\n\nDeep learning\n\nComputer vision\n\nHealthcare\n\nGenomics\n\n\n\n\n\n\n\n\n\n2023-09-19\n\n\nDaifeng Wang, Zachary Huemann, Pedro Morgado\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Generative AI: An Introduction to Large Language Models and Diffusion Models\n\n\n\nVideos\n\nML4MI\n\nUW-Madison\n\nLLM\n\nVLM\n\nLLaVA\n\nLLM\n\nGenAI\n\nDiffusion\n\nDeep learning\n\nMultimodal learning\n\n\n\n\n\n\n\n\n\n2023-09-11\n\n\nKangwook Lee\n\n\n\n\n\n\n\n\n\n\n\n\nAI and Medical Imaging\n\n\n\nVideos\n\nExploring AI@UW\n\nUW-Madison\n\nHealthcare\n\nMedical imaging\n\nImage classification\n\nComputer vision\n\nDeep learning\n\n\n\n\n\n\n\n\n\n2023-08-11\n\n\nLaurel Belman, Todd Shechter, Alan McMillan\n\n\n\n\n\n\n\n\n\n\n\n\nAI in Action: Intelligent Systems and Business Operations\n\n\n\nVideos\n\nUW-Madison\n\nExploring AI@UW\n\nBusiness\n\nBias\n\nEthical AI\n\n\n\n\n\n\n\n\n\n2023-07-28\n\n\nLaura Albert\n\n\n\n\n\n\n\n\n\n\n\n\nUnifying Audio-Visual Machine Perception – Tasks & Architectures\n\n\n\nVideos\n\nML4MI\n\nUW-Madison\n\nHealthcare\n\nMultimodal learning\n\nContrastive learning\n\nPerception\n\nEarly-fusion\n\nAutoencoder\n\n\n\n\n\n\n\n\n\n2023-07-12\n\n\nPedro Morgado\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Applications",
      "Videos"
    ]
  },
  {
    "objectID": "Applications/Videos/Other/25-01-28_AlphaFold_ComBEE.html",
    "href": "Applications/Videos/Other/25-01-28_AlphaFold_ComBEE.html",
    "title": "AlphaFold and Protein Language Models",
    "section": "",
    "text": "In a talk hosted by the University of Wisconsin-Madison’s Computational Biology, Ecology, and Evolution (ComBEE) community, Dr. Hannah Wayment-Steele discusses AlphaFold, an model developed by DeepMind that predicts protein structures from amino acid sequences, along with other similar proten language models ESM and EvoFormer. Wayment-Steele describes the model’s architecture, applications in structural biology, and current pitfalls such as with isoforms.\nBio: Dr. Hannah Wayment-Steele is a assistant professor at UW Madison with a PHD in chemistry from Standford. Her lab focus on biomolecular dynamics and mechanisms that underlie biological function and shape evolution, bringing together the fields of data science, deep learning, and NMR spectroscopy."
  },
  {
    "objectID": "Applications/Videos/Other/25-01-28_AlphaFold_ComBEE.html#see-also",
    "href": "Applications/Videos/Other/25-01-28_AlphaFold_ComBEE.html#see-also",
    "title": "AlphaFold and Protein Language Models",
    "section": "See also",
    "text": "See also\n\n\nWayman Steele Lab: Learn more about Dr. Wayment-Steel’s work from her labratory website.\nComBEE: Main paige for ComBEE.\nML 4 Protein Engineering: Join this research community’s mailing list and visit their YouTube channel to explore the intersection of ML and protein engineering."
  },
  {
    "objectID": "Applications/Videos/SILO/2023-11-22_KennethMarino_ World-Knowledge-in-the-Time-of-Large-Models.html",
    "href": "Applications/Videos/SILO/2023-11-22_KennethMarino_ World-Knowledge-in-the-Time-of-Large-Models.html",
    "title": "World Knowledge in the Time of Large Models",
    "section": "",
    "text": "Summary from SILO website\n\nBio: Kenneth Marino is Research Scientist at Google DeepMind in NYC, focusing on improving knowledge-based systems such as retrieval and information extraction as well as embodied reasoning with language. He graduated in 2021 from Carnegie Mellon University advised by Abhinav Gupta, where his thesis focused on incorporating knowledge into embodied systems. He has an adjunct appointment at Columbia University where he teaches a class focused on the impact of datasets on machine learning and how to collect good datasets. He received his undergraduate degree from the Georgia Institute of Technology where he studied Computer Engineering and Computer Science.\n\n\nAbstract: This talk will discuss the massive shift that has come about in the vision and ML community as a result of the large pre-trained language and language and vision models such as Flamingo, GPT-4, and other models. We begin by looking at the work on knowledge-based systems in CV and robotics before the large model revolution and discuss the impact it had. This impact can be broken down into three areas in which world knowledge should be studied in the context of these new models: evaluation, harnessing large models, and building outside knowledge. First, evaluating world knowledge is even more important as the large model revolution gives more easy access to world knowledge. Next, we discuss recent work in harnessing models such as Flamingo and Chinchilla for visual and procedural knowledge. Finally, the talk discusses how, by focusing on knowledge acquisition as an agent-centric problem, we can make developments in retrieving and collecting world knowledge.\n\n\n\nLinks\n\nAbout the Speaker → kennethmarino.com\nOK-VQA paper and dataset → okvqa.allenai.org/index.html\nKRISP paper → arxiv.org/abs/2012.11014\nSame Object, Different Grasps paper → arxiv.org/abs/2011.06431\nA-OKVQA dataset/GitHub → github.com/allenai/aokvqa\nDistilling Internet-Scale Vision-Language Models into Embodied Agents paper → arxiv.org/abs/2301.12507\n\n\n\n\nJump to section\n\n[0:00] Introducing Kenneth Marino\n[1:11] Begin presentation\n[1:37] What do we want from AI?\n[3:00] The old way: treating all tasks individually\n[3:50] Knowledge / priors matter\n[5:00] LMs have built-in knowledge\n[7:14] Prologue: Before the LLM/VLM revolution\n[8:15] Evaluating knowledge\n[9:30] Evaluating knowledge with OK-VQA\n[10:30] KRISP: Incorporating knowledge graphs\n[11:15] LLMs and VLMs: Accessible world knowledge\n[13:28] Evaluating knowledge in LLMs/VLMs\n[14:40] Many kinds of knowledge\n[16:17] Evaluating knowledge with A-OKVQA\n[22:38] From evaluating to using LLMs/VLMs\n[25:31] Extracting knoweldeg from LLMs\n[26:15] Bringing Flamingo’s knowledge into agents\n[35:48] The only constant is change\n[36:45] Inquisitive agents\n[40:11] Wikipedia navigation as a benchmark\n[53:00] Takeaways\n[53:55] Q&A"
  },
  {
    "objectID": "Applications/Videos/ML4MI/2024-09-16_Vision-Language-and-VisionLanguage-Modeling-in-Radiology_Tyler-Bradshaw.html",
    "href": "Applications/Videos/ML4MI/2024-09-16_Vision-Language-and-VisionLanguage-Modeling-in-Radiology_Tyler-Bradshaw.html",
    "title": "Vision, Language, and Vision-Language Modeling in Radiology",
    "section": "",
    "text": "In this talk from the Machine Learning for Medical Imaging (ML4MI) community, Tyler Bradshaw (PhD) discusses the historical context (e.g., CNN, VGG) leading up to the new era of multimodal learning (e.g., vision-language models), and explores how these models are currently being leveraged in the radiology field.\nA netID is required to view ML4MI videos: View 24-09-16 ML4MI recording."
  },
  {
    "objectID": "Applications/Videos/ML4MI/2024-09-16_Vision-Language-and-VisionLanguage-Modeling-in-Radiology_Tyler-Bradshaw.html#see-also",
    "href": "Applications/Videos/ML4MI/2024-09-16_Vision-Language-and-VisionLanguage-Modeling-in-Radiology_Tyler-Bradshaw.html#see-also",
    "title": "Vision, Language, and Vision-Language Modeling in Radiology",
    "section": "See also",
    "text": "See also\n\nModel: UNET: Learn more about the UNET model."
  },
  {
    "objectID": "Applications/Videos/ML4MI/2023-07-12_Unifying-Audio-Visual-Machine-Perception-Tasks-and-Architectures_Pedro-Morgado.html",
    "href": "Applications/Videos/ML4MI/2023-07-12_Unifying-Audio-Visual-Machine-Perception-Tasks-and-Architectures_Pedro-Morgado.html",
    "title": "Unifying Audio-Visual Machine Perception – Tasks & Architectures",
    "section": "",
    "text": "Accurately recognizing, localizing, and separating sound sources is essential for effective audio-visual perception. Traditionally, these tasks have been approached independently, with separate methods developed for each. However, the interdependencies between source localization, separation, and recognition make it clear that independent models may yield suboptimal performance. To address this, our research focuses on unifying audio-visual learning tasks and architectures to integrate audio and visual cues for joint localization, separation, and recognition. In this talk, I will present our recent progress in this field. I will introduce a unified pretraining framework that enables simultaneous learning of audio-visual recognition, localization, and separation. Additionally, I will showcase a novel early fusion architecture that incorporates local audio-visual interactions, which can be efficiently pre-trained using an audio-visual masked autoencoding framework. The objective of unified pre-training of early fusion models is to replicate human-like multi-modal perception, promising a deeper and more sophisticated understanding of audio-visual interactions, crucial for these true multimodal applications. Throughout the talk, I will share a sequence of compelling findings that demonstrate the strong positive transfer between these tasks. Furthermore, I will highlight the substantial benefits that early audio-visual fusion can provide in enhancing model expressivity and consequently performance on challenging audio-visual applications.\nBio: Pedro is an Assistant Professor at the University of Wisconsin-Madison in the department of Electrical and Computer Engineering, and affiliated with the Computer Sciences department. His research interests are at the intersection of computer vision and machine learning, focusing on developing systems that continuously learn to perceive the world through multiple sensory modalities without direct human supervision. Prior to joining UW-Madison, he was a post-doctoral fellow at Carnegie Mellon University, working with Abhinav Gupta. He earned his Ph.D. degree from the University of California San Diego advised by Prof. Nuno Vasconcelos, and his B.Sc. and M.Sc. degrees from Universidade de Lisboa, Portugal.\nA netID is required to view ML4MI videos: View 2023-07-12 recording."
  },
  {
    "objectID": "Applications/Videos/ML4MI/2023-07-12_Unifying-Audio-Visual-Machine-Perception-Tasks-and-Architectures_Pedro-Morgado.html#see-also",
    "href": "Applications/Videos/ML4MI/2023-07-12_Unifying-Audio-Visual-Machine-Perception-Tasks-and-Architectures_Pedro-Morgado.html#see-also",
    "title": "Unifying Audio-Visual Machine Perception – Tasks & Architectures",
    "section": "See also",
    "text": "See also\n\nML4MI: Explore other talks from the ML4MI group at UW-Madison."
  },
  {
    "objectID": "Applications/Videos/ML4MI/2024-10-14_Using-Electronic-Health-Record-Data-to-Predict-Deterioriation-in-Hostpitalized-Children_Anoop-Mayampurath.html",
    "href": "Applications/Videos/ML4MI/2024-10-14_Using-Electronic-Health-Record-Data-to-Predict-Deterioriation-in-Hostpitalized-Children_Anoop-Mayampurath.html",
    "title": "Using Electronic Health Record Data to Predict Deterioriation in Hospitalized Children",
    "section": "",
    "text": "In this talk from the Machine Learning for Medical Imaging (ML4MI) community, Anoop Mayampurath (PhD) discusses the use of electronic health record (EHR) data and machine learning to predict clinical deterioration in hospitalized children. The presentation explores how traditional methods like the Pediatric Early Warning System (PEWS) fall short and introduces a novel model, pCART, which significantly improves outcomes by enabling early and accurate detection of at-risk patients. pCART (Pediatric Cardiac Arrest Risk Tool) is a gradient boosted tree model designed to identify clinical deterioration in hospitalized children, particularly those at risk of requiring ICU transfers. Unlike traditional methods like the Pediatric Early Warning System (PEWS), which rely on static, age-dependent cutoffs and subjective assessments, pCART utilizes advanced analytics and continuous tracking to provide a more accurate and actionable risk assessment.\nA netID is required to view ML4MI videos: View 2024-10-14 recording."
  },
  {
    "objectID": "Applications/Videos/ML4MI/2024-10-14_Using-Electronic-Health-Record-Data-to-Predict-Deterioriation-in-Hostpitalized-Children_Anoop-Mayampurath.html#see-also",
    "href": "Applications/Videos/ML4MI/2024-10-14_Using-Electronic-Health-Record-Data-to-Predict-Deterioriation-in-Hostpitalized-Children_Anoop-Mayampurath.html#see-also",
    "title": "Using Electronic Health Record Data to Predict Deterioriation in Hospitalized Children",
    "section": "See also",
    "text": "See also\n\nML4MI: Explore other talks from the ML4MI group at UW-Madison."
  },
  {
    "objectID": "Applications/Videos/Exploring-AI-at-UW/2023-09-22_LoriKidoLopez_ Racism-in-AI.html",
    "href": "Applications/Videos/Exploring-AI-at-UW/2023-09-22_LoriKidoLopez_ Racism-in-AI.html",
    "title": "Racism in AI",
    "section": "",
    "text": "Is artificial intelligence racist? In short, yes, it sure has been. But there’s more to the story. Lori Kido Lopez, UW–Madison professor and author of “Race and Digital Media”” discusses how histories of institutionalized racism in big data, tech industries and society are shaping AI today.\nThis is the 13th installment in our webinar series about AI. You can learn more about the series, catch up on past seminars, and see what we have planned next on our “Exploring Artificial Intelligence @ UW–Madison” page.\n\nA netID is required to view this recording: View 2023-09-22 recording."
  },
  {
    "objectID": "Applications/Videos/Exploring-AI-at-UW/2023-09-22_LoriKidoLopez_ Racism-in-AI.html#see-also",
    "href": "Applications/Videos/Exploring-AI-at-UW/2023-09-22_LoriKidoLopez_ Racism-in-AI.html#see-also",
    "title": "Racism in AI",
    "section": "See also",
    "text": "See also\n\nExploring Artificial Intelligence @ UW-Madison: Explore other talks from the Exploring AI @ UW series."
  },
  {
    "objectID": "Applications/Videos/Exploring-AI-at-UW/index.html",
    "href": "Applications/Videos/Exploring-AI-at-UW/index.html",
    "title": "Exploring Artificial Intelligence @ UW-Madison",
    "section": "",
    "text": "Explore a library of recordings (netID required) from the Exploring Artificial Intelligence @ UW-Madison series! Additional talks can also be found on the seminar website.\nFrom their website:\n\nIn today’s rapidly evolving world, the importance of AI in our academic environment cannot be overstated. As technological advancements continue to shape our society, we believe it is crucial to explore the opportunities and challenges AI brings to the forefront of higher education.\nThis webinar series aimed to provide a platform for experts and visionaries in the field of AI to share their insights, research and experiences in the classroom, research lab and wider academic community. By delving into topics such as AI ethics, cutting-edge machine learning algorithms, automation and human-machine collaboration, we hoped to foster a deeper understanding of AI’s transformative potential and its implications for higher education.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBiomedical AI Research and Applications\n\n\n\nVideos\n\nUW-Madison\n\nExploring AI@UW\n\nBiology\n\nHealthcare\n\nLLM\n\nComputer vision\n\nMultimodal learning\n\n\n\n\n\n\n\n\n\n2023-09-22\n\n\nAnthony Gitter, Junjie Hu, Yin Li, Anoop Mayampurath, Vikas Singh\n\n\n\n\n\n\n\n\n\n\n\n\nRacism in AI\n\n\n\nVideos\n\nUW-Madison\n\nExploring AI@UW\n\nEthical AI\n\nBias\n\nFairness\n\n\n\n\n\n\n\n\n\n2023-09-22\n\n\nLori Kido Lopez\n\n\n\n\n\n\n\n\n\n\n\n\nAI and Medical Imaging\n\n\n\nVideos\n\nExploring AI@UW\n\nUW-Madison\n\nHealthcare\n\nMedical imaging\n\nImage classification\n\nComputer vision\n\nDeep learning\n\n\n\n\n\n\n\n\n\n2023-08-11\n\n\nLaurel Belman, Todd Shechter, Alan McMillan\n\n\n\n\n\n\n\n\n\n\n\n\nAI in Action: Intelligent Systems and Business Operations\n\n\n\nVideos\n\nUW-Madison\n\nExploring AI@UW\n\nBusiness\n\nBias\n\nEthical AI\n\n\n\n\n\n\n\n\n\n2023-07-28\n\n\nLaura Albert\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Applications",
      "Videos",
      "AI @ UW"
    ]
  },
  {
    "objectID": "Learn/Workshops/Intro-TextAnalysis_Python.html",
    "href": "Learn/Workshops/Intro-TextAnalysis_Python.html",
    "title": "Intro to Text Analysis / NLP",
    "section": "",
    "text": "The Intro to Text Analysis workshop introduces the field of Natural Language Processing (NLP) and how to gain insights from collections of text data (i.e., a corpus). This includes a hands-on, step-by-step guide on how to source and prepare a corpus for analysis, generate text (document/sentence/word) embeddings, perform topic modeling, deploy common models (e.g., Word2Vec and large language models using Hugging Face), and ethical considerations. Students and researchers working with text data (especially the digital humanities!) are encouraged to take this workshop!\n\n\nLearners are expected to have basic Python programming skills and familiarity with the Pandas package. If you need a refresher, the Introductory Python lesson materials are available for independent study.\n\n\n\nThis workshop takes approximately 16 hours to complete.\n\n\n\nThe Carpentries is a global organization of researchers who volunteer their time and effort to create workshops that teach software engineering and data analysis skills to other researchers. UW-Madison has its own local Carpentries community which is actively engaged in developing new ML/AI workshops. To be notified of upcoming workshops offered by the Carpentries, make sure to subscribe to the Data Science @ UW Newsletter.\n\n\n\nAll Carpentries lessons are published as open source educational materials. You are welcome and encouraged to visit the lesson materials to work through them on your own. Our friends at UW–Milwaukee have also published recorded walkthrough videos of this workshop that you can follow along with asynchronously. This is a great option if you prefer learning at your own pace or want to revisit the material after participating in a live session.\nIf you are involved with a research lab at UW-Madison campus, you may attend Coding Meetup (Tue/Thur, 2:30-4:30pm) to get help working through the materials.\n\n\n\n\nWorkshop: Intro to Deep Learning with Keras: Explore deep learning concepts in greater detail. This will help you better understand the technology (neural networks) needed for Word2Vec and large language models.\nBook: Understanding Deep Learning - Simon J.D. Prince: This free textbook is a good modern overview of deep learning (and machine learning in general), and provides colab notebooks to explore deep learning concepts and implementations. The book uses PyTorch as its framework of choice. You may find additional details in this book that the workshop only briefly touches on."
  },
  {
    "objectID": "Learn/Workshops/Intro-TextAnalysis_Python.html#about-this-resource",
    "href": "Learn/Workshops/Intro-TextAnalysis_Python.html#about-this-resource",
    "title": "Intro to Text Analysis / NLP",
    "section": "",
    "text": "The Intro to Text Analysis workshop introduces the field of Natural Language Processing (NLP) and how to gain insights from collections of text data (i.e., a corpus). This includes a hands-on, step-by-step guide on how to source and prepare a corpus for analysis, generate text (document/sentence/word) embeddings, perform topic modeling, deploy common models (e.g., Word2Vec and large language models using Hugging Face), and ethical considerations. Students and researchers working with text data (especially the digital humanities!) are encouraged to take this workshop!\n\n\nLearners are expected to have basic Python programming skills and familiarity with the Pandas package. If you need a refresher, the Introductory Python lesson materials are available for independent study.\n\n\n\nThis workshop takes approximately 16 hours to complete.\n\n\n\nThe Carpentries is a global organization of researchers who volunteer their time and effort to create workshops that teach software engineering and data analysis skills to other researchers. UW-Madison has its own local Carpentries community which is actively engaged in developing new ML/AI workshops. To be notified of upcoming workshops offered by the Carpentries, make sure to subscribe to the Data Science @ UW Newsletter.\n\n\n\nAll Carpentries lessons are published as open source educational materials. You are welcome and encouraged to visit the lesson materials to work through them on your own. Our friends at UW–Milwaukee have also published recorded walkthrough videos of this workshop that you can follow along with asynchronously. This is a great option if you prefer learning at your own pace or want to revisit the material after participating in a live session.\nIf you are involved with a research lab at UW-Madison campus, you may attend Coding Meetup (Tue/Thur, 2:30-4:30pm) to get help working through the materials.\n\n\n\n\nWorkshop: Intro to Deep Learning with Keras: Explore deep learning concepts in greater detail. This will help you better understand the technology (neural networks) needed for Word2Vec and large language models.\nBook: Understanding Deep Learning - Simon J.D. Prince: This free textbook is a good modern overview of deep learning (and machine learning in general), and provides colab notebooks to explore deep learning concepts and implementations. The book uses PyTorch as its framework of choice. You may find additional details in this book that the workshop only briefly touches on."
  },
  {
    "objectID": "Learn/Workshops/TrustworthyAI_Explainability-Bias-Fairness-OODdetection.html",
    "href": "Learn/Workshops/TrustworthyAI_Explainability-Bias-Fairness-OODdetection.html",
    "title": "Trustworthy AI - Explainability, Bias, Fairness, and Safety",
    "section": "",
    "text": "The Trustworthy AI: Explainability, Bias, Fairness, and Safety workshop equips participants with practical skills to evaluate and improve the trustworthiness of machine learning models. Spanning structured data (tabular), natural language processing (NLP), and computer vision applications, this workshop integrates tools and techniques to enhance fairness, explainability, and reliability. Participants will explore real-world applications using cutting-edge libraries like AIF360, PyTorch-OOD, and GradCAM.\nThe lesson is now in the alpha phase, meaning it has been piloted and is ready for broader use and feedback. Materials may continue to evolve with additional topics and exercises.\nHave ideas for other topics or tools to include? We welcome suggestions! Please post an issue on the lesson’s GitHub repository.\n\n\n\nFairness: Hands-on bias detection and mitigation using AIF360.\nExplainability: Techniques such as GradCAM and linear probes for visualizing model behavior.\nSafety and Reliability: Out-of-distribution (OOD) detection with PyTorch-OOD.\nTrustworthiness: Ensuring models are interpretable, reproducible, and aligned with ethical practices.\n\n\n\nLearners should have:\n\nPython programming experience.\nFamiliarity with machine learning concepts like train/test splits and cross-validation.\nExposure to neural networks and basic model training.\n\n\n\n\nThis workshop takes approximately 12 hours, divided across multiple hands-on episodes.\n\n\n\n\nThis workshop is part of UW-Madison’s local Carpentries community efforts to develop advanced ML/AI educational materials. To stay updated about upcoming workshops, subscribe to the Data Science @ UW Newsletter.\n\n\n\nThe full lesson is available as open-source materials. Visit the lesson materials to explore on your own. If you’re at UW-Madison, join Coding Meetup (Tue/Thur, 2:30-4:30pm) for assistance."
  },
  {
    "objectID": "Learn/Workshops/TrustworthyAI_Explainability-Bias-Fairness-OODdetection.html#about-this-resource",
    "href": "Learn/Workshops/TrustworthyAI_Explainability-Bias-Fairness-OODdetection.html#about-this-resource",
    "title": "Trustworthy AI - Explainability, Bias, Fairness, and Safety",
    "section": "",
    "text": "The Trustworthy AI: Explainability, Bias, Fairness, and Safety workshop equips participants with practical skills to evaluate and improve the trustworthiness of machine learning models. Spanning structured data (tabular), natural language processing (NLP), and computer vision applications, this workshop integrates tools and techniques to enhance fairness, explainability, and reliability. Participants will explore real-world applications using cutting-edge libraries like AIF360, PyTorch-OOD, and GradCAM.\nThe lesson is now in the alpha phase, meaning it has been piloted and is ready for broader use and feedback. Materials may continue to evolve with additional topics and exercises.\nHave ideas for other topics or tools to include? We welcome suggestions! Please post an issue on the lesson’s GitHub repository.\n\n\n\nFairness: Hands-on bias detection and mitigation using AIF360.\nExplainability: Techniques such as GradCAM and linear probes for visualizing model behavior.\nSafety and Reliability: Out-of-distribution (OOD) detection with PyTorch-OOD.\nTrustworthiness: Ensuring models are interpretable, reproducible, and aligned with ethical practices.\n\n\n\nLearners should have:\n\nPython programming experience.\nFamiliarity with machine learning concepts like train/test splits and cross-validation.\nExposure to neural networks and basic model training.\n\n\n\n\nThis workshop takes approximately 12 hours, divided across multiple hands-on episodes.\n\n\n\n\nThis workshop is part of UW-Madison’s local Carpentries community efforts to develop advanced ML/AI educational materials. To stay updated about upcoming workshops, subscribe to the Data Science @ UW Newsletter.\n\n\n\nThe full lesson is available as open-source materials. Visit the lesson materials to explore on your own. If you’re at UW-Madison, join Coding Meetup (Tue/Thur, 2:30-4:30pm) for assistance."
  },
  {
    "objectID": "Learn/Workshops/TrustworthyAI_Explainability-Bias-Fairness-OODdetection.html#questions",
    "href": "Learn/Workshops/TrustworthyAI_Explainability-Bias-Fairness-OODdetection.html#questions",
    "title": "Trustworthy AI - Explainability, Bias, Fairness, and Safety",
    "section": "Questions?",
    "text": "Questions?\nIf you have questions about this workshop, visit the Nexus Q&A on GitHub. Feedback is welcome and will help improve future iterations of the workshop."
  },
  {
    "objectID": "Learn/Workshops/TrustworthyAI_Explainability-Bias-Fairness-OODdetection.html#see-also",
    "href": "Learn/Workshops/TrustworthyAI_Explainability-Bias-Fairness-OODdetection.html#see-also",
    "title": "Trustworthy AI - Explainability, Bias, Fairness, and Safety",
    "section": "See also",
    "text": "See also\n\nLibrary: AIF360: Learn about fairness metrics and bias mitigation strategies for ML.\nLibrary: PyTorch-OOD: Tools for detecting out-of-distribution data and anomalies.\nWorkshop: Intro to Machine Learning with Sklearn: Once you master Python fundamentals, start using the scikit-learn package to begin exploring “classical” ML methods (e.g., regression, clustering, decision trees).\nWorkshop: Intro to Deep Learning with PyTorch: Dive into PyTorch as an alternative deep learning framework.\nTalk: Intro to Out-of-Distribution Detection: Learn more about the pervasive problem of out-of-distribution data, and techniques available to mitigate this problem.\nTalk: Trustworthy LLMs & Ethical AI: Learn how DeTox can be used to remove toxic embeddings in LLMs."
  },
  {
    "objectID": "Learn/Workshops/Intro-Amazon_SageMaker.html",
    "href": "Learn/Workshops/Intro-Amazon_SageMaker.html",
    "title": "Intro to AWS SageMaker for Predictive ML/AI",
    "section": "",
    "text": "This introductory AWS SageMaker workshop teaches core workflows for running predictive ML/AI models in AWS SageMaker, an AWS-managed machine learning environment. Participants will learn to set up data, configure SageMaker Notebooks, manage code repositories, train and tune models, and optimize resource costs effectively within AWS. While currently tailored for 2024 Machine Learning Marathon participants, a more general version of the materials will be released in the coming months. The lesson is in the pre-alpha phase, meaning it is under development and has not yet been taught. Users will benefit from tips on controlling AWS expenses and scaling models efficiently, with real-world guidance on choosing appropriate CPU and GPU resources.\n\n\nRunning through this workshop should cost approximately $10-$15 on AWS, assuming moderate usage of GPU instances and a few parallel jobs. For new AWS accounts, the AWS Free Tier may cover some of these costs, including 250 hours per month of the ml.t2.medium instance for the first two months, as well as some limited S3 storage. This means new users may be able to complete certain parts of the workshop for free or at a significantly reduced cost. We recommend monitoring usage through the AWS Billing Dashboard to stay within the free tier and manage any extra expenses effectively.\n\n\n\n\nIntro to Machine Learning\nBasic Python Programming\n\n\n\n\n3-5 hours: Based on running through training, tuning, and experimenting with example code setups."
  },
  {
    "objectID": "Learn/Workshops/Intro-Amazon_SageMaker.html#about-this-resource",
    "href": "Learn/Workshops/Intro-Amazon_SageMaker.html#about-this-resource",
    "title": "Intro to AWS SageMaker for Predictive ML/AI",
    "section": "",
    "text": "This introductory AWS SageMaker workshop teaches core workflows for running predictive ML/AI models in AWS SageMaker, an AWS-managed machine learning environment. Participants will learn to set up data, configure SageMaker Notebooks, manage code repositories, train and tune models, and optimize resource costs effectively within AWS. While currently tailored for 2024 Machine Learning Marathon participants, a more general version of the materials will be released in the coming months. The lesson is in the pre-alpha phase, meaning it is under development and has not yet been taught. Users will benefit from tips on controlling AWS expenses and scaling models efficiently, with real-world guidance on choosing appropriate CPU and GPU resources.\n\n\nRunning through this workshop should cost approximately $10-$15 on AWS, assuming moderate usage of GPU instances and a few parallel jobs. For new AWS accounts, the AWS Free Tier may cover some of these costs, including 250 hours per month of the ml.t2.medium instance for the first two months, as well as some limited S3 storage. This means new users may be able to complete certain parts of the workshop for free or at a significantly reduced cost. We recommend monitoring usage through the AWS Billing Dashboard to stay within the free tier and manage any extra expenses effectively.\n\n\n\n\nIntro to Machine Learning\nBasic Python Programming\n\n\n\n\n3-5 hours: Based on running through training, tuning, and experimenting with example code setups."
  },
  {
    "objectID": "Learn/Workshops/Intro-Amazon_SageMaker.html#questions",
    "href": "Learn/Workshops/Intro-Amazon_SageMaker.html#questions",
    "title": "Intro to AWS SageMaker for Predictive ML/AI",
    "section": "Questions?",
    "text": "Questions?\nFor any questions, please post to the Nexus Q&A on GitHub. Feedback is especially helpful in these early stages to improve workshop materials!"
  },
  {
    "objectID": "Learn/Workshops/Intro-Amazon_SageMaker.html#see-also",
    "href": "Learn/Workshops/Intro-Amazon_SageMaker.html#see-also",
    "title": "Intro to AWS SageMaker for Predictive ML/AI",
    "section": "See also",
    "text": "See also\n\nAWS Free Tier Guide: An overview of the AWS Free Tier, including limitations and expected costs for beginner users.\nCenter for High Throughput Computing (CHTC): CHTC at UW-Madison offers free compute resources to researchers."
  },
  {
    "objectID": "Learn/Workshops/index.html",
    "href": "Learn/Workshops/index.html",
    "title": "Workshops",
    "section": "",
    "text": "UW-Madison has its own local Carpentries community which is actively engaged in developing and teaching new ML/AI workshops. To be notified of upcoming workshops offered by the Carpentries and other organizations at UW-Madison, make sure to subscribe to the Data Science @ UW Newsletter.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrustworthy AI - Explainability, Bias, Fairness, and Safety\n\n\n\nWorkshops\n\nTrustworthy AI\n\nEthical AI\n\nExplainability\n\nFairness\n\nBias\n\nOOD detection\n\nAIF360\n\nPyTorch-OOD\n\nAnomaly detection\n\nGradCAM\n\nTabular\n\nComputer vision\n\nNLP\n\nText analysis\n\nCode-along\n\nCarpentries\n\nDeep learning\n\nModel sharing\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-12-03\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to AWS SageMaker for Predictive ML/AI\n\n\n\nWorkshops\n\nCode-along\n\nCarpentries\n\nCompute\n\nAWS\n\nGPU\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-11-07\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Python\n\n\n\nWorkshops\n\nPython\n\nCarpentries\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Machine Learning with Sklearn\n\n\n\nWorkshops\n\nLibraries\n\nClassical ML\n\nBoosting\n\nDecision trees\n\nSVM\n\nClustering\n\nRegression\n\nSklearn\n\nCode-along\n\nCarpentries\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-17\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Deep Learning with Keras\n\n\n\nWorkshops\n\nLibraries\n\nDeep learning\n\nKeras\n\nCode-along\n\nCarpentries\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-16\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Deep Learning with PyTorch\n\n\n\nWorkshops\n\nLibraries\n\nDeep learning\n\nPyTorch\n\nUdacity\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-15\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Text Analysis / NLP\n\n\n\nWorkshops\n\nDeep learning\n\nHugging Face\n\nText analysis\n\nNLP\n\nLLM\n\nCode-along\n\nCarpentries\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-13\n\n\n\n\n\n\n\n\n\n\n\n\n\nVersion Control with Git and GitHub (Carpentries)\n\n\n\nWorkshops\n\nVideos\n\nReproducibility\n\nGit/GitHub\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-11\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Learn",
      "Workshops"
    ]
  },
  {
    "objectID": "Learn/Books/Intro-StatisticalLearning_JamesGareth-DanielaWitten-HastieTrevor-TibshiraniRob.html",
    "href": "Learn/Books/Intro-StatisticalLearning_JamesGareth-DanielaWitten-HastieTrevor-TibshiraniRob.html",
    "title": "Introduction to Statistical Learning",
    "section": "",
    "text": "An Introduction to Statistical Learning (ISL) is a foundational book for anyone interested in statistical learning and its applications in data analysis. Authored by Gareth James, Daniela Witten, Trevor Hastie, and Rob Tibshirani, ISL provides an approachable yet comprehensive introduction to the field, covering critical topics like regression, classification, tree-based methods, and deep learning. It is designed to cater to readers with minimal mathematical background, making it suitable for a broad audience. Each chapter includes practical labs that use R or Python to solidify concepts through hands-on coding.\nStatistical learning, a term predating the widespread use of machine learning, emphasizes rigorous statistical frameworks and interpretability. While the term is less common today, it remains central in academic and statistical contexts. ISL bridges statistical learning with modern ML approaches, offering readers a practical foundation in both.\nThe first edition of ISL (with R) was published in 2013, and a second edition followed in 2021. In 2023, a Python edition was released, broadening the book’s accessibility to users of different programming languages. This resource has been translated into multiple languages and remains a cornerstone text for learning statistical learning methods.\n\n\n\nAs the scale and scope of data collection continue to increase across virtually all fields, statistical learning has become a critical toolkit for anyone who wishes to understand data. An Introduction to Statistical Learning provides a broad and less technical treatment of key topics in statistical learning. This book is appropriate for anyone who wishes to use contemporary tools for data analysis.\n\n\n\n\nBasic understanding of statistics and linear algebra\nFamiliarity with R or Python is helpful for the labs\n\n\n\n\nTBD: Use the Improve this page functionality to add your own estimate!"
  },
  {
    "objectID": "Learn/Books/Intro-StatisticalLearning_JamesGareth-DanielaWitten-HastieTrevor-TibshiraniRob.html#about-this-resource",
    "href": "Learn/Books/Intro-StatisticalLearning_JamesGareth-DanielaWitten-HastieTrevor-TibshiraniRob.html#about-this-resource",
    "title": "Introduction to Statistical Learning",
    "section": "",
    "text": "An Introduction to Statistical Learning (ISL) is a foundational book for anyone interested in statistical learning and its applications in data analysis. Authored by Gareth James, Daniela Witten, Trevor Hastie, and Rob Tibshirani, ISL provides an approachable yet comprehensive introduction to the field, covering critical topics like regression, classification, tree-based methods, and deep learning. It is designed to cater to readers with minimal mathematical background, making it suitable for a broad audience. Each chapter includes practical labs that use R or Python to solidify concepts through hands-on coding.\nStatistical learning, a term predating the widespread use of machine learning, emphasizes rigorous statistical frameworks and interpretability. While the term is less common today, it remains central in academic and statistical contexts. ISL bridges statistical learning with modern ML approaches, offering readers a practical foundation in both.\nThe first edition of ISL (with R) was published in 2013, and a second edition followed in 2021. In 2023, a Python edition was released, broadening the book’s accessibility to users of different programming languages. This resource has been translated into multiple languages and remains a cornerstone text for learning statistical learning methods.\n\n\n\nAs the scale and scope of data collection continue to increase across virtually all fields, statistical learning has become a critical toolkit for anyone who wishes to understand data. An Introduction to Statistical Learning provides a broad and less technical treatment of key topics in statistical learning. This book is appropriate for anyone who wishes to use contemporary tools for data analysis.\n\n\n\n\nBasic understanding of statistics and linear algebra\nFamiliarity with R or Python is helpful for the labs\n\n\n\n\nTBD: Use the Improve this page functionality to add your own estimate!"
  },
  {
    "objectID": "Learn/Books/Intro-StatisticalLearning_JamesGareth-DanielaWitten-HastieTrevor-TibshiraniRob.html#questions",
    "href": "Learn/Books/Intro-StatisticalLearning_JamesGareth-DanielaWitten-HastieTrevor-TibshiraniRob.html#questions",
    "title": "Introduction to Statistical Learning",
    "section": "Questions?",
    "text": "Questions?\nIf you have any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Learn/Books/Intro-StatisticalLearning_JamesGareth-DanielaWitten-HastieTrevor-TibshiraniRob.html#see-also",
    "href": "Learn/Books/Intro-StatisticalLearning_JamesGareth-DanielaWitten-HastieTrevor-TibshiraniRob.html#see-also",
    "title": "Introduction to Statistical Learning",
    "section": "See also",
    "text": "See also\n\nISL Official Website: Access downloads, errata, and additional resources for ISL.\nThe Elements of Statistical Learning: A more advanced companion book by the same authors.\nUnderstanding Deep Learning: A similar code-along style reading that provides a modern overview of deep learning."
  },
  {
    "objectID": "Learn/Books/index.html",
    "href": "Learn/Books/index.html",
    "title": "Books",
    "section": "",
    "text": "Introduction to Statistical Learning\n\n\n\nBooks\n\nStatistical learning\n\nClassical ML\n\nRegression\n\nClassification\n\nDeep learning\n\nDecision trees\n\nUnsupervised Learning\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2025-01-06\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Deep Learning\n\n\n\nBooks\n\nDeep learning\n\nPyTorch\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-14\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Learn",
      "Books"
    ]
  },
  {
    "objectID": "Learn/Blogs/one-useful-thing.html",
    "href": "Learn/Blogs/one-useful-thing.html",
    "title": "One Useful Thing",
    "section": "",
    "text": "One Useful Thing is a blog where Wharton professor Ethan Mollick “provides a research-based view on the implications of AI.” Since November 2022 Mollick has been sharing a new post several times a month, reflecting on the impacts of consumer-facing LLMs throughout society, but especially in the workplace.\nTopics range from practical advice on how to encourage adoption of AI in the workplace, to reflections on AI ethics questions, to in-depth studies of the abilities and limitations of consumer-facing AI products, to futurist explorations of the potential impacts of AI."
  },
  {
    "objectID": "Learn/Blogs/one-useful-thing.html#about-this-resource",
    "href": "Learn/Blogs/one-useful-thing.html#about-this-resource",
    "title": "One Useful Thing",
    "section": "",
    "text": "One Useful Thing is a blog where Wharton professor Ethan Mollick “provides a research-based view on the implications of AI.” Since November 2022 Mollick has been sharing a new post several times a month, reflecting on the impacts of consumer-facing LLMs throughout society, but especially in the workplace.\nTopics range from practical advice on how to encourage adoption of AI in the workplace, to reflections on AI ethics questions, to in-depth studies of the abilities and limitations of consumer-facing AI products, to futurist explorations of the potential impacts of AI."
  },
  {
    "objectID": "Learn/Blogs/one-useful-thing.html#questions",
    "href": "Learn/Blogs/one-useful-thing.html#questions",
    "title": "One Useful Thing",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Learn/Videos/OOD-detection.html",
    "href": "Learn/Videos/OOD-detection.html",
    "title": "Intro to Out-of-Distribution Detection",
    "section": "",
    "text": "The below tutorial from Sharon Li, an Assistant Professor in the Department of Computer Sciences at the University of Wisconsin-Madison, introduces a pervasive problem faced by many machine learning systems deployed in the wild — out-of-distribution data.\nOut-of-distribution data, often overlooked but immensely consequential, poses a significant threat to the reliability and efficacy of machine learning models. Through Sharon’s presentation, viewers gain a comprehensive understanding of this complex phenomenon and its potential ramifications on predictive accuracy.\nCheck out the video below to learn more about this problem and the cutting-edge methods you can equip yourself with to prevent inaccurate model predictions.\n\n\nDetails, slides and videos from other talks at ICCV 2023: abursuc.github.io/many-faces-reliability/"
  },
  {
    "objectID": "Learn/Videos/OOD-detection.html#about-this-resource",
    "href": "Learn/Videos/OOD-detection.html#about-this-resource",
    "title": "Intro to Out-of-Distribution Detection",
    "section": "",
    "text": "The below tutorial from Sharon Li, an Assistant Professor in the Department of Computer Sciences at the University of Wisconsin-Madison, introduces a pervasive problem faced by many machine learning systems deployed in the wild — out-of-distribution data.\nOut-of-distribution data, often overlooked but immensely consequential, poses a significant threat to the reliability and efficacy of machine learning models. Through Sharon’s presentation, viewers gain a comprehensive understanding of this complex phenomenon and its potential ramifications on predictive accuracy.\nCheck out the video below to learn more about this problem and the cutting-edge methods you can equip yourself with to prevent inaccurate model predictions.\n\n\nDetails, slides and videos from other talks at ICCV 2023: abursuc.github.io/many-faces-reliability/"
  },
  {
    "objectID": "Learn/Videos/OOD-detection.html#questions",
    "href": "Learn/Videos/OOD-detection.html#questions",
    "title": "Intro to Out-of-Distribution Detection",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Learn/Videos/Intro-git-github.html",
    "href": "Learn/Videos/Intro-git-github.html",
    "title": "Version Control with Git and GitHub (Carpentries)",
    "section": "",
    "text": "The below video (and the 7 subsequent videos in the workshop playlist) will walk you through this introductory Git workshop from the Carpentries: Version Control with Git/GitHub. Git is a free and open source version control system that has become the #1 choice for software developers both in research and industry. Unlike centralized version control systems where there is a single central repository, Git allows every user to have a full copy of the entire project history on their own machine. This distributed nature enables multiple people to work on a project simultaneously without interfering with each other’s work. Git stores the history of changes in a project, enabling users to track progress, revert to previous states, and manage branches for different features or versions of a project. GitHub is a web-based platform that uses Git for version control. It provides a collaborative environment where users can host and review code, manage projects, and build software alongside millions of other developers. GitHub also offers additional features such as issue tracking, project management tools, and continuous integration workflows.\n\n\n\nIn this lesson we use Git from the Unix Shell. Some previous experience with the shell is expected, but isn’t mandatory. For help with Unix Shell, check out the Intro to Unix Shell workshop (Carpentries).\n\n\n\n\n\nThis workshop takes rough 3-4 hours to complete.\n\n\n\n\nThe Carpentries is a global organization of researchers who volunteer their time and effort to create workshops that teach software engineering and data analysis skills to other researchers. UW-Madison has its own local Carpentries community which is actively engaged in developing new ML/AI workshops. To be notified of upcoming workshops offered by the Carpentries, make sure to subscribe to the Data Science @ UW Newsletter."
  },
  {
    "objectID": "Learn/Videos/Intro-git-github.html#about-this-resource",
    "href": "Learn/Videos/Intro-git-github.html#about-this-resource",
    "title": "Version Control with Git and GitHub (Carpentries)",
    "section": "",
    "text": "The below video (and the 7 subsequent videos in the workshop playlist) will walk you through this introductory Git workshop from the Carpentries: Version Control with Git/GitHub. Git is a free and open source version control system that has become the #1 choice for software developers both in research and industry. Unlike centralized version control systems where there is a single central repository, Git allows every user to have a full copy of the entire project history on their own machine. This distributed nature enables multiple people to work on a project simultaneously without interfering with each other’s work. Git stores the history of changes in a project, enabling users to track progress, revert to previous states, and manage branches for different features or versions of a project. GitHub is a web-based platform that uses Git for version control. It provides a collaborative environment where users can host and review code, manage projects, and build software alongside millions of other developers. GitHub also offers additional features such as issue tracking, project management tools, and continuous integration workflows.\n\n\n\nIn this lesson we use Git from the Unix Shell. Some previous experience with the shell is expected, but isn’t mandatory. For help with Unix Shell, check out the Intro to Unix Shell workshop (Carpentries).\n\n\n\n\n\nThis workshop takes rough 3-4 hours to complete.\n\n\n\n\nThe Carpentries is a global organization of researchers who volunteer their time and effort to create workshops that teach software engineering and data analysis skills to other researchers. UW-Madison has its own local Carpentries community which is actively engaged in developing new ML/AI workshops. To be notified of upcoming workshops offered by the Carpentries, make sure to subscribe to the Data Science @ UW Newsletter."
  },
  {
    "objectID": "Learn/Videos/Intro-git-github.html#questions",
    "href": "Learn/Videos/Intro-git-github.html#questions",
    "title": "Version Control with Git and GitHub (Carpentries)",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Learn/Videos/Intro-git-github.html#see-also",
    "href": "Learn/Videos/Intro-git-github.html#see-also",
    "title": "Version Control with Git and GitHub (Carpentries)",
    "section": "See also",
    "text": "See also\n\nGuide: Version Control with GitHub Desktop: GitHub Desktop is a graphical user interface (GUI) application that simplifies the use of Git and GitHub. It is designed for users who prefer not to use the command line interface, offering a more intuitive and visual approach to version control. With GitHub Desktop, you can easily perform common Git tasks such as committing changes, creating branches, and resolving merge conflicts, all within a user-friendly interface.\nVideo: Reproducibility Overview Lecture: If you’re curious to learn how to use Git via shell commands (or just want to become more fluent with Git), check out this YouTube playlist from the Data Science Hub!"
  },
  {
    "objectID": "Learn/Videos/index.html",
    "href": "Learn/Videos/index.html",
    "title": "Videos",
    "section": "",
    "text": "Learning Through Comparison: Use Cases of Contrastive Learning\n\n\n\nVideos\n\nML+X\n\nUW-Madison\n\nDeep learning\n\nContrastive learning\n\nClustering\n\nOOD detection\n\nMultimodal learning\n\nTrustworthy AI\n\nRepresentation learning\n\nCIFAR\n\n\n\n\n\n\n\nYin Li, Chris Endemann\n\n\n2025-02-10\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrokking\n\n\n\nVideos\n\nDeep learning\n\nEmpirical patterns\n\nGrokking\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview of Reproducibility Lecture\n\n\n\nVideos\n\nReproducibility\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\n\nVersion Control with Git and GitHub (Carpentries)\n\n\n\nWorkshops\n\nVideos\n\nReproducibility\n\nGit/GitHub\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-11\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Out-of-Distribution Detection\n\n\n\nVideos\n\nICCV\n\nOOD detection\n\nTrustworthy AI\n\n\n\n\n\n\n\nSharon Li\n\n\n2024-07-11\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Learn",
      "Videos"
    ]
  },
  {
    "objectID": "Learn/Guides/index.html",
    "href": "Learn/Guides/index.html",
    "title": "Guides",
    "section": "",
    "text": "Version Control with GitHub Desktop\n\n\n\nGuides\n\nReproducibility\n\nGit/GitHub\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-11\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Contribute?\n\n\n\nGuides\n\nContribute\n\n\n\n\n\n\n\nML+X\n\n\n2024-06-24\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Learn",
      "Guides"
    ]
  },
  {
    "objectID": "Learn/Notebooks/2025-05-07_RAG-Romeo-Juliet.html",
    "href": "Learn/Notebooks/2025-05-07_RAG-Romeo-Juliet.html",
    "title": "Exploring Fact-Based QA with RAG: Romeo and Juliet",
    "section": "",
    "text": "This notebook demonstrates the use of a Retrieval-Augmented Generation (RAG) system to answer factual questions from Shakespeare’s Romeo and Juliet. Our long-term goal is to build a RAG-powered chatbot that supports literary exploration—helping readers investigate character dynamics, thematic development, and emotional subtext.\nIn this first part of the demo, we focus on low-hanging fruit: factual, quote-supported questions that a RAG pipeline can answer reliably. These examples will help us introduce key RAG components, and set a performance baseline before tackling more interpretive questions."
  },
  {
    "objectID": "Learn/Notebooks/2025-05-07_RAG-Romeo-Juliet.html#step-1-load-the-corpus",
    "href": "Learn/Notebooks/2025-05-07_RAG-Romeo-Juliet.html#step-1-load-the-corpus",
    "title": "Exploring Fact-Based QA with RAG: Romeo and Juliet",
    "section": "Step 1: Load the corpus",
    "text": "Step 1: Load the corpus\nIn this example, we’ll use “Romeo and Juliet” as our text corpus. This text is freely available via Project Gutenberg.\nPreview the file\n\n# Download Romeo and Juliet from Project Gutenberg\nimport requests\n\nurl = 'https://www.gutenberg.org/files/1112/1112-0.txt'\nresponse = requests.get(url)\nfile_contents = response.text\n\n# Preview first 3000 characters\npreview_len = 3000\nprint(file_contents[:preview_len])"
  },
  {
    "objectID": "Learn/Notebooks/2025-05-07_RAG-Romeo-Juliet.html#step-2-split-text-into-chunks",
    "href": "Learn/Notebooks/2025-05-07_RAG-Romeo-Juliet.html#step-2-split-text-into-chunks",
    "title": "Exploring Fact-Based QA with RAG: Romeo and Juliet",
    "section": "Step 2: Split text into “chunks”",
    "text": "Step 2: Split text into “chunks”\nNext, we define a function to split the corpus into smaller chunks based on word count. The simplest “chunking” approach is to chunk by word count or character count.\n\ndef chunk_text(text, max_words=200):\n    import re  # Regular expressions will help us split the text more precisely\n\n    # Use regex to tokenize the text:\n    # This pattern splits the text into:\n    #   - words (\\w+)\n    #   - whitespace (\\s+)\n    #   - punctuation or other non-whitespace symbols ([^\\w\\s])\n    words = re.findall(r'\\w+|\\s+|[^\\w\\s]', text)\n\n    chunks = []  # List to store the resulting text chunks\n    chunk = []   # Temporary buffer to build up each chunk\n\n    # Iterate through each token (word, space, or punctuation)\n    for word in words:\n        chunk.append(word)  # Add token to the current chunk\n        if len(chunk) &gt;= max_words:\n            # Once we reach the max word count, join tokens into a string and store the chunk\n            chunks.append(\"\".join(chunk))  # Use \"\".join() to preserve punctuation/spacing\n            chunk = []  # Reset for the next chunk\n\n    # If there's leftover content after the loop, add the final chunk\n    if chunk:\n        chunks.append(\"\".join(chunk))\n\n    return chunks  # Return list of chunks\n\nWe then apply our chunking function to the corpus.\n\n# Apply the chunking function to your full text file\nchunks = chunk_text(file_contents, max_words=200)\n\n# Show how many chunks were created\nprint(f\"Number of chunks: {len(chunks)}\")\n\n# Preview one of the chunks (by index)\nchunk_ex_ind = 1  # Feel free to change this number to explore different parts of the text\nprint(f\"Chunk {chunk_ex_ind} \\n{chunks[chunk_ex_ind]}\")"
  },
  {
    "objectID": "Learn/Notebooks/2025-05-07_RAG-Romeo-Juliet.html#step-3-embed-chunks-with-sentence-transformers",
    "href": "Learn/Notebooks/2025-05-07_RAG-Romeo-Juliet.html#step-3-embed-chunks-with-sentence-transformers",
    "title": "Exploring Fact-Based QA with RAG: Romeo and Juliet",
    "section": "Step 3: Embed chunks with sentence transformers",
    "text": "Step 3: Embed chunks with sentence transformers\nTo enable semantic search, we need to convert our text chunks into numerical vectors—high-dimensional representations that capture meaning beyond simple keyword overlap. This process is called embedding, and it allows us to compare the semantic similarity between a user’s question and the contents of a document.\nThis is done using an encoder-only transformer model. Unlike decoder or encoder-decoder models, encoder-only models are not designed to generate text. Instead, they are optimized for understanding input sequences and producing meaningful vector representations. These models take in text and output fixed-size embeddings that capture semantic content—ideal for tasks like search, retrieval, and clustering.\nWe’ll use:\n\nThe sentence-transformers library\n\nA widely used library that wraps encoder-only transformer models for generating sentence- and paragraph-level embeddings.\nIt provides a simple interface (model.encode()) and is optimized for performance and batching, making it well-suited for retrieval-augmented generation (RAG) workflows.\nIt supports both short queries and longer document chunks, embedding them into the same shared vector space.\n\nA pretrained model: multi-qa-MiniLM-L6-cos-v1\n\nA compact encoder-only model (6 layers) designed for semantic search and question answering.\nTrained using contrastive learning on query-passage pairs, so it learns to embed related questions and answers close together in vector space.\nIt’s efficient enough to run on CPUs or entry-level GPUs, making it great for experimentation and prototyping.\n\n\n\nWhy embeddings matter in RAG\nIn a RAG system, embeddings are the foundation for connecting a user’s question to the most relevant content in your corpus.\nRather than relying on exact keyword matches, embeddings represent both queries and document chunks in the same semantic space. When a user asks a question, we:\n\nConvert the user’s question into a vector using the same encoder-only embedding model that was used to encode the document chunks.\nCompute similarity scores (e.g., cosine similarity) between the query vector and each chunk vector.\nRetrieve the top-matching chunks to pass along as context to the language model.\n\nThis allows the system to surface text that is meaningfully related to the question—even if it doesn’t use the same words. For example, a question like “What does Juliet think of Romeo?” might retrieve a passage describing her inner turmoil or emotional reaction, even if the words “think” or “Romeo” aren’t explicitly present. Embedding-based retrieval improves relevance, flexibility, and ultimately the quality of the answers your language model can generate.\n\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\nimport torch\n\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu' # make sure you have GPU enabled in colab to speed things up!\nprint(f'device={device}')\n\nmodel = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1', device=device)\nembeddings = model.encode(chunks, device=device)\n\nprint(f\"Shape of embedding matrix: {np.array(embeddings).shape}\")\n\nNote: The shape of our embedding matrix is (283, 384) — representing the 283 chunks we prepared, and the 384 features describing each chunk. These are neural network derived features, lacking direct interpretability."
  },
  {
    "objectID": "Learn/Notebooks/2025-05-07_RAG-Romeo-Juliet.html#step-4-retrieve-relevant-chunks",
    "href": "Learn/Notebooks/2025-05-07_RAG-Romeo-Juliet.html#step-4-retrieve-relevant-chunks",
    "title": "Exploring Fact-Based QA with RAG: Romeo and Juliet",
    "section": "Step 4: Retrieve Relevant Chunks",
    "text": "Step 4: Retrieve Relevant Chunks\nIn this step, we demonstrate a core component of a RAG (Retrieval-Augmented Generation) pipeline — finding the most relevant pieces of text to answer a user’s question. Here’s how it works:\n\nWe take the user’s question and convert it into a vector embedding using the same model we used to embed the original text chunks.\nThen we use cosine similarity to compare the question’s embedding to all text chunk embeddings.\nWe select the top N most similar chunks to use as context for the language model.\n\n\nAre question embeddings and chunk embeddings really comparable?\nWe’re assuming that the embedding model (e.g., all-MiniLM-L6-v2) was trained in such a way that questions and answers occupy the same semantic space. That is, if a question and a passage are semantically aligned (e.g., about the same topic or fact), their embeddings should be close. This assumption holds reasonably well for general-purpose models trained on sentence pairs, but it’s not perfect — especially for very abstract or indirect questions. If a model was only trained to embed statements, it may not align questions correctly. You might retrieve chunks that are related but not directly useful for answering the question.\n\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ndef retrieve_relevant_chunks(model, query, chunks, embeddings, top_n=3):\n    query_embedding = model.encode([query],device=device)\n    scores = cosine_similarity(query_embedding, embeddings)[0]\n    top_indices = scores.argsort()[-top_n:][::-1]\n    results = [(chunks[i], scores[i]) for i in top_indices]\n    return results\n\n\nquestion = \"Who kills Mercutio?\" # Answer: Tybalt, Juliet's cousin\ntop_chunks = retrieve_relevant_chunks(model, question, chunks, embeddings)\n\nfor i, (chunk, score) in enumerate(top_chunks, 1):\n    print(f\"\\n\\n############ CHUNK {i} ############\")\n    print(f\"Score: {score:.4f}\")\n    print(chunk)\n\n\n\nSummary: Retrieval results for factual query\nThe following output shows how a RAG system handles the factual question “Who kills Mercutio?” using a chunked version of Romeo and Juliet. While no chunk explicitly states “Tybalt kills Mercutio” in modern phrasing, the system successfully retrieves highly relevant context. The Project Gutenberg edition uses the older spelling “Tibalt”, which the retriever still resolves semantically.\n\nChunk 1 is the most direct and useful. It captures the aftermath of the duel, with citizens exclaiming:\n\n“Which way ran he that kild Mercutio? Tibalt that Murtherer, which way ran he?”. Despite the archaic spelling and phrasing, this chunk effectively provides the answer when interpreted in context.\n\nChunk 2 sets up the conflict. It includes Mercutio and Benvolio discussing that:\n\n“Tibalt, the kinsman to old Capulet, hath sent a Letter” … “A challenge on my life”. While it doesn’t answer the question directly, it reinforces that Tibalt is the antagonist and establishes his role in escalating the violence.\n\nChunk 3 presents the Prince’s legal judgment:\n\n“Romeo, Prince, he was Mercutios Friend… The life of Tibalt.” The Prince confirms that Tybalt (Tibalt) has been killed in consequence of Mercutio’s death. This chunk emphasizes closure rather than causality, but still supports the factual chain.\n\n\n\n\nObservations\n\nEarly modern spelling (e.g., Tibalt) doesn’t hinder embedding-based retrieval — a strength of semantic models.\nNo chunk contains a complete “question + answer” sentence, but together they establish who killed whom, why, and what happened next.\nThe system retrieves scenes with narrative and legal resolution, not just the killing itself.\n\nThis result demonstrates how chunk-level RAG with sentence-transformer embeddings can surface relevant evidence across spelling and stylistic variation, even when chunk boundaries split key action and dialogue.\n\n\nRun a few additional queries & report top-ranked chunk\n\n# Run a few factual queries and inspect the top-ranked chunks\nfactual_questions = [\n    \"Who kills Mercutio?\", # Tybalt\n    \"Where does Romeo meet Juliet?\", # Capulet's masquerade ball (party), which takes place at the Capulet family home in Verona\n    \"What punishment does the Prince give Romeo?\" # exile / banishment\n]\n\nfor q in factual_questions:\n    print(f\"\\n=== Query: {q} ===\")\n    results = retrieve_relevant_chunks(model, q, chunks, embeddings, top_n=1)\n    for i, (chunk, score) in enumerate(results, 1):\n        print(f\"\\n--- CHUNK {i} (Score: {score:.4f}) ---\")\n        print(chunk[:800])  # print first ~800 chars for readability\n\n\n\nImproving retrieved chunks\nBefore we move on to having a language model generate answers, we need to take a closer look at the quality of the retrieved content.\nAs we just saw, our current retrieval method brings back passages that are topically related but often miss the actual moment where the answer appears. In some cases, the correct chunk is nearby but not retrieved. In others, key information may be split across multiple chunks or surrounded by distracting dialogue.\nTo address this, we’ll focus on a key area of improvement: refining the chunking strategy.\n\nWhy chunking matters\nThe current approach uses a simple method such as splitting the text by a fixed word count. While this works for general purposes, it often cuts across meaningful dramatic units:\n\nA character’s speech may be interrupted mid-line\nA fight scene may be split just before or after a critical action\nA conversation between characters may be split across chunks\n\nThis leads to less coherent retrieval and lowers the chance that a single chunk can fully answer the question.\nHere are two practical adjustments we can use to improve the retrievals:\n\nGroup complete speaker turns into chunks: Instead of arbitrary lengths, we can group text based on who is speaking. This ensures each chunk preserves the flow and tone of the conversation.\nUse scene- or event-aware chunking: By chunking based on scene boundaries or key events (e.g. “Romeo kills Tybalt”), we improve the chance that retrieved content captures complete dramatic moments, not just pieces of them.\n\nThese changes don’t require a new model—they just help the existing model work with more meaningful input.\nNext, we’ll apply dialogue-aware chunking and rerun one of our earlier factual queries to see whether the results improve.\n\n\n\nRefining chunking strategy\nOur current chunks are only based on word length. Instead, we can create chunks that are more tuned to the dataset and potential questions we might ask by defining a chunk as a “dialogue block”, i.e., as a group of N full speaker turns (e.g., JULIET. + her lines, ROMEO. + his lines, etc.).\nLet’s give this a shot to see how it impacts retrieval.\n\nimport re\n\ndef chunk_by_speaker_blocks(text, block_size=4):\n    # This regex matches short speaker tags at the beginning of lines, e.g., \"Ben.\" or \"Rom.\"\n    # Followed by speech text (either same line or indented on next)\n    speaker_line_pattern = re.compile(r'^\\s{0,3}([A-Z][a-z]+)\\.\\s+(.*)', re.MULTILINE)\n\n    dialogue_blocks = []\n    current_speaker = None\n    current_lines = []\n\n    for line in text.splitlines():\n        match = speaker_line_pattern.match(line)\n        if match:\n            # Save previous speaker block if one was accumulating\n            if current_speaker:\n                dialogue_blocks.append(f\"{current_speaker}.\\n\" + \"\\n\".join(current_lines).strip())\n            current_speaker = match.group(1)\n            current_lines = [match.group(2)]\n        elif current_speaker and line.strip():\n            # Indented continuation of the same speaker\n            current_lines.append(line)\n        else:\n            # Blank line or noise: treat as boundary\n            if current_speaker and current_lines:\n                dialogue_blocks.append(f\"{current_speaker}.\\n\" + \"\\n\".join(current_lines).strip())\n                current_speaker = None\n                current_lines = []\n\n    # Add last block if exists\n    if current_speaker and current_lines:\n        dialogue_blocks.append(f\"{current_speaker}.\\n\" + \"\\n\".join(current_lines).strip())\n\n    # Chunk into groups of speaker turns\n    grouped_chunks = []\n    for i in range(0, len(dialogue_blocks), block_size):\n        chunk = \"\\n\\n\".join(dialogue_blocks[i:i + block_size])\n        grouped_chunks.append(chunk.strip())\n\n    return grouped_chunks\n\n\nspeaker_chunks = chunk_by_speaker_blocks(file_contents, block_size=4)\nprint(f\"Total speaker_chunks: {len(speaker_chunks)}\")\nprint(f\"Preview of first chunk:\\n\\n{speaker_chunks[0]}\")\n\nOur chunks have now been improved so that we aren’t cutting off any diagloue mid-sentence, and each chunk contains a few turns between speakers – allowing us to better capture the overall semantics of short passages from Romeo and Juliet.\n\ndialogue_embeddings = model.encode(speaker_chunks, device=device)\n\nprint(f\"Shape of dialogue_embeddings matrix: {np.array(dialogue_embeddings).shape}\")\n\n\n# Run a few factual queries and inspect the top-ranked chunks\nfactual_questions = [\n    \"Who kills Mercutio?\", # Tybalt\n    \"Where does Romeo meet Juliet?\", # Capulet's masquerade ball (party), which takes place at the Capulet family home in Verona\n    \"What punishment does the Prince give Romeo?\" # exile / banishment\n]\n\nfor q in factual_questions:\n    print(f\"\\n=== Query: {q} ===\")\n    results = retrieve_relevant_chunks(model, q, speaker_chunks, dialogue_embeddings, top_n=1)\n    for i, (chunk, score) in enumerate(results, 1):\n        print(f\"\\n--- CHUNK {i} (Score: {score:.4f}) ---\")\n        print(chunk)  # print first ~800 chars for readability\n\n\n\nTakeaway\nRefining our chunking strategy to preserve full speaker turns—and grouping several turns together—has already improved the relevance of the chunks retrieved. The content is more coherent, more complete, and better aligned with the structure of a play. This shows how much retrieval quality depends not just on the model, but on the way we prepare and represent the source material.\nThat said, even with better chunks, retrieval doesn’t always land on the exact moment that answers the question. Sometimes it gets close but stops short; other times it picks up a scene with similar characters or themes, but not the one we need.\nThis points to a deeper challenge: semantic similarity alone doesn’t always capture answer relevance. The chunk that’s closest in meaning isn’t always the one that answers the question. One way to address this is through a process called reranking.\n\n\nWhat is reranking?\nReranking means retrieving a small set of candidate chunks—say, the top 5—and then using an additional method to determine which of those is the best fit for the question.\nThat method could be:\n\nA custom scoring function (e.g., based on keyword overlap, speaker identity, or chunk metadata),\nOr—more powerfully—a separate language model.\n\nThis separate model can be small or large, depending on your resource availability:\n\nA smaller open-source model (like mistral, falcon, or phi) can often handle basic ranking tasks at low cost.\nA larger LLM (like GPT-3.5 or GPT-4) may be better at reasoning through subtleties and weighing relevance when answers are indirect or distributed across lines.\n\nYou might ask this model something like:\n\nHere are three passages. Which one best answers the question: “Who kills Mercutio?”\n\nAt first, it might feel strange to use one language model to support another—but this layered setup is common in production RAG pipelines. It separates concerns:\n\nThe retriever quickly narrows down the universe of text,\nThe reranker evaluates those chunks more deeply, focusing on which is most likely to be useful.\n\nWe won’t implement this yet, but it’s worth introducing now. As we start exploring more ambiguous or emotionally driven questions in later sections, reranking becomes one of the key techniques for bridging the gap between retrieval and meaningful response.\nFor now, we’ve established a strong foundation: well-structured chunks that carry clear speaker information and preserve narrative flow. That’s a critical step toward building a RAG system that doesn’t just respond, but interprets.\n\n\nUpgrading our retrieval model\nThe model we’ve used so far, multi-qa-MiniLM-L6-cos-v1, is a solid starting point for retrieval-augmented generation (RAG) pipelines, it is relatively lightweight (22M parameters, ~500–800 MB GPU memory), which makes it efficient but less expressive than larger models.\nHowever, larger embedding models have more capacity to capture subtle semantic relationships, including indirect phrasing or domain-specific language. This can make a dramatic difference in tasks like matching Shakespearean dialogue to modern questions—something smaller models often struggle with.\nLet’s try a slightly larger model with 109 M parameters, all-mpnet-base-v2\n\nfrom sentence_transformers import SentenceTransformer\n\n# Load the dot-product version of the same model\nmodel_larger = SentenceTransformer('all-mpnet-base-v2', device=device) # larger model\n\n# Generate embeddings for all chunks\ndialogue_embeddings = model_larger.encode(speaker_chunks, device=device)\n\n\n# Run a few factual queries and inspect the top-ranked chunks\nfactual_questions = [\n    \"Who kills Mercutio?\", # Tybalt\n    \"Where does Romeo meet Juliet?\", # Capulet's masquerade ball (party), which takes place at the Capulet family home in Verona\n    \"What punishment does the Prince give Romeo?\" # exile / banishment\n]\n\nfor q in factual_questions:\n    print(f\"\\n=== Query: {q} ===\")\n    results = retrieve_relevant_chunks(model_larger, q, speaker_chunks, dialogue_embeddings, top_n=1)\n    for i, (chunk, score) in enumerate(results, 1):\n        print(f\"\\n--- CHUNK {i} (Score: {score:.4f}) ---\")\n        print(chunk)  # print first ~800 chars for readability\n\nIf you’re interested in exploring more powerful options for RAG pipelines, consider:\n\nintfloat/e5-large-v2: A 24‑layer encoder (335M params) fine-tuned for dense retrieval with query: / passage: formatting.\nBAAI/bge-large-en-v1.5: A high-performing English retriever (335M params) that tops MTEB benchmarks.\ndeepseek-ai/DeepSeek-V2: A large-scale mixture-of-experts model (236 B params) pioneering efficient retrieval architectures, but note it’s not a small encoder model—it’s listed here to showcase advanced retrieval methods.\n\nAll of these are trained for dot-product similarity and work best with a high-performance index like faiss.IndexFlatIP.\nNote: We didn’t use FAISS in this notebook, since our dataset is small enough for brute-force similarity search. But once you move to larger models or bigger corpora, FAISS becomes essential for scalable and efficient retrieval."
  },
  {
    "objectID": "Learn/Notebooks/2025-05-07_RAG-Romeo-Juliet.html#step-5-generate-answer-using-retrieved-context",
    "href": "Learn/Notebooks/2025-05-07_RAG-Romeo-Juliet.html#step-5-generate-answer-using-retrieved-context",
    "title": "Exploring Fact-Based QA with RAG: Romeo and Juliet",
    "section": "Step 5: Generate answer using retrieved context",
    "text": "Step 5: Generate answer using retrieved context\n\nPutting it all together: Answering a question with a language model\nNow that we’ve improved our chunking and retrieval process, we’re ready to pass the retrieved content to yet another language model and generate an answer.\nThis step completes the typical RAG (Retrieval-Augmented Generation) workflow:\n\nRetrieve the top-ranked passage(s) using a retrieval language model to embed the corpus into a Q&A semantic space\nConcatenate retrieved results them into a structured prompt\nAsk a (generative) language model to answer the user’s question using only that retrieved context\n\nThis approach grounds the model’s answer in specific evidence from the text, making it more trustworthy than asking the model to “hallucinate” an answer from general pretraining.\n\nThe prompt format\nWe use a basic prompt like this:\nUse only the following passage to answer this question.\nBEGIN_PASSAGE: [Top retrieved chunk(s) go here] END_PASSAGE \nQUESTION: [your question]\nANSWER:\nBy framing the input this way, we signal to the model that it should focus only on the retrieved content. We’re not asking it to draw from general knowledge of the play—just from the selected passages.\nLet’s begin assembling the full prompt:\n\nquestion = \"Who killed Mercutio?\" # Tybalt/Tibalt\n\n\ntop_dialgoue_chunks = retrieve_relevant_chunks(model_larger, question, speaker_chunks, dialogue_embeddings, top_n=3)\n\n# Extract only the chunk text from (chunk, score) tuples\ncontext = \"\\n\".join(chunk for chunk, score in top_dialgoue_chunks)\nprint(context)\n\n\nprompt = f\"Use the following passage to answer this question.\\nBEGIN_PASSAGE:\\n{context}\\nEND_PASSAGE\\nQUESTION: {question}\\nANSWER:\"\nprint(prompt)\n\n\n\n\nLanguage model for generation\nFor this section, we’re using tiiuae/falcon-rw-1b, a small 1.3B parameter decoder-only model trained on the RefinedWeb dataset. It’s designed for general-purpose text continuation, not for answering questions or following instructions.\nThis makes it a good baseline for testing how much a generative model can do with only retrieved context and minimal guidance. As we’ll see, its output often reflects surface-level patterns or recent tokens, rather than accurate reasoning grounded in the text.\n\nfrom transformers import pipeline\n\nllm = pipeline(\"text-generation\", model=\"tiiuae/falcon-rw-1b\", device_map=\"auto\")\n\n\nModel parameters and generation behavior\nWhen we call the language model, we specify parameters like:\n\nmax_new_tokens: Limits how much it can generate (e.g., 100 tokens)\ndo_sample=True: Enables creative variation rather than deterministic output. For the purposes of getting a reproducible result, we’ll set this to False\n\nThese parameters influence not just length, but also how literal or speculative the answer might be. Sampling increases variety but can also introduce tangents or continuation artifacts.\n\nresult = llm(prompt, max_new_tokens=10, do_sample=False)[0][\"generated_text\"]\n\n\nprint(result)\n\n\n\n\nWhy the model output inludes the prompt\nWhen using a decoder-only language model (like Falcon or GPT) with the Hugging Face pipeline(\"text-generation\"), the output will include the entire input prompt followed by the model’s generated continuation.\nThis happens because decoder-only models are trained to predict the next token given all previous tokens, not to separate a prompt from a response. So when you pass in a prompt, the model simply continues generating text — it doesn’t know where “input” ends and “output” begins.\nAs a result, the pipeline will return a string that contains both:\n[prompt] + [generated text]\nIf you’re only interested in the generated part (e.g., the model’s answer), you’ll need to remove the prompt manually after generation.\nWe can strip off the final answer / generated result with the next code cell.\n\ngenerated_answer = result[len(prompt):].strip()\nprint(generated_answer)\n\n\nWhy the output might drift or repeat\nEven though we ask just one question, you might see the model:\n\nAnswer multiple questions in a row\nInvent follow-up questions and answers\nContinue in a Q&A or list format beyond what was asked\n\nThis usually happens when:\n\nThe passage is long or covers multiple narrative beats\nThe model detects a repeated pattern (e.g., “Question: … Answer: …”) and keeps going\n\nFor example, with a passage that includes both a fight and a romantic scene, the model might output:\nQuestion: Who kills Mercutio?\nAnswer: Romeo.\nQuestion: What does Juliet say about fate?\nAnswer: She curses fortune.\nEven though we only asked the first question.\nTo limit this behavior, you can:\n\nSet a lower max_new_tokens\nAdd a stop sequence after the first answer (if supported)\nUse a tighter or more explicit prompt style\n\n\nresult = llm(prompt, max_new_tokens=1, do_sample=False)[0][\"generated_text\"] # adjust to inlcude max of 1 new tokens\ngenerated_answer = result[len(prompt):].strip()\nprint(generated_answer)\n\n\n\n\nNote on model accuracy and hallucination\nSmaller decoder-only models like tiiuae/falcon-rw-1b are fast and lightweight, but they can make factual errors, especially when summarizing events from structured texts like plays or historical records. For example, when asked “Who killed Mercutio?”, the model incorrectly responded:\n\"Romeo\"\nThis is not correct. Mercutio is killed by Tybalt during a street duel. Romeo kills Tybalt afterward in retaliation.\nInterestingly, the correct information was present in the top retrieved chunk, but the phrasing may have confused the model:\n\nMer.\nI am hurt.\nA plague a both the Houses, I am sped:\nIs he gone and hath nothing?\n\n\nBen.\nWhat art thou hurt?\n\n\nPrin.\nRomeo slew him, he slew Mercutio,\nWho now the price of his deare blood doth owe\n\n\nCap.\nNot Romeo Prince, he was Mercutio’s Friend,\nHis fault concludes, but what the law should end,\nThe life of Tybalt\n\n\nInstruction tuning improves perfomance\nTo improve factual accuracy in your RAG pipeline, it’s helpful to use an instruction-tuned model rather than a base language model. You’ve been using falcon-rw-1b (where “rw” stands for “Refined Web”), which is trained only to continue text — not to follow specific question-and-answer instructions. That’s why it often hallucinates factual events.\nA lightweight upgrade is to instead use tiiuae/Falcon3-1B-Instruct, an instruction-tuned version of Falcon. It still runs on modest hardware but is trained to follow prompts and answer questions in a focused way.\n\nfrom transformers import pipeline\n\nllm = pipeline(\n    \"text-generation\",\n    model=\"tiiuae/falcon3-1b-instruct\",\n    device_map=\"auto\",\n    torch_dtype=\"auto\",  # optional, helps with GPU memory\n)\n\n\n# NOTE: We use max_new_tokens=3 here because words like \"Tybalt\" may be split into multiple tokens (e.g., \"Ty\", \"b\", \"alt\").\n# It's often tricky to get exactly one word due to subword tokenization.\nresult = llm(prompt, max_new_tokens=3, do_sample=False)[0][\"generated_text\"]\n\n# extract answer from full result, as before\ngenerated_answer = result[len(prompt):].strip()\nprint(generated_answer)\n\nIf all else fails, we can start to try larger models to handle the answer generation step. Other models you could substitute here depending on your resources include:\n\nmistralai/Mistral-7B-Instruct-v0.1 — for stronger instruction-following\nmeta-llama/Meta-Llama-3-8B-Instruct — for more fluent answers\nopenai/gpt-3.5-turbo — via API (not Hugging Face)\n\nFor most open-source models, using transformers + pipeline() allows easy swapping once your retrieval system is set up.\nKeep in mind:\n\nLarger models require more memory (ideally a 12–16GB GPU)\nInstruction-tuned models typically follow prompts more reliably than base models\nYou may still need to post-process outputs to extract just the answer\n\nIf you’re working in Colab, consider using quantized models (e.g., via bitsandbytes) or calling the model via Hugging Face’s hosted Inference API."
  },
  {
    "objectID": "Learn/Notebooks/2025-05-07_RAG-Romeo-Juliet.html#concluding-remarks",
    "href": "Learn/Notebooks/2025-05-07_RAG-Romeo-Juliet.html#concluding-remarks",
    "title": "Exploring Fact-Based QA with RAG: Romeo and Juliet",
    "section": "Concluding remarks",
    "text": "Concluding remarks\nThis notebook introduced a basic Retrieval-Augmented Generation (RAG) pipeline for factual question answering using Romeo and Juliet. The goal was to build a simple but functioning system and surface practical lessons about how to improve performance.\nFor retrieval, we explored and discussed improvements such as:\n\nUsing stronger embedding models (e.g., upgrading from MiniLM to all-mpnet-base-v2).\nAdopting a question-aligned chunking strategy, where chunks were grouped by speaker turns to better match the structure of expected queries.\nImplementing cosine similarity retrieval, which better handles variation in chunk lengths and embedding magnitudes.\nBriefly mentioning reranking as a next step, though not yet implemented.\n\nFor generation, we found that:\n\nInstruction-tuned language models yield more precise and context-sensitive answers.\nPrompt formatting significantly affects the clarity and relevance of the generated output.\nPost-processing may be necessary for trimming or cleaning model responses, especially in short-form QA tasks.\n\nWhile larger models consistently improve both retrieval and generation, thoughtful design choices—such as aligning chunk structure to question types, using the right embedding normalization, and writing effective prompts—can yield substantial gains, even in smaller pipelines.\nThis notebook serves as a first step in a broader RAG workflow. Future notebooks will experiment with more flexible chunking, incorporate reranking, and test the system’s ability to handle interpretive or subjective questions."
  },
  {
    "objectID": "Learn/Notebooks/Titanic-Dataset.html#step-0-looking-up-each-feature",
    "href": "Learn/Notebooks/Titanic-Dataset.html#step-0-looking-up-each-feature",
    "title": "Exploring the Titanic Dataset",
    "section": "Step 0: Looking Up Each Feature",
    "text": "Step 0: Looking Up Each Feature\nBefore diving into the analysis, it’s important to understand what each feature in the dataset represents. This ensures we’re interpreting the data correctly and allows us to make informed decisions during the analysis.\n\nTitanic Dataset Features:\n\nsurvived: Whether the passenger survived (0 = No, 1 = Yes).\npclass: Ticket class (1 = 1st, 2 = 2nd, 3 = 3rd).\nsex: Gender of the passenger.\nage: Age of the passenger in years. Some values are missing.\nsibsp: Number of siblings/spouses aboard the Titanic.\nparch: Number of parents/children aboard the Titanic.\nfare: Passenger fare.\nembarked: Port of embarkation (C = Cherbourg; Q = Queenstown; S = Southampton).\nclass: Duplicate of ‘pclass’ (used for plotting by Seaborn).\nwho: Describes whether the passenger is a man, woman, or child.\nadult_male: Indicates whether the passenger is an adult male (True/False).\ndeck: The deck the passenger was on (missing for many passengers).\nembark_town: The name of the town where the passenger boarded.\nalive: Indicator of whether the passenger survived (Yes/No, derived from ‘survived’).\nalone: Indicates whether the passenger was traveling alone (True/False)."
  },
  {
    "objectID": "Learn/Notebooks/Titanic-Dataset.html#step-1-visualizing-data-in-its-rawest-form",
    "href": "Learn/Notebooks/Titanic-Dataset.html#step-1-visualizing-data-in-its-rawest-form",
    "title": "Exploring the Titanic Dataset",
    "section": "Step 1: Visualizing data in its rawest form",
    "text": "Step 1: Visualizing data in its rawest form\nLet’s take a look at a small sample of the dataset to understand the raw data we’re working with. This gives us a chance to spot obvious issues or patterns.\n\nImport libraries and load dataset\n\n# Import necessary libraries\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load the Titanic dataset from seaborn\ndf = sns.load_dataset('titanic')\n\n# Display the first few rows of the dataset\ndf.head()\n\n\n# Show a small random sample of the data\nprint(\"Sample of 30 passengers:\")\ndf.sample(30)\n\n\n\nInsights\n\nWe see various features such as age, sex, class, fare, and whether the passenger survived.\nThis helps us get a quick overview of what kind of data we’re working with, including potential issues like missing values.\nAppears we have some redundant columns.\nSome NaNs clearly visible in deck and age.\n\n\nRemove redundant columns\nIt looks like alive and survived are identical. Same for embarked and emback_town. Let’s run a check to see if these columns are truly identical and remove them, if so.\n\n# Check if the two columns are identical\nare_identical = df['survived'].equals(df['alive'].apply(lambda x: 1 if x == 'yes' else 0))\n\n# Print the result\nprint(f\"Are 'survived' and 'alive' identical? {are_identical}\")\n\n\n# Check unique values in both columns\nprint(\"Unique values in 'embarked':\", df['embarked'].unique())\nprint(\"Unique values in 'embark_town':\", df['embark_town'].unique())\n\n# Map 'embarked' codes to 'embark_town' names\nembarked_mapping = {'S': 'Southampton', 'C': 'Cherbourg', 'Q': 'Queenstown'}\n\n# Apply the mapping to the 'embarked' column\ndf['embarked_mapped'] = df['embarked'].map(embarked_mapping)\n\n# Check if the mapped 'embarked' column is identical to 'embark_town'\nare_identical = df['embarked_mapped'].equals(df['embark_town'])\n\n# Print the result\nprint(f\"Are 'embarked' and 'embark_town' identical? {are_identical}\")\n\n\ndf.drop('alive',axis=1,inplace=True)\ndf.drop('embark_town',axis=1,inplace=True)\ndf.drop('embarked_mapped',axis=1,inplace=True)\ndf.drop('class',axis=1,inplace=True) #explicitly equiv. from documentation"
  },
  {
    "objectID": "Learn/Notebooks/Titanic-Dataset.html#step-2-check-data-types",
    "href": "Learn/Notebooks/Titanic-Dataset.html#step-2-check-data-types",
    "title": "Exploring the Titanic Dataset",
    "section": "Step 2: Check data types",
    "text": "Step 2: Check data types\n\ndf.dtypes"
  },
  {
    "objectID": "Learn/Notebooks/Titanic-Dataset.html#step-3-basic-statistics",
    "href": "Learn/Notebooks/Titanic-Dataset.html#step-3-basic-statistics",
    "title": "Exploring the Titanic Dataset",
    "section": "Step 3: Basic Statistics",
    "text": "Step 3: Basic Statistics\nNow we’ll summarize the numerical and categorical columns to better understand the central tendencies, variability, and potential missing data.\nWhen exploring numerical data:\n\nMean vs Median: Compare the mean and median. If the mean is much higher or lower than the median, this suggests skewness in the data, possibly due to outliers.\nMin and Max: Look at the minimum and maximum values to detect extreme outliers or potential data entry errors.\nStandard deviation: A high standard deviation indicates that the data points are spread out over a wide range of values, while a low standard deviation suggests that data points are clustered around the mean.\n\nWhen exploring categorical data:\n\nUnique counts: Check the number of unique categories. For example, the ‘sex’ column has two unique categories (‘male’ and ‘female’), while ‘pclass’ has three.\nMode: The mode (most frequent category) helps us understand which category dominates the dataset. For example, if most passengers are male or most belong to a particular class, this would inform our analysis.\nFrequency distribution: Look at the frequency of each category to identify potential imbalances. For example, are there more passengers from a particular class or embarkation point? Imbalanced categories can bias the model if not handled properly.\nMissing or rare categories: If a category has very few occurrences, this might suggest noise or anomalies in the data.\n\n\n# Summarize the numerical and categorical columns\nprint(\"\\nBasic statistics for numerical columns:\")\ndf.describe(include='all') # include='all' ensures both numeric and binary features are described\n\n\nInsights\n\nAge: The mean age is around 30, but with missing values and some extreme values (min = 0.42, max = 80). The mean and median are close, suggesting a fairly symmetric distribution for most passengers.\nFare: The fare has a wide range, from 0 to 512. This large range and the difference between mean and median (mean &gt; median) suggest the presence of outliers, with some passengers paying much higher fares.\nParch and SibSp: Most passengers had few or no relatives onboard, with a median of 0 for both columns.\nPclass: Most passengers are in 3rd class (mode: 3).\n\nThe df.describe(include=‘object’) part of the code is used to generate summary statistics specifically for categorical (or object) columns in a DataFrame.\n\nprint(\"\\nBasic statistics for categorical columns:\")\ndf.describe(include='object')\n\n\n\nInsights\n\nA significant portion of passengers did not survive (survival rate is less than 50%).\nThere are more males than females in this data, with adult males being the most common (vs boys)\nSouthampton (S) is the most common embarkation point (out of three options)\n\nWhile this summary provides useful information, it does not reveal the distribution of all categories, especially the rare categories. For instance, it only shows the most frequent category (the mode) and its frequency, but it doesn’t show how the other categories are distributed, especially if there are categories with very few occurrences."
  },
  {
    "objectID": "Learn/Notebooks/Titanic-Dataset.html#step-2.1-identifying-rare-categories",
    "href": "Learn/Notebooks/Titanic-Dataset.html#step-2.1-identifying-rare-categories",
    "title": "Exploring the Titanic Dataset",
    "section": "Step 2.1: Identifying Rare Categories",
    "text": "Step 2.1: Identifying Rare Categories\nTo detect rare categories in the dataset, we will examine the frequency distribution of each categorical column. This will help us understand if there are categories with very few occurrences, which could either be noise or anomalies.\nRare categories are important to identify because they can introduce bias or affect model performance if not handled properly.\n\n# Frequency distribution for categorical columns\ncategorical_cols = df.select_dtypes(include='object').columns\n\nfor col in categorical_cols:\n    print(f\"Value counts for {col}:\")\n    print(df[col].value_counts())\n    print(\"\\n\")\n\n\nInsights\n\nSex: No rare categories, with a fairly even distribution between males and females.\nPclass: Most passengers are in 3rd class, but no rare categories.\nEmbarked: The ‘C’ and ‘Q’ embarkation points are far less common than ‘S’ (Southampton)."
  },
  {
    "objectID": "Learn/Notebooks/Titanic-Dataset.html#step-3-counting-missing-values-nans",
    "href": "Learn/Notebooks/Titanic-Dataset.html#step-3-counting-missing-values-nans",
    "title": "Exploring the Titanic Dataset",
    "section": "Step 3: Counting Missing Values (NaNs)",
    "text": "Step 3: Counting Missing Values (NaNs)\nTo get a better understanding of where data is missing, we’ll count the number of NaN values in each column. This is important for understanding which features will need imputation or may need to be excluded from analysis.\n\n# Count the number of NaN values in each column\nprint(\"Number of NaNs per column:\")\nprint(df.isna().sum())"
  },
  {
    "objectID": "Learn/Notebooks/Titanic-Dataset.html#step-3.1-visualizing-missing-data",
    "href": "Learn/Notebooks/Titanic-Dataset.html#step-3.1-visualizing-missing-data",
    "title": "Exploring the Titanic Dataset",
    "section": "Step 3.1: Visualizing Missing Data",
    "text": "Step 3.1: Visualizing Missing Data\nVisualizing missing data helps us understand how much data is missing and where. This informs how we should handle missing values during preprocessing.\nThe ‘age’ column has many missing values, which could affect our analysis, especially when trying to assess survival rates across different age groups. One potential approach to deal with missing ‘age’ values is to use the ‘who’ feature, which categorizes passengers as men, women, or children. By looking at the distribution of age within each ‘who’ group, we could potentially im\n\nimport missingno as msno\n\n# Visualize missing data using missingno\nmsno.matrix(df)\nplt.show()\n\nmsno.bar(df)\nplt.show()\n\n\nInsights\n\nThe ‘age’ and ‘deck’ columns have a significant number of missing values.\n‘embarked’ has a couple of missing values\nWe’ll need to decide how to handle these NaNs before proceeding with modeling. The ‘deck’ column may have too many missing values to be useful without significant imputation.\n\n\n# remove deck column (not enough info here)\ndf.drop(['deck'],axis=1,inplace=True)\n\n\n# Let's explore the relationship between 'who' and 'age' to see if we can use 'who' for imputing missing ages\nsns.boxplot(x='who', y='age', data=df)\nplt.title(\"Age Distribution by Who (Men, Women, Children)\")\nplt.show()\n\n\n\nInsights\n\nThe ‘age’ column has many missing values, and visualizing the missing data shows that age is a significant feature with gaps. We need to address this issue to avoid biasing the model.\nBy plotting ‘age’ against ‘who’, we see distinct distributions: children tend to be younger, while men and women have overlapping but distinct age ranges.\nA potential strategy is to impute missing age values based on the ‘who’ category, filling in likely ages for children, women, and men based on these distributions.\nThis could provide a more accurate imputation than using the overall mean or median, especially in the case of children who are expected to have significantly lower ages.\n\n\n\nRemove NaNs\nImputing age could be a good stratgey here. For simplicity, we will just remove the rows where age has any NaNs, but keep in mind that this effectively tosses out ~20% of the data/information we have. In a real-world scenario, imputing is worth testing out.\n\n# Drop all rows containing NaN values\nprint(df.shape)\ndf_clean = df.dropna()\ndf_clean.shape\n\n\n# Count the number of NaN values in each column\nprint(\"Number of NaNs per column:\")\nprint(df_clean.isna().sum())"
  },
  {
    "objectID": "Learn/Notebooks/Titanic-Dataset.html#step-4-identifying-outliers-across-multiple-features",
    "href": "Learn/Notebooks/Titanic-Dataset.html#step-4-identifying-outliers-across-multiple-features",
    "title": "Exploring the Titanic Dataset",
    "section": "Step 4: Identifying Outliers (Across Multiple Features)",
    "text": "Step 4: Identifying Outliers (Across Multiple Features)\nOutliers can distort model performance and influence the relationships between features. We will use boxplots to identify outliers across multiple numerical columns, including ‘age’, ‘fare’, ‘sibsp’, and ‘parch’.\n\nWhy Look for Outliers?\n\nAge: Extreme values (e.g., very young or old passengers) might influence survival predictions.\nFare: We’ve already identified skewness in the fare data, and high fares could represent wealthy individuals who had better chances of survival.\n\n\nimport seaborn as sns\n# Create boxplots for multiple numerical features to check for outliers\nnumerical_cols = ['age', 'fare']\n\nfor col in numerical_cols:\n    sns.boxplot(x=df_clean[col])\n    plt.title(f\"Boxplot of {col.capitalize()}\")\n    plt.show()\n\n\n\nInsights\n\nAge: Most passengers fall within a reasonable range, but there are a few extreme values for older passengers. These could be outliers that might need special attention during modeling.\nFare: The boxplot confirms the presence of outliers, with several passengers paying significantly more than the majority."
  },
  {
    "objectID": "Learn/Notebooks/Titanic-Dataset.html#step-5.1-probability-density-plot-for-fare",
    "href": "Learn/Notebooks/Titanic-Dataset.html#step-5.1-probability-density-plot-for-fare",
    "title": "Exploring the Titanic Dataset",
    "section": "Step 5.1: Probability Density Plot for Fare",
    "text": "Step 5.1: Probability Density Plot for Fare\nIn addition to the boxplot for identifying outliers, we can draw a probability density plot (PDF) to visualize the overall distribution of the ‘fare’ column.\nThis plot will show the likelihood of different fare values occurring, highlighting any skewness or concentration of values in certain ranges.\n\n# Plot the probability density plot (PDF) for fare\nsns.kdeplot(df_clean['fare'].dropna(), fill=True)\nplt.title(\"Probability Density Plot for Fare\")\nplt.xlabel(\"Fare\")\nplt.ylabel(\"Density\")\nplt.show()\n\n\nInsights\n\nThe PDF for the fare column shows a strong right skew, with most passengers paying lower fares, but a few passengers paying significantly higher fares.\nThis confirms the presence of outliers at the higher end, which we also observed in the boxplot.\nThe density plot helps visualize how fares are concentrated in lower ranges and taper off gradually toward the higher end."
  },
  {
    "objectID": "Learn/Notebooks/Titanic-Dataset.html#step-5.2-log-scaling-for-fare",
    "href": "Learn/Notebooks/Titanic-Dataset.html#step-5.2-log-scaling-for-fare",
    "title": "Exploring the Titanic Dataset",
    "section": "Step 5.2: Log Scaling for Fare",
    "text": "Step 5.2: Log Scaling for Fare\nTo deal with the strong skewness and outliers in the ‘fare’ column, we can apply a log transformation. This will compress the range of the fare values, reducing the influence of extreme outliers while keeping the relative differences intact.\nLog scaling is particularly useful for highly skewed distributions, making them more normal-like and easier for models to handle.\n\nimport numpy as np\n# Apply log scaling to the fare column (adding 1 to avoid log(0))\ndf_clean.loc[:, 'log_fare'] = df_clean['fare'].apply(lambda x: np.log(x + 1))\n\n# Plot the PDF for the log-transformed fare column\nsns.kdeplot(df_clean['log_fare'].dropna(), fill=True)\nplt.title(\"Probability Density Plot for Log-Scaled Fare\")\nplt.xlabel(\"Log(Fare + 1)\")\nplt.ylabel(\"Density\")\nplt.show()"
  },
  {
    "objectID": "Learn/Notebooks/Titanic-Dataset.html#step-6-exploring-correlations",
    "href": "Learn/Notebooks/Titanic-Dataset.html#step-6-exploring-correlations",
    "title": "Exploring the Titanic Dataset",
    "section": "Step 6: Exploring Correlations",
    "text": "Step 6: Exploring Correlations\nNext, we’ll check for correlations between numerical features. This helps us see whether some features are strongly related and could introduce multicollinearity.\nFor example: - Correlations close to 1 or -1 indicate a strong relationship between features. - Correlations close to 0 indicate little to no linear relationship between features."
  },
  {
    "objectID": "Learn/Notebooks/Titanic-Dataset.html#step-6.1-encode-categoriecal-data-as-numeric",
    "href": "Learn/Notebooks/Titanic-Dataset.html#step-6.1-encode-categoriecal-data-as-numeric",
    "title": "Exploring the Titanic Dataset",
    "section": "Step 6.1: Encode categoriecal data as numeric",
    "text": "Step 6.1: Encode categoriecal data as numeric\nEncoding the categorical data will allow us to measure correlations across different levels of our categorical variables. Encoded data is also needed for the modeling step. After encoding, you may want to visit some of the previous steps in this notebook to ensure there aren’t any problems with the encoded version of the data. Some people like to encode right after loading their data, but this can make the data unnecessarily complicated while we do some basic browsing of the data (e.g., check for redundnat columns, check for NaNs, check data types, etc.)\nCode explanation:\n\npd.get_dummies(df, drop_first=True): This one-hot encodes all categorical columns, converting them into binary columns. The drop_first=True argument prevents multicollinearity by removing one of the categories in each column (since they are mutually exclusive).\n\n\n# One-hot encode the categorical columns in the dataset\ndf_encoded = pd.get_dummies(df, drop_first=True)\ndf_encoded.head()\n\nCode explanation.\n\ncorr_matrix_encoded = df_encoded.corr(): This computes the correlation matrix for both the numerical and newly one-hot encoded features.\nSeaborn heatmap: The heatmap will visualize correlations across all features, both numerical and categorical (now encoded).\n\n\n# Calculate the correlation matrix for all features (including one-hot encoded)\ncorr_matrix_encoded = df_encoded.corr()\n\n# Plot the heatmap of the correlation matrix\nplt.figure(figsize=(16, 12))\nsns.heatmap(corr_matrix_encoded, annot=False, cmap='coolwarm')\nplt.title(\"Correlation Matrix (Numerical and One-Hot Encoded Features)\")\nplt.show()\ncorr_matrix_encoded\n\n\nInsights\n\nThere is a strong correlation between ‘fare’ and ‘pclass’, which makes sense since higher-class tickets typically have higher fares.\n‘SibSp’ and ‘Parch’ have a weak positive correlation, indicating that larger families might be traveling together.\nThere aren’t many strong correlations with ‘survived’, suggesting that more advanced feature engineering might be needed to improve model performance.\nwho_man and adult_male are 100% correlated. We can remove one of these columns.\n\nPro-tip: The variance-inflation factor score can also be very helpful for assessing correlation. This measure looks at how well you can predict a given predictor (y) using all other predictors (X) as input variables to a linear regression model. The nice thing about it is that it gives you a different score for each predictor, which can be helpful when deciding which problematic features to remove.\n\ndf_encoded.drop('who_man',axis=1,inplace=True)"
  },
  {
    "objectID": "Learn/Notebooks/Titanic-Dataset.html#step-6.1-pairplot-for-visualizing-pairwise-relationships",
    "href": "Learn/Notebooks/Titanic-Dataset.html#step-6.1-pairplot-for-visualizing-pairwise-relationships",
    "title": "Exploring the Titanic Dataset",
    "section": "Step 6.1: Pairplot for Visualizing Pairwise Relationships",
    "text": "Step 6.1: Pairplot for Visualizing Pairwise Relationships\nWe’ll use Seaborn’s pairplot to visualize pairwise relationships between the numerical features, colored by whether the passenger survived.\nThis can help us identify any patterns or clusters that may inform our modeling decisions.\n\n# Pairplot to explore relationships between features, colored by 'survived'\nsns.pairplot(df_encoded, hue='survived', diag_kind='kde')\nplt.show()\n\n\nInsights\n\nThe pairplot shows some separation between survivors and non-survivors for certain features, such as ‘pclass’ and ‘adult_male’\nThe pairwise relationships between ‘age’, ‘fare’, and other numerical features are not perfectly linear, suggesting that non-linear models might perform better.\nVisualizing these relationships helps in identifying where additional feature engineering may be required to boost model performance."
  },
  {
    "objectID": "Learn/Notebooks/Titanic-Dataset.html#conclusion-next-steps-for-modeling-and-iterative-eda",
    "href": "Learn/Notebooks/Titanic-Dataset.html#conclusion-next-steps-for-modeling-and-iterative-eda",
    "title": "Exploring the Titanic Dataset",
    "section": "Conclusion: Next Steps for Modeling and Iterative EDA",
    "text": "Conclusion: Next Steps for Modeling and Iterative EDA\nNow that we’ve explored the Titanic dataset through extensive EDA, we’ve gained valuable insights that can guide our next steps in the modeling process. However, it’s important to remember that EDA is not a one-time process—it’s iterative and should continue as new patterns or issues arise during modeling.\n\nKey Takeaways:\n\nFeature Engineering:\n\nWe’ve identified that the ‘fare’ and ‘pclass’ columns are strongly correlated, suggesting that we might combine or transform these features for better model performance.\nLog scaling ‘fare’ has helped reduce skewness, making this feature more suitable for modeling. We can apply similar transformations to other skewed features as necessary.\nFeatures like ‘who’ and ‘age’ might benefit from imputation or interaction terms to capture deeper relationships with survival outcomes.\n\nHandling Missing Data:\n\n‘Age’ and ‘deck’ have substantial missing values. Imputing missing values based on insights from other features (e.g., using ‘who’ to impute ‘age’) could improve model robustness. Alternatively, we could explore more advanced techniques like multiple imputation or train models that handle missing data natively.\n\nAddressing Outliers:\n\nThe high outliers in ‘fare’ present a potential challenge for models like linear regression. In addition to log scaling, other techniques such as robust models or trimming/capping the extreme values could be useful.\n\nModel Selection:\n\nWith weak correlations between ‘survived’ and other numerical features, we may need to consider more complex, non-linear models like random forests, gradient boosting, or even deep learning methods that can capture non-linear patterns and interactions.\nThe insights from the pairplot suggest that non-linear relationships might exist between certain features, making tree-based models or ensemble methods a promising direction.\n\nIterative EDA:\n\nEDA doesn’t end here. As we start building models, we may encounter unexpected patterns or issues (e.g., poor model performance on certain subgroups, overfitting due to outliers). This will prompt us to revisit the EDA, iterating on feature engineering, transforming variables, or handling missing data more effectively.\nEvaluating model performance through techniques like cross-validation will provide additional insights, leading to further refinements in both data preprocessing and feature selection.\n\n\n\n\nInspirational Next Steps:\n\nBegin Modeling: Start by testing simple models (e.g., logistic regression) with the current feature set to get a baseline understanding of model performance. Use these models as a foundation for experimenting with more advanced methods.\nKeep Exploring: Stay curious and open to revisiting your EDA. As you iterate through feature engineering and model development, new questions will arise that could lead to even deeper insights.\nExperiment: Try different combinations of features, scaling techniques, and models. Use the insights from the EDA to inform these decisions but be prepared to experiment and validate your assumptions.\nIterate and Improve: Each iteration of modeling and EDA will bring you closer to a robust solution. Keep refining your approach as new patterns emerge and as your understanding of the dataset deepens.\n\nRemember, successful data science projects are not linear—they involve constant refinement, exploration, and learning. Keep iterating, keep questioning, and keep improving!\nGood luck on your journey from EDA to building powerful predictive models!"
  },
  {
    "objectID": "insights/Parse_leaderboard.html",
    "href": "insights/Parse_leaderboard.html",
    "title": "Nexus: Crowdsourced ML Resources",
    "section": "",
    "text": "import os\nimport yaml\nfrom collections import defaultdict\n\ndef parse_authors(base_path):\n    author_count = defaultdict(int)\n    for root, dirs, files in os.walk(base_path):\n        for file in files:\n            if file.endswith(\".qmd\"):\n                file_path = os.path.join(root, file)\n                if \"Applications/Videos\" in file_path:\n                    continue  # Skip presenter files\n                with open(file_path, 'r') as f:\n                    try:\n                        content = yaml.safe_load(f)\n                        authors = content.get(\"author\", [])\n                        if isinstance(authors, str):\n                            authors = [authors]\n                        for author in authors:\n                            author_count[author] += 1\n                    except Exception as e:\n                        print(f\"Error parsing {file_path}: {e}\")\n    return author_count\n\n# base_path = \"path/to/nexus/files\"\nbase_path = \"./../\"\nauthors = parse_authors(base_path)\n\nError parsing ./../glossary.qmd: expected a single document in the stream\n  in \"./../glossary.qmd\", line 2, column 1\nbut found another document\n  in \"./../glossary.qmd\", line 8, column 1\nError parsing ./../index.qmd: expected a single document in the stream\n  in \"./../index.qmd\", line 2, column 1\nbut found another document\n  in \"./../index.qmd\", line 42, column 1\nError parsing ./../Applications\\index.qmd: expected a single document in the stream\n  in \"./../Applications\\index.qmd\", line 2, column 1\nbut found another document\n  in \"./../Applications\\index.qmd\", line 23, column 1\nError parsing ./../Applications\\Blogs\\blog-music-identification.qmd: expected a single document in the stream\n  in \"./../Applications\\Blogs\\blog-music-identification.qmd\", line 2, column 1\nbut found another document\n  in \"./../Applications\\Blogs\\blog-music-identification.qmd\", line 26, column 1\nError parsing ./../Applications\\Blogs\\index.qmd: expected a single document in the stream\n  in \"./../Applications\\Blogs\\index.qmd\", line 2, column 1\nbut found another document\n  in \"./../Applications\\Blogs\\index.qmd\", line 22, column 1\nError parsing ./../Applications\\EDA\\index.qmd: expected a single document in the stream\n  in \"./../Applications\\EDA\\index.qmd\", line 2, column 1\nbut found another document\n  in \"./../Applications\\EDA\\index.qmd\", line 22, column 1\nError parsing ./../Applications\\EDA\\Titanic-Dataset.qmd: expected a single document in the stream\n  in \"./../Applications\\EDA\\Titanic-Dataset.qmd\", line 2, column 1\nbut found another document\n  in \"./../Applications\\EDA\\Titanic-Dataset.qmd\", line 16, column 1\nError parsing ./../Applications\\Videos\\index.qmd: expected a single document in the stream\n  in \"./../Applications\\Videos\\index.qmd\", line 2, column 1\nbut found another document\n  in \"./../Applications\\Videos\\index.qmd\", line 23, column 1\nError parsing ./../Applications\\Videos\\Forums\\index.qmd: expected a single document in the stream\n  in \"./../Applications\\Videos\\Forums\\index.qmd\", line 2, column 1\nbut found another document\n  in \"./../Applications\\Videos\\Forums\\index.qmd\", line 21, column 1\nError parsing ./../Applications\\Videos\\Forums\\mlx_2023-09-19b.qmd: expected a single document in the stream\n  in \"./../Applications\\Videos\\Forums\\mlx_2023-09-19b.qmd\", line 2, column 1\nbut found another document\n  in \"./../Applications\\Videos\\Forums\\mlx_2023-09-19b.qmd\", line 24, column 1\nError parsing ./../Applications\\Videos\\Forums\\mlx_2023-10-10.qmd: expected a single document in the stream\n  in \"./../Applications\\Videos\\Forums\\mlx_2023-10-10.qmd\", line 2, column 1\nbut found another document\n  in \"./../Applications\\Videos\\Forums\\mlx_2023-10-10.qmd\", line 20, column 1\nError parsing ./../Applications\\Videos\\Forums\\mlx_2023-11-07.qmd: expected a single document in the stream\n  in \"./../Applications\\Videos\\Forums\\mlx_2023-11-07.qmd\", line 2, column 1\nbut found another document\n  in \"./../Applications\\Videos\\Forums\\mlx_2023-11-07.qmd\", line 27, column 1\nError parsing ./../Applications\\Videos\\Forums\\mlx_2023-12-12.qmd: expected a single document in the stream\n  in \"./../Applications\\Videos\\Forums\\mlx_2023-12-12.qmd\", line 2, column 1\nbut found another document\n  in \"./../Applications\\Videos\\Forums\\mlx_2023-12-12.qmd\", line 24, column 1\nError parsing ./../Applications\\Videos\\Forums\\mlx_2024-02-13.qmd: expected a single document in the stream\n  in \"./../Applications\\Videos\\Forums\\mlx_2024-02-13.qmd\", line 2, column 1\nbut found another document\n  in \"./../Applications\\Videos\\Forums\\mlx_2024-02-13.qmd\", line 24, column 1\nError parsing ./../Applications\\Videos\\Forums\\mlx_2024-03-12.qmd: expected a single document in the stream\n  in \"./../Applications\\Videos\\Forums\\mlx_2024-03-12.qmd\", line 2, column 1\nbut found another document\n  in \"./../Applications\\Videos\\Forums\\mlx_2024-03-12.qmd\", line 28, column 1\nError parsing ./../Applications\\Videos\\Forums\\mlx_2024-04-09.qmd: expected a single document in the stream\n  in \"./../Applications\\Videos\\Forums\\mlx_2024-04-09.qmd\", line 2, column 1\nbut found another document\n  in \"./../Applications\\Videos\\Forums\\mlx_2024-04-09.qmd\", line 30, column 1\nError parsing ./../Applications\\Videos\\Forums\\mlx_2024-05-11.qmd: expected a single document in the stream\n  in \"./../Applications\\Videos\\Forums\\mlx_2024-05-11.qmd\", line 2, column 1\nbut found another document\n  in \"./../Applications\\Videos\\Forums\\mlx_2024-05-11.qmd\", line 29, column 1\nError parsing ./../Applications\\Videos\\Forums\\mlx_2024-11-05.qmd: expected a single document in the stream\n  in \"./../Applications\\Videos\\Forums\\mlx_2024-11-05.qmd\", line 2, column 1\nbut found another document\n  in \"./../Applications\\Videos\\Forums\\mlx_2024-11-05.qmd\", line 33, column 1\nError parsing ./../Applications\\Videos\\ML4MI\\24-09-16_Vision-Language-and-VisionLanguage-Modeling-in-Radiology_Tyler-Bradshaw.qmd: expected a single document in the stream\n  in \"./../Applications\\Videos\\ML4MI\\24-09-16_Vision-Language-and-VisionLanguage-Modeling-in-Radiology_Tyler-Bradshaw.qmd\", line 2, column 1\nbut found another document\n  in \"./../Applications\\Videos\\ML4MI\\24-09-16_Vision-Language-and-VisionLanguage-Modeling-in-Radiology_Tyler-Bradshaw.qmd\", line 26, column 1\nError parsing ./../Applications\\Videos\\ML4MI\\24-10-14_Using-Electronic-Health-Record-Data-to-Predict-Deterioriation-in-Hostpitalized-Children_Anoop-Mayampurath.qmd: expected a single document in the stream\n  in \"./../Applications\\Videos\\ML4MI\\24-10-14_Using-Electronic-Health-Record-Data-to-Predict-Deterioriation-in-Hostpitalized-Children_Anoop-Mayampurath.qmd\", line 2, column 1\nbut found another document\n  in \"./../Applications\\Videos\\ML4MI\\24-10-14_Using-Electronic-Health-Record-Data-to-Predict-Deterioriation-in-Hostpitalized-Children_Anoop-Mayampurath.qmd\", line 21, column 1\nError parsing ./../Applications\\Videos\\ML4MI\\index.qmd: 'charmap' codec can't decode byte 0x9d in position 1426: character maps to &lt;undefined&gt;\nError parsing ./../Applications\\Videos\\Other\\CrossLabsAI-CrossRoads45-METL-Biophysics-based-Protein-Language-Model.qmd: expected a single document in the stream\n  in \"./../Applications\\Videos\\Other\\CrossLabsAI-CrossRoads45-METL-Biophysics-based-Protein-Language-Model.qmd\", line 2, column 1\nbut found another document\n  in \"./../Applications\\Videos\\Other\\CrossLabsAI-CrossRoads45-METL-Biophysics-based-Protein-Language-Model.qmd\", line 25, column 1\nError parsing ./../Applications\\Videos\\Other\\Exploring-AI-at-UW.qmd: expected a single document in the stream\n  in \"./../Applications\\Videos\\Other\\Exploring-AI-at-UW.qmd\", line 2, column 1\nbut found another document\n  in \"./../Applications\\Videos\\Other\\Exploring-AI-at-UW.qmd\", line 13, column 1\nError parsing ./../Applications\\Videos\\Other\\index.qmd: expected a single document in the stream\n  in \"./../Applications\\Videos\\Other\\index.qmd\", line 2, column 1\nbut found another document\n  in \"./../Applications\\Videos\\Other\\index.qmd\", line 21, column 1\nError parsing ./../Applications\\Videos\\Other\\MetFactFindingsLLM.qmd: expected a single document in the stream\n  in \"./../Applications\\Videos\\Other\\MetFactFindingsLLM.qmd\", line 2, column 1\nbut found another document\n  in \"./../Applications\\Videos\\Other\\MetFactFindingsLLM.qmd\", line 44, column 1\nError parsing ./../Applications\\Videos\\SILO\\23-11-22_KennethMarino_ World-Knowledge-in-the-Time-of-Large-Models.qmd: expected a single document in the stream\n  in \"./../Applications\\Videos\\SILO\\23-11-22_KennethMarino_ World-Knowledge-in-the-Time-of-Large-Models.qmd\", line 2, column 1\nbut found another document\n  in \"./../Applications\\Videos\\SILO\\23-11-22_KennethMarino_ World-Knowledge-in-the-Time-of-Large-Models.qmd\", line 25, column 1\nError parsing ./../Applications\\Videos\\SILO\\index.qmd: 'charmap' codec can't decode byte 0x9d in position 680: character maps to &lt;undefined&gt;\nError parsing ./../Contributor-templates\\template_application-blog.qmd: expected a single document in the stream\n  in \"./../Contributor-templates\\template_application-blog.qmd\", line 2, column 1\nbut found another document\n  in \"./../Contributor-templates\\template_application-blog.qmd\", line 22, column 1\nError parsing ./../Contributor-templates\\template_application-video.qmd: expected a single document in the stream\n  in \"./../Contributor-templates\\template_application-video.qmd\", line 2, column 1\nbut found another document\n  in \"./../Contributor-templates\\template_application-video.qmd\", line 42, column 1\nError parsing ./../Contributor-templates\\template_learn-blog.qmd: expected a single document in the stream\n  in \"./../Contributor-templates\\template_learn-blog.qmd\", line 2, column 1\nbut found another document\n  in \"./../Contributor-templates\\template_learn-blog.qmd\", line 20, column 1\nError parsing ./../Contributor-templates\\template_learn-book.qmd: expected a single document in the stream\n  in \"./../Contributor-templates\\template_learn-book.qmd\", line 2, column 1\nbut found another document\n  in \"./../Contributor-templates\\template_learn-book.qmd\", line 18, column 1\nError parsing ./../Contributor-templates\\template_learn-video.qmd: expected a single document in the stream\n  in \"./../Contributor-templates\\template_learn-video.qmd\", line 2, column 1\nbut found another document\n  in \"./../Contributor-templates\\template_learn-video.qmd\", line 31, column 1\nError parsing ./../Contributor-templates\\template_learn-workshop.qmd: expected a single document in the stream\n  in \"./../Contributor-templates\\template_learn-workshop.qmd\", line 2, column 1\nbut found another document\n  in \"./../Contributor-templates\\template_learn-workshop.qmd\", line 18, column 1\nError parsing ./../Contributor-templates\\template_learn.qmd: expected a single document in the stream\n  in \"./../Contributor-templates\\template_learn.qmd\", line 2, column 1\nbut found another document\n  in \"./../Contributor-templates\\template_learn.qmd\", line 18, column 1\nError parsing ./../Contributor-templates\\template_toolbox-data.qmd: expected a single document in the stream\n  in \"./../Contributor-templates\\template_toolbox-data.qmd\", line 2, column 1\nbut found another document\n  in \"./../Contributor-templates\\template_toolbox-data.qmd\", line 18, column 1\nError parsing ./../Contributor-templates\\template_toolbox-library.qmd: expected a single document in the stream\n  in \"./../Contributor-templates\\template_toolbox-library.qmd\", line 2, column 1\nbut found another document\n  in \"./../Contributor-templates\\template_toolbox-library.qmd\", line 17, column 1\nError parsing ./../Contributor-templates\\template_toolbox-model.qmd: expected a single document in the stream\n  in \"./../Contributor-templates\\template_toolbox-model.qmd\", line 2, column 1\nbut found another document\n  in \"./../Contributor-templates\\template_toolbox-model.qmd\", line 17, column 1\nError parsing ./../includes\\common-resources-text.qmd: mapping values are not allowed here\n  in \"./../includes\\common-resources-text.qmd\", line 3, column 95\nError parsing ./../includes\\netID-disclaimer.qmd: while scanning an alias\n  in \"./../includes\\netID-disclaimer.qmd\", line 1, column 1\nexpected alphabetic or numeric character, but found '*'\n  in \"./../includes\\netID-disclaimer.qmd\", line 1, column 2\nError parsing ./../Learn\\index.qmd: expected a single document in the stream\n  in \"./../Learn\\index.qmd\", line 2, column 1\nbut found another document\n  in \"./../Learn\\index.qmd\", line 24, column 1\nError parsing ./../Learn\\Blogs\\index.qmd: expected a single document in the stream\n  in \"./../Learn\\Blogs\\index.qmd\", line 2, column 1\nbut found another document\n  in \"./../Learn\\Blogs\\index.qmd\", line 22, column 1\nError parsing ./../Learn\\Blogs\\one-useful-thing.qmd: expected a single document in the stream\n  in \"./../Learn\\Blogs\\one-useful-thing.qmd\", line 2, column 1\nbut found another document\n  in \"./../Learn\\Blogs\\one-useful-thing.qmd\", line 21, column 1\nError parsing ./../Learn\\Books\\index.qmd: expected a single document in the stream\n  in \"./../Learn\\Books\\index.qmd\", line 2, column 1\nbut found another document\n  in \"./../Learn\\Books\\index.qmd\", line 19, column 1\nError parsing ./../Learn\\Books\\Intro-Deeplearning_SimonJDPrince.qmd: 'charmap' codec can't decode byte 0x9d in position 1016: character maps to &lt;undefined&gt;\nError parsing ./../Learn\\Guides\\Github-desktop.qmd: 'charmap' codec can't decode byte 0x9d in position 76: character maps to &lt;undefined&gt;\nError parsing ./../Learn\\Guides\\How-to-contribute.qmd: 'charmap' codec can't decode byte 0x9d in position 6569: character maps to &lt;undefined&gt;\nError parsing ./../Learn\\Guides\\index.qmd: expected a single document in the stream\n  in \"./../Learn\\Guides\\index.qmd\", line 2, column 1\nbut found another document\n  in \"./../Learn\\Guides\\index.qmd\", line 20, column 1\nError parsing ./../Learn\\Videos\\Grokking.qmd: expected a single document in the stream\n  in \"./../Learn\\Videos\\Grokking.qmd\", line 2, column 1\nbut found another document\n  in \"./../Learn\\Videos\\Grokking.qmd\", line 14, column 1\nError parsing ./../Learn\\Videos\\index.qmd: expected a single document in the stream\n  in \"./../Learn\\Videos\\index.qmd\", line 2, column 1\nbut found another document\n  in \"./../Learn\\Videos\\index.qmd\", line 19, column 1\nError parsing ./../Learn\\Videos\\Intro-git-github.qmd: expected a single document in the stream\n  in \"./../Learn\\Videos\\Intro-git-github.qmd\", line 2, column 1\nbut found another document\n  in \"./../Learn\\Videos\\Intro-git-github.qmd\", line 15, column 1\nError parsing ./../Learn\\Videos\\OOD-detection.qmd: expected a single document in the stream\n  in \"./../Learn\\Videos\\OOD-detection.qmd\", line 2, column 1\nbut found another document\n  in \"./../Learn\\Videos\\OOD-detection.qmd\", line 13, column 1\nError parsing ./../Learn\\Videos\\Reproducibility-overview.qmd: expected a single document in the stream\n  in \"./../Learn\\Videos\\Reproducibility-overview.qmd\", line 2, column 1\nbut found another document\n  in \"./../Learn\\Videos\\Reproducibility-overview.qmd\", line 12, column 1\nError parsing ./../Learn\\Workshops\\index.qmd: expected a single document in the stream\n  in \"./../Learn\\Workshops\\index.qmd\", line 2, column 1\nbut found another document\n  in \"./../Learn\\Workshops\\index.qmd\", line 21, column 1\nError parsing ./../Learn\\Workshops\\Intro-Amazon_SageMaker.qmd: expected a single document in the stream\n  in \"./../Learn\\Workshops\\Intro-Amazon_SageMaker.qmd\", line 2, column 1\nbut found another document\n  in \"./../Learn\\Workshops\\Intro-Amazon_SageMaker.qmd\", line 18, column 1\nError parsing ./../Learn\\Workshops\\Intro-Deeplearning_Keras.qmd: expected a single document in the stream\n  in \"./../Learn\\Workshops\\Intro-Deeplearning_Keras.qmd\", line 2, column 1\nbut found another document\n  in \"./../Learn\\Workshops\\Intro-Deeplearning_Keras.qmd\", line 17, column 1\nError parsing ./../Learn\\Workshops\\Intro-Deeplearning_PyTorch.qmd: expected a single document in the stream\n  in \"./../Learn\\Workshops\\Intro-Deeplearning_PyTorch.qmd\", line 2, column 1\nbut found another document\n  in \"./../Learn\\Workshops\\Intro-Deeplearning_PyTorch.qmd\", line 16, column 1\nError parsing ./../Learn\\Workshops\\Intro-ML_Sklearn.qmd: expected a single document in the stream\n  in \"./../Learn\\Workshops\\Intro-ML_Sklearn.qmd\", line 2, column 1\nbut found another document\n  in \"./../Learn\\Workshops\\Intro-ML_Sklearn.qmd\", line 22, column 1\nError parsing ./../Learn\\Workshops\\Intro-Python_Gapminder.qmd: expected a single document in the stream\n  in \"./../Learn\\Workshops\\Intro-Python_Gapminder.qmd\", line 2, column 1\nbut found another document\n  in \"./../Learn\\Workshops\\Intro-Python_Gapminder.qmd\", line 15, column 1\nError parsing ./../Learn\\Workshops\\Intro-TextAnalysis_Python.qmd: expected a single document in the stream\n  in \"./../Learn\\Workshops\\Intro-TextAnalysis_Python.qmd\", line 2, column 1\nbut found another document\n  in \"./../Learn\\Workshops\\Intro-TextAnalysis_Python.qmd\", line 19, column 1\nError parsing ./../Learn\\Workshops\\TrustworthyAI_Explainability-Bias-Fairness-OODdetection.qmd: expected a single document in the stream\n  in \"./../Learn\\Workshops\\TrustworthyAI_Explainability-Bias-Fairness-OODdetection.qmd\", line 2, column 1\nbut found another document\n  in \"./../Learn\\Workshops\\TrustworthyAI_Explainability-Bias-Fairness-OODdetection.qmd\", line 28, column 1\nError parsing ./../Toolbox\\index.qmd: expected a single document in the stream\n  in \"./../Toolbox\\index.qmd\", line 2, column 1\nbut found another document\n  in \"./../Toolbox\\index.qmd\", line 27, column 1\nError parsing ./../Toolbox\\Compute\\CHTC.qmd: expected a single document in the stream\n  in \"./../Toolbox\\Compute\\CHTC.qmd\", line 2, column 1\nbut found another document\n  in \"./../Toolbox\\Compute\\CHTC.qmd\", line 12, column 1\nError parsing ./../Toolbox\\Compute\\index.qmd: expected a single document in the stream\n  in \"./../Toolbox\\Compute\\index.qmd\", line 2, column 1\nbut found another document\n  in \"./../Toolbox\\Compute\\index.qmd\", line 20, column 1\nError parsing ./../Toolbox\\Data\\Gutenberg.qmd: expected a single document in the stream\n  in \"./../Toolbox\\Data\\Gutenberg.qmd\", line 2, column 1\nbut found another document\n  in \"./../Toolbox\\Data\\Gutenberg.qmd\", line 20, column 1\nError parsing ./../Toolbox\\Data\\index.qmd: expected a single document in the stream\n  in \"./../Toolbox\\Data\\index.qmd\", line 2, column 1\nbut found another document\n  in \"./../Toolbox\\Data\\index.qmd\", line 19, column 1\nError parsing ./../Toolbox\\Data\\LVD2021_Leaf-Vein-Dataset.qmd: expected a single document in the stream\n  in \"./../Toolbox\\Data\\LVD2021_Leaf-Vein-Dataset.qmd\", line 2, column 1\nbut found another document\n  in \"./../Toolbox\\Data\\LVD2021_Leaf-Vein-Dataset.qmd\", line 17, column 1\nError parsing ./../Toolbox\\GenAI\\index.qmd: expected a single document in the stream\n  in \"./../Toolbox\\GenAI\\index.qmd\", line 2, column 1\nbut found another document\n  in \"./../Toolbox\\GenAI\\index.qmd\", line 20, column 1\nError parsing ./../Toolbox\\GenAI\\NotebookLM.qmd: expected a single document in the stream\n  in \"./../Toolbox\\GenAI\\NotebookLM.qmd\", line 2, column 1\nbut found another document\n  in \"./../Toolbox\\GenAI\\NotebookLM.qmd\", line 17, column 1\nError parsing ./../Toolbox\\Libraries\\AI-Fairness-360.qmd: expected a single document in the stream\n  in \"./../Toolbox\\Libraries\\AI-Fairness-360.qmd\", line 2, column 1\nbut found another document\n  in \"./../Toolbox\\Libraries\\AI-Fairness-360.qmd\", line 23, column 1\nError parsing ./../Toolbox\\Libraries\\index.qmd: expected a single document in the stream\n  in \"./../Toolbox\\Libraries\\index.qmd\", line 2, column 1\nbut found another document\n  in \"./../Toolbox\\Libraries\\index.qmd\", line 23, column 1\nError parsing ./../Toolbox\\Libraries\\kornia.qmd: expected a single document in the stream\n  in \"./../Toolbox\\Libraries\\kornia.qmd\", line 2, column 1\nbut found another document\n  in \"./../Toolbox\\Libraries\\kornia.qmd\", line 23, column 1\nError parsing ./../Toolbox\\Libraries\\MONAI.qmd: expected a single document in the stream\n  in \"./../Toolbox\\Libraries\\MONAI.qmd\", line 2, column 1\nbut found another document\n  in \"./../Toolbox\\Libraries\\MONAI.qmd\", line 17, column 1\nError parsing ./../Toolbox\\Libraries\\PyTorch-OOD.qmd: expected a single document in the stream\n  in \"./../Toolbox\\Libraries\\PyTorch-OOD.qmd\", line 2, column 1\nbut found another document\n  in \"./../Toolbox\\Libraries\\PyTorch-OOD.qmd\", line 25, column 1\nError parsing ./../Toolbox\\Models\\index.qmd: expected a single document in the stream\n  in \"./../Toolbox\\Models\\index.qmd\", line 2, column 1\nbut found another document\n  in \"./../Toolbox\\Models\\index.qmd\", line 19, column 1\nError parsing ./../Toolbox\\Models\\UNET.qmd: expected a single document in the stream\n  in \"./../Toolbox\\Models\\UNET.qmd\", line 2, column 1\nbut found another document\n  in \"./../Toolbox\\Models\\UNET.qmd\", line 17, column 1\nError parsing ./../Toolbox\\Models\\XGBoost.qmd: expected a single document in the stream\n  in \"./../Toolbox\\Models\\XGBoost.qmd\", line 2, column 1\nbut found another document\n  in \"./../Toolbox\\Models\\XGBoost.qmd\", line 17, column 1\n\n\n\n%pwd\n\n'C:\\\\Users\\\\Endemann\\\\Documents\\\\GitHub\\\\ML-X-Nexus\\\\insights'"
  },
  {
    "objectID": "Contributor-templates/template_toolbox-library.html",
    "href": "Contributor-templates/template_toolbox-library.html",
    "title": "Library Name",
    "section": "",
    "text": "[Library Name] is a [brief description of the library: its purpose and key features]. Developed by [Author(s)/Organization] and introduced in [Year], [Library Name] offers tools for [specific task(s), e.g., image transformations, data augmentation, visualization]. Its design allows users to [mention ease of use, efficiency, flexibility], making it ideal for [target audience or task, e.g., deep learning practitioners, data engineers, etc.].\n\n\n\nFeature 1: Describe a core feature, e.g., fast image transformations, robust preprocessing for NLP, etc.\n\nFor vision libraries: Could describe specific transformations like rotation, flipping, scaling.\nFor NLP libraries: Could describe tokenization methods, text augmentation.\nFor tabular data libraries: Could describe missing data imputation, feature scaling, etc.\n\nFeature 2: Another key feature of the library\nPerformance: Mention performance benchmarks if available, or give an overview of efficiency and scalability"
  },
  {
    "objectID": "Contributor-templates/template_toolbox-library.html#about-this-resource",
    "href": "Contributor-templates/template_toolbox-library.html#about-this-resource",
    "title": "Library Name",
    "section": "",
    "text": "[Library Name] is a [brief description of the library: its purpose and key features]. Developed by [Author(s)/Organization] and introduced in [Year], [Library Name] offers tools for [specific task(s), e.g., image transformations, data augmentation, visualization]. Its design allows users to [mention ease of use, efficiency, flexibility], making it ideal for [target audience or task, e.g., deep learning practitioners, data engineers, etc.].\n\n\n\nFeature 1: Describe a core feature, e.g., fast image transformations, robust preprocessing for NLP, etc.\n\nFor vision libraries: Could describe specific transformations like rotation, flipping, scaling.\nFor NLP libraries: Could describe tokenization methods, text augmentation.\nFor tabular data libraries: Could describe missing data imputation, feature scaling, etc.\n\nFeature 2: Another key feature of the library\nPerformance: Mention performance benchmarks if available, or give an overview of efficiency and scalability"
  },
  {
    "objectID": "Contributor-templates/template_toolbox-library.html#integration-and-compatibility",
    "href": "Contributor-templates/template_toolbox-library.html#integration-and-compatibility",
    "title": "Library Name",
    "section": "Integration and compatibility",
    "text": "Integration and compatibility\n[Library Name] integrates with various machine learning frameworks and libraries, making it versatile for a range of tasks.\n\nFrameworks Supported: List supported frameworks, e.g., PyTorch, TensorFlow, scikit-learn\nCompatible Libraries: Mention compatible libraries, e.g., OpenCV, Hugging Face Transformers\nInstallation Instructions: Provide installation commands, e.g., pip install library-name, conda install library-name"
  },
  {
    "objectID": "Contributor-templates/template_toolbox-library.html#use-cases",
    "href": "Contributor-templates/template_toolbox-library.html#use-cases",
    "title": "Library Name",
    "section": "Use cases",
    "text": "Use cases\nHere are some examples of how [Library Name] can be applied to different machine learning tasks.\n\nUse Case 1: Description of a specific use case, e.g., augmenting image datasets for training CNNs\nUse Case 2: Another relevant application, e.g., processing large text corpora for NLP models\nUse Case 3: Optional: Add more use cases if applicable"
  },
  {
    "objectID": "Contributor-templates/template_toolbox-library.html#tutorials-and-resources",
    "href": "Contributor-templates/template_toolbox-library.html#tutorials-and-resources",
    "title": "Library Name",
    "section": "Tutorials and resources",
    "text": "Tutorials and resources\n\nGetting started\n\nOfficial Tutorial]: Link to a beginner-friendly tutorial or notebook\nExample Colab/Notebook: Link to an example Colab notebook or GitHub repository showcasing the library in action\n\n\n\nHigh-level tips for effective use\n\nOptimization: Describe ways to optimize performance, e.g., batch processing for large datasets\nMemory Management: Tips on handling memory usage, especially for large datasets\nCommon Pitfalls: Mention common mistakes and how to avoid them, e.g., ensuring data formats match\n\n\n\nRelated libraries & tools\n\nRelated Library 1: Mention a related library and what it adds/compares to the current library\nTool/Plugin 1: Mention tools/plugins that complement this library"
  },
  {
    "objectID": "Contributor-templates/template_toolbox-library.html#questions",
    "href": "Contributor-templates/template_toolbox-library.html#questions",
    "title": "Library Name",
    "section": "Questions?",
    "text": "Questions?\nIf you have any lingering questions about this resource, feel free to post them on the ML+X Nexus Q&A on GitHub."
  },
  {
    "objectID": "Contributor-templates/template_toolbox-library.html#see-also",
    "href": "Contributor-templates/template_toolbox-library.html#see-also",
    "title": "Library Name",
    "section": "See also",
    "text": "See also\n\n\nRelated Resource 1: Brief description of related resource 1.\nRelated Resource 2: Brief description of related resource 2.\nRelated Resource 3: Brief description of related resource 3"
  },
  {
    "objectID": "Contributor-templates/template_toolbox-data.html",
    "href": "Contributor-templates/template_toolbox-data.html",
    "title": "Dataset Name",
    "section": "",
    "text": "The [Dataset Name] dataset contains [description of data: what it consists of, its size, and the variety of contents]. Researchers and students working on machine learning applications can use this dataset to explore tasks such as [relevant tasks: classification, regression, generation, etc.]. The dataset’s availability in [formats, e.g., text, image, tabular, audio, etc.] makes it suitable for [type of tasks, e.g., multimodal learning, NLP, image analysis, etc.] tasks as well.\n\n\n\nFeature 1: Describe a key feature of the dataset, e.g., modality, format availability, data variety, etc.\n\nFor text: Could include multilingual content, long-form text, etc.\nFor images: Could include resolution, labeled/annotated data, specific objects/subjects present, etc.\nFor tabular: Could include number of features, type of features (categorical, continuous), missing data, etc.\n\nFeature 2: Describe another key feature\n\nFor text: Possible audio pairings, language variety\nFor images: Pre-segmented images, spatial resolution, etc.\nFor tabular: Structured metadata, time-series elements, etc.\n\nFeature 3: Highlight any distinguishing characteristics\n\nFor text: Long-form, document-level annotations, etc.\nFor images: Bounding boxes, pixel-level segmentation, medical imaging, etc.\nFor tabular: Feature engineering potential, handling large-scale datasets, etc.\n\n\n\n\n\n\nApplication 1: Describe how the dataset can be used in a specific task, e.g., language modeling, classification\n\nFor text: Language modeling, translation, classification, Sentiment analysis, summarization, Topic modeling, question answering\nFor images: Object detection, image classification, image segmentation, Face detection, scene understanding, Image generation, style transfer, medical diagnostics\nFor tabular: Predictive modeling, classification, time-series forecasting\n\nApplication 2: Include more specific machine learning tasks the dataset is commonly applied to\nMultimodal Learning, if applicable: If the dataset supports multimodal tasks, describe how it can be used in this context\n\nE.g., paired image and text, or text and audio datasets\n\nTransfer Learning, if applicable: Mention transfer learning use cases\n\nFor all types: Fine-tuning pre-trained models for specific tasks using this dataset\n\nData Augmentation, if applicable: Explain how the dataset can be used to augment smaller datasets\n\nText: Generating more training data via paraphrasing, etc.\nImages: Rotations, flips, color variations\nTabular: Synthetic data generation, imputation of missing values\n\n\n\n\n\n\nRelated Dataset 1: Provide a brief description of a related dataset or project\n\nFor text: Could include other large corpora (e.g., Common Crawl)\nFor images: COCO, ImageNet, etc.\nFor tabular: UCI Machine Learning Repository, Kaggle datasets\n\nRelated Dataset 2: Include any other related projects or datasets that users may find useful"
  },
  {
    "objectID": "Contributor-templates/template_toolbox-data.html#about-this-resource",
    "href": "Contributor-templates/template_toolbox-data.html#about-this-resource",
    "title": "Dataset Name",
    "section": "",
    "text": "The [Dataset Name] dataset contains [description of data: what it consists of, its size, and the variety of contents]. Researchers and students working on machine learning applications can use this dataset to explore tasks such as [relevant tasks: classification, regression, generation, etc.]. The dataset’s availability in [formats, e.g., text, image, tabular, audio, etc.] makes it suitable for [type of tasks, e.g., multimodal learning, NLP, image analysis, etc.] tasks as well.\n\n\n\nFeature 1: Describe a key feature of the dataset, e.g., modality, format availability, data variety, etc.\n\nFor text: Could include multilingual content, long-form text, etc.\nFor images: Could include resolution, labeled/annotated data, specific objects/subjects present, etc.\nFor tabular: Could include number of features, type of features (categorical, continuous), missing data, etc.\n\nFeature 2: Describe another key feature\n\nFor text: Possible audio pairings, language variety\nFor images: Pre-segmented images, spatial resolution, etc.\nFor tabular: Structured metadata, time-series elements, etc.\n\nFeature 3: Highlight any distinguishing characteristics\n\nFor text: Long-form, document-level annotations, etc.\nFor images: Bounding boxes, pixel-level segmentation, medical imaging, etc.\nFor tabular: Feature engineering potential, handling large-scale datasets, etc.\n\n\n\n\n\n\nApplication 1: Describe how the dataset can be used in a specific task, e.g., language modeling, classification\n\nFor text: Language modeling, translation, classification, Sentiment analysis, summarization, Topic modeling, question answering\nFor images: Object detection, image classification, image segmentation, Face detection, scene understanding, Image generation, style transfer, medical diagnostics\nFor tabular: Predictive modeling, classification, time-series forecasting\n\nApplication 2: Include more specific machine learning tasks the dataset is commonly applied to\nMultimodal Learning, if applicable: If the dataset supports multimodal tasks, describe how it can be used in this context\n\nE.g., paired image and text, or text and audio datasets\n\nTransfer Learning, if applicable: Mention transfer learning use cases\n\nFor all types: Fine-tuning pre-trained models for specific tasks using this dataset\n\nData Augmentation, if applicable: Explain how the dataset can be used to augment smaller datasets\n\nText: Generating more training data via paraphrasing, etc.\nImages: Rotations, flips, color variations\nTabular: Synthetic data generation, imputation of missing values\n\n\n\n\n\n\nRelated Dataset 1: Provide a brief description of a related dataset or project\n\nFor text: Could include other large corpora (e.g., Common Crawl)\nFor images: COCO, ImageNet, etc.\nFor tabular: UCI Machine Learning Repository, Kaggle datasets\n\nRelated Dataset 2: Include any other related projects or datasets that users may find useful"
  },
  {
    "objectID": "Contributor-templates/template_toolbox-data.html#questions",
    "href": "Contributor-templates/template_toolbox-data.html#questions",
    "title": "Dataset Name",
    "section": "Questions?",
    "text": "Questions?\nIf you have any lingering questions about this resource, feel free to post them on the ML+X Nexus Q&A on GitHub. We will update this resource as new information or applications arise."
  },
  {
    "objectID": "Contributor-templates/template_toolbox-data.html#see-also",
    "href": "Contributor-templates/template_toolbox-data.html#see-also",
    "title": "Dataset Name",
    "section": "See also",
    "text": "See also\n\n\nRelated Resource 1: Brief description of related resource 1.\nRelated Resource 2: Brief description of related resource 2.\nRelated Resource 3: Brief description of related resource 3"
  },
  {
    "objectID": "Contributor-templates/template_application-blog.html",
    "href": "Contributor-templates/template_application-blog.html",
    "title": "Title of Your Blog Post",
    "section": "",
    "text": "Provide a short introduction to your blog, including its main focus and why it’s relevant for the ML+X Nexus community. Highlight any key topics, challenges, or learnings you will discuss. Feel free to include a text embedded link to any related resources or references."
  },
  {
    "objectID": "Contributor-templates/template_application-blog.html#code-snippets-optional",
    "href": "Contributor-templates/template_application-blog.html#code-snippets-optional",
    "title": "Title of Your Blog Post",
    "section": "Code Snippets (Optional)",
    "text": "Code Snippets (Optional)\nIf you’re sharing code directly, include short snippets or explanations of the key components below. If you’d prefer to link to a repository, feel free to link your GitHub repo here.\n```python # Example code snippet import numpy as np print(“Hello ML+X Nexus!”)"
  },
  {
    "objectID": "Contributor-templates/template_application-blog.html#questions",
    "href": "Contributor-templates/template_application-blog.html#questions",
    "title": "Title of Your Blog Post",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Contributor-templates/template_application-blog.html#see-also",
    "href": "Contributor-templates/template_application-blog.html#see-also",
    "title": "Title of Your Blog Post",
    "section": "See also",
    "text": "See also\n\n\nRelated Resource 1: Brief description of related resource 1.\nRelated Resource 2: Brief description of related resource 2.\nRelated Resource 3: Brief description of related resource 3."
  },
  {
    "objectID": "Contributor-templates/template_learn-blog.html",
    "href": "Contributor-templates/template_learn-blog.html",
    "title": "Title/Topic of Resource",
    "section": "",
    "text": "Brief description of the resource, including a text embedded link in the first 1-2 sentences. Explain what the resource covers and its relevance. Mention any specific features, strengths, or weaknesses. This section should help potential users understand the value of the resource and what they can expect to learn or achieve by using it. n\n\n\n\n\nPrerequisite Resource 1\nPrerequisite Resource 2"
  },
  {
    "objectID": "Contributor-templates/template_learn-blog.html#about-this-resource",
    "href": "Contributor-templates/template_learn-blog.html#about-this-resource",
    "title": "Title/Topic of Resource",
    "section": "",
    "text": "Brief description of the resource, including a text embedded link in the first 1-2 sentences. Explain what the resource covers and its relevance. Mention any specific features, strengths, or weaknesses. This section should help potential users understand the value of the resource and what they can expect to learn or achieve by using it. n\n\n\n\n\nPrerequisite Resource 1\nPrerequisite Resource 2"
  },
  {
    "objectID": "Contributor-templates/template_learn-blog.html#questions",
    "href": "Contributor-templates/template_learn-blog.html#questions",
    "title": "Title/Topic of Resource",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Contributor-templates/template_learn-blog.html#see-also",
    "href": "Contributor-templates/template_learn-blog.html#see-also",
    "title": "Title/Topic of Resource",
    "section": "See also",
    "text": "See also\n\n\nRelated Resource 1: Brief description of related resource 1.\nRelated Resource 2: Brief description of related resource 2.\nRelated Resource 3: Brief description of related resource 3."
  },
  {
    "objectID": "Contributor-templates/template_learn-workshop.html",
    "href": "Contributor-templates/template_learn-workshop.html",
    "title": "Title/Topic of Resource",
    "section": "",
    "text": "Brief description of the resource, including a text embedded link in the first 1-2 sentences. Explain what the resource covers and its relevance. Mention any specific features, strengths, or weaknesses. This section should help potential users understand the value of the resource and what they can expect to learn or achieve by using it.\n\n\n\n\nPrerequisite Resource 1\nPrerequisite Resource 2"
  },
  {
    "objectID": "Contributor-templates/template_learn-workshop.html#about-this-resource",
    "href": "Contributor-templates/template_learn-workshop.html#about-this-resource",
    "title": "Title/Topic of Resource",
    "section": "",
    "text": "Brief description of the resource, including a text embedded link in the first 1-2 sentences. Explain what the resource covers and its relevance. Mention any specific features, strengths, or weaknesses. This section should help potential users understand the value of the resource and what they can expect to learn or achieve by using it.\n\n\n\n\nPrerequisite Resource 1\nPrerequisite Resource 2"
  },
  {
    "objectID": "Contributor-templates/template_learn-workshop.html#questions",
    "href": "Contributor-templates/template_learn-workshop.html#questions",
    "title": "Title/Topic of Resource",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Contributor-templates/template_learn-workshop.html#see-also",
    "href": "Contributor-templates/template_learn-workshop.html#see-also",
    "title": "Title/Topic of Resource",
    "section": "See also",
    "text": "See also\n\n\nRelated Resource 1: Brief description of related resource 1.\nRelated Resource 2: Brief description of related resource 2.\nRelated Resource 3: Brief description of related resource 3."
  },
  {
    "objectID": "Toolbox/Models/XGBoost.html",
    "href": "Toolbox/Models/XGBoost.html",
    "title": "XGBoost: Tree-Based Gradient Boosting for Tabular Data",
    "section": "",
    "text": "XGBoost, or eXtreme Gradient Boosting, is a machine learning algorithm built upon the foundation of decision trees, extending their power through boosting. Originally introduced by Tianqi Chen in 2016, XGBoost has revolutionized predictive modeling, especially for tabular data, thanks to its efficiency, scalability, and performance. It is particularly well-suited for tasks like regression, classification, and ranking, where interpretability, speed, and precision are key."
  },
  {
    "objectID": "Toolbox/Models/XGBoost.html#about-this-resource",
    "href": "Toolbox/Models/XGBoost.html#about-this-resource",
    "title": "XGBoost: Tree-Based Gradient Boosting for Tabular Data",
    "section": "",
    "text": "XGBoost, or eXtreme Gradient Boosting, is a machine learning algorithm built upon the foundation of decision trees, extending their power through boosting. Originally introduced by Tianqi Chen in 2016, XGBoost has revolutionized predictive modeling, especially for tabular data, thanks to its efficiency, scalability, and performance. It is particularly well-suited for tasks like regression, classification, and ranking, where interpretability, speed, and precision are key."
  },
  {
    "objectID": "Toolbox/Models/XGBoost.html#tree-based-learning-and-enhancements-in-xgboost",
    "href": "Toolbox/Models/XGBoost.html#tree-based-learning-and-enhancements-in-xgboost",
    "title": "XGBoost: Tree-Based Gradient Boosting for Tabular Data",
    "section": "Tree-based learning and enhancements in XGBoost",
    "text": "Tree-based learning and enhancements in XGBoost\nAt its core, XGBoost uses decision trees to model complex patterns in data, applying gradient boosting to improve model accuracy:\n\nAdditive learning: XGBoost builds trees sequentially, where each new tree focuses on the residuals (errors) of the previous trees.\nGradient boosting: By minimizing a loss function (using gradient descent), XGBoost creates an ensemble of decision trees that progressively improve predictions.\n\n\nEnhancements\n\nRegularization: XGBoost applies L1 (Lasso) and L2 (Ridge) regularization to control model complexity and reduce overfitting.\nPruning: Decision trees in XGBoost are pruned during training to avoid overfitting and maintain generalization ability.\nMissing data handling: XGBoost can automatically handle missing data, making it particularly robust for real-world datasets."
  },
  {
    "objectID": "Toolbox/Models/XGBoost.html#comparisons-to-deep-learning",
    "href": "Toolbox/Models/XGBoost.html#comparisons-to-deep-learning",
    "title": "XGBoost: Tree-Based Gradient Boosting for Tabular Data",
    "section": "Comparisons to deep learning",
    "text": "Comparisons to deep learning\nWhile deep learning excels at tasks like image recognition and natural language processing, XGBoost often outperforms deep learning models on tabular data, especially in the following scenarios:\n\nSmall to medium-sized datasets: Deep learning models require large amounts of data to perform well, whereas XGBoost can achieve high accuracy even with smaller datasets.\nMedical contexts: In medical datasets, where structured/tabular data is prevalent, XGBoost often outperforms deep learning due to its efficiency and the importance of feature engineering.\nFaster training Time: XGBoost is much faster to train compared to deep learning models, making it ideal for applications with limited computational resources or time constraints.\n\n\nInterpretability\nXGBoost offers greater interpretability than deep learning models, but it is less interpretable than simpler models like decision trees or linear regressions:\n\nFeature Importance: XGBoost provides feature importance scores, showing which features contribute the most to model accuracy.\nEnsemble Complexity: While individual trees in the XGBoost ensemble are interpretable, the combined model (with potentially hundreds of trees) can be difficult to fully explain.\nSHAP Values and LIME: These tools allow you to understand how each feature influences predictions both locally (for individual instances) and globally (across the whole model)."
  },
  {
    "objectID": "Toolbox/Models/XGBoost.html#timeline-context",
    "href": "Toolbox/Models/XGBoost.html#timeline-context",
    "title": "XGBoost: Tree-Based Gradient Boosting for Tabular Data",
    "section": "Timeline context",
    "text": "Timeline context\nXGBoost builds on a rich history of decision tree-based models. Here’s how it fits into the broader development of machine learning models that rely on trees:\n\nClassification and Regression Trees - CART (1984): The foundation of decision tree algorithms, focusing on binary splits of data based on feature values.\nRandom Forest (2001): Combines many independent decision trees for better accuracy and robustness by reducing variance through bootstrapping.\nXGBoost (2016): An optimized implementation of gradient boosting, which leverages tree-based models and introduces regularization, parallelism, and handling of missing values.\nCatBoost (2018): Introduced categorical feature handling directly in gradient boosting."
  },
  {
    "objectID": "Toolbox/Models/XGBoost.html#xgboost-variants",
    "href": "Toolbox/Models/XGBoost.html#xgboost-variants",
    "title": "XGBoost: Tree-Based Gradient Boosting for Tabular Data",
    "section": "XGBoost variants",
    "text": "XGBoost variants\n\nXGBoost with DART: Modifies the traditional tree boosting approach by dropping trees randomly during training, preventing overfitting and improving generalization.\nXGBoost with Linear Booster: Instead of building trees, this variant uses a linear model as the base learner, blending gradient boosting with linear regression or classification."
  },
  {
    "objectID": "Toolbox/Models/XGBoost.html#model-playground",
    "href": "Toolbox/Models/XGBoost.html#model-playground",
    "title": "XGBoost: Tree-Based Gradient Boosting for Tabular Data",
    "section": "Model playground",
    "text": "Model playground\n\nTutorials and Getting Started Notebooks\n\nXGBoost: Check out the XGBoost Python Documentation for installation, basic usage, and advanced tuning tips.\n\n\n\nHigh-level tips for effective sse\n\nHyperparameter tuning: Parameters like max depth and learning rate control the complexity and speed of the trees; tuning these is crucial for optimal performance.\nHandling missing data: XGBoost’s ability to handle missing values natively means there’s no need for complex imputation strategies.\nEarly stopping: Use early stopping with cross-validation to prevent overfitting, especially when training deep trees.\nImbalanced datasets: Adjust the scale_pos_weight parameter to manage class imbalance in datasets, like those seen in fraud detection or rare event classification.\n\n\n\nRelated datasets & Kaggle challenges\n\nAmes House Price Prediction: A regression task where XGBoost often outperforms deep learning due to the tabular nature of the data.\nDisaster Tweets Classification: A classification task that involves text, but XGBoost can excel with good feature engineering."
  },
  {
    "objectID": "Toolbox/Models/XGBoost.html#questions",
    "href": "Toolbox/Models/XGBoost.html#questions",
    "title": "XGBoost: Tree-Based Gradient Boosting for Tabular Data",
    "section": "Questions?",
    "text": "Questions?\nIf you have any lingering questions about this resource, feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Toolbox/Models/index.html",
    "href": "Toolbox/Models/index.html",
    "title": "Models",
    "section": "",
    "text": "Explore popular model architectures, pretrained models, and foundation models. You may wish to also check out Libraries: Model exploration for libraries (e.g., MONAI, Kornia) that allow you to implement and test a variety of models with ease.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPytesseract: OCR with Tesseract (LSTM) in Python\n\n\n\nLibraries\n\nOCR\n\nNLP\n\nComputer vision\n\nText extraction\n\nMultilingual\n\nDeep learning\n\nLSTM\n\n\n\n\n\n\n\nChris Endemann\n\n\n2025-04-05\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeepForest: A Toolkit of Models for Tree and Wildlife Detection in Aerial Imagery\n\n\n\nModels\n\nModel exploration\n\nObject detection\n\nRemote sensing\n\nEcology\n\nForest monitoring\n\nGeospatial data\n\nComputer vision\n\nDeep learning\n\nCNN\n\n\n\n\n\n\n\nChris Endemann\n\n\n2025-04-04\n\n\n\n\n\n\n\n\n\n\n\n\n\nXGBoost: Tree-Based Gradient Boosting for Tabular Data\n\n\n\nModels\n\nBoosting\n\nXGBoost\n\nDecision trees\n\nTabular\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-10-17\n\n\n\n\n\n\n\n\n\n\n\n\n\nU-Net: Convolutional Networks for Biomedical Image Segmentation\n\n\n\nModels\n\nDeep learning\n\nMedical imaging\n\nImage segmentation\n\nCNN\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-09-16\n\n\n\n\n\n\n\n\n\n\n\n\n\nMONAI: Medical Open Network for AI\n\n\n\nLibraries\n\nDeep learning\n\nPyTorch\n\nMedical imaging\n\nModel exploration\n\n\n\n\n\n\n\nAlan McMillan\n\n\n2024-08-14\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Toolbox",
      "Models"
    ]
  },
  {
    "objectID": "Toolbox/Data/Gutenberg.html",
    "href": "Toolbox/Data/Gutenberg.html",
    "title": "Project Gutenberg: Text & Audio Books",
    "section": "",
    "text": "The Project Gutenberg dataset contains text from thousands of books, spanning a variety of genres and styles, and in some cases, corresponding audiobooks. Researchers and students working on machine learning applications can use this dataset to explore tasks such as language modeling, text classification, summarization, and speech synthesis. The dataset’s availability in both text and audio formats makes it suitable for multimodal learning tasks as well.\n\n\n\nText & audio: Many books in the Gutenberg collection have corresponding audiobooks (through Librivox), enabling both text and audio-based learning tasks.\nMultilingual content: While primarily in English, the dataset includes books in other languages such as French, German, and Spanish, providing opportunities for multilingual and cross-lingual research in NLP.\nLong-form text: The dataset includes full-length novels, short stories, and essays, making it ideal for tasks that require understanding context over longer sequences of text.\n\n\n\n\n\nLanguage modeling: With its vast variety of literary styles and genres, Gutenberg serves as a valuable resource for training and evaluating language models like GPT and BERT. Pre-training on Gutenberg’s diverse text corpus allows models to capture nuanced linguistic patterns, which can later be fine-tuned for more specific NLP tasks.\nText classification: The dataset can be applied to classification tasks such as genre classification or sentiment analysis. Researchers often use Gutenberg to train classifiers that distinguish between literary styles or detect emotional tone in texts.\nSummarization and translation: Due to the diversity in content, Gutenberg is commonly used to test summarization models (e.g., creating concise book summaries) and translation algorithms across different literary forms.\nTopic modeling: The diverse collection of texts allows for the exploration of underlying themes or topics through techniques like Latent Dirichlet Allocation (LDA) or Non-negative Matrix Factorization (NMF), enabling researchers to uncover hidden patterns in the literature.\nMultimodal learning: Paired with Librivox audiobooks, the Gutenberg dataset enables multimodal tasks like text-to-speech synthesis, speech recognition, and aligning spoken text with its written counterpart. This supports the development of models like Tacotron and Wav2Vec.\nTransfer learning: Researchers frequently fine-tune pre-trained language models on Gutenberg to test performance on literary and long-form text, often comparing results with models trained on broader corpora like Common Crawl.\nData augmentation: Gutenberg’s large-scale, structured text is ideal for augmenting smaller datasets and improving model robustness through data imputation or other generalization techniques.\n\n\n\n\n\nLibrivox Audiobook Collection: Provides audiobooks to accompany the texts available in Project Gutenberg.\nCommon Crawl: Large-scale web crawl dataset often used to pre-train language models. Gutenberg can provide a more structured and curated supplement to such datasets."
  },
  {
    "objectID": "Toolbox/Data/Gutenberg.html#about-this-resource",
    "href": "Toolbox/Data/Gutenberg.html#about-this-resource",
    "title": "Project Gutenberg: Text & Audio Books",
    "section": "",
    "text": "The Project Gutenberg dataset contains text from thousands of books, spanning a variety of genres and styles, and in some cases, corresponding audiobooks. Researchers and students working on machine learning applications can use this dataset to explore tasks such as language modeling, text classification, summarization, and speech synthesis. The dataset’s availability in both text and audio formats makes it suitable for multimodal learning tasks as well.\n\n\n\nText & audio: Many books in the Gutenberg collection have corresponding audiobooks (through Librivox), enabling both text and audio-based learning tasks.\nMultilingual content: While primarily in English, the dataset includes books in other languages such as French, German, and Spanish, providing opportunities for multilingual and cross-lingual research in NLP.\nLong-form text: The dataset includes full-length novels, short stories, and essays, making it ideal for tasks that require understanding context over longer sequences of text.\n\n\n\n\n\nLanguage modeling: With its vast variety of literary styles and genres, Gutenberg serves as a valuable resource for training and evaluating language models like GPT and BERT. Pre-training on Gutenberg’s diverse text corpus allows models to capture nuanced linguistic patterns, which can later be fine-tuned for more specific NLP tasks.\nText classification: The dataset can be applied to classification tasks such as genre classification or sentiment analysis. Researchers often use Gutenberg to train classifiers that distinguish between literary styles or detect emotional tone in texts.\nSummarization and translation: Due to the diversity in content, Gutenberg is commonly used to test summarization models (e.g., creating concise book summaries) and translation algorithms across different literary forms.\nTopic modeling: The diverse collection of texts allows for the exploration of underlying themes or topics through techniques like Latent Dirichlet Allocation (LDA) or Non-negative Matrix Factorization (NMF), enabling researchers to uncover hidden patterns in the literature.\nMultimodal learning: Paired with Librivox audiobooks, the Gutenberg dataset enables multimodal tasks like text-to-speech synthesis, speech recognition, and aligning spoken text with its written counterpart. This supports the development of models like Tacotron and Wav2Vec.\nTransfer learning: Researchers frequently fine-tune pre-trained language models on Gutenberg to test performance on literary and long-form text, often comparing results with models trained on broader corpora like Common Crawl.\nData augmentation: Gutenberg’s large-scale, structured text is ideal for augmenting smaller datasets and improving model robustness through data imputation or other generalization techniques.\n\n\n\n\n\nLibrivox Audiobook Collection: Provides audiobooks to accompany the texts available in Project Gutenberg.\nCommon Crawl: Large-scale web crawl dataset often used to pre-train language models. Gutenberg can provide a more structured and curated supplement to such datasets."
  },
  {
    "objectID": "Toolbox/Data/Gutenberg.html#loading-data-in-python",
    "href": "Toolbox/Data/Gutenberg.html#loading-data-in-python",
    "title": "Project Gutenberg: Text & Audio Books",
    "section": "Loading data in Python",
    "text": "Loading data in Python\nYou can easily load text data from Project Gutenberg in Python using the gutenbergpy or requests libraries. Here’s a basic example using gutenbergpy:\n\nInstall the gutenbergpy library:\n\n!pip install gutenbergpy\n\nLoad a book from Project Gutenberg You’ll need the Gutenberg Book ID, which you can find by searching the Gutenberg website for the book you want. The Book ID is the number found at the end of the book’s URL. For example, the URL https://www.gutenberg.org/ebooks/1342 corresponds to Pride and Prejudice, and the Book ID is 1342.\n\nfrom gutenbergpy.textget import get_text_by_id\nfrom gutenbergpy.textget import strip_headers\n\n# Replace '1342' with the ID of the book you want to download\nbook_id = 1342\nbook_text = get_text_by_id(book_id)\nbook_text_clean = strip_headers(book_text).strip()\n\n# Print the first 500 characters\nprint(book_text_clean[:500])"
  },
  {
    "objectID": "Toolbox/Data/Gutenberg.html#questions",
    "href": "Toolbox/Data/Gutenberg.html#questions",
    "title": "Project Gutenberg: Text & Audio Books",
    "section": "Questions?",
    "text": "Questions?\nIf you have any lingering questions about this resource, feel free to post them on the ML+X Nexus Q&A on GitHub. We will update this resource as new information or applications arise."
  },
  {
    "objectID": "Toolbox/Data/Gutenberg.html#see-also",
    "href": "Toolbox/Data/Gutenberg.html#see-also",
    "title": "Project Gutenberg: Text & Audio Books",
    "section": "See also",
    "text": "See also\n\nWorkshop: Intro to Text Analysis / NLP: A hands-on introduction to natural language processing and how to extract insights from text data."
  },
  {
    "objectID": "Toolbox/Data/iNaturalist.html",
    "href": "Toolbox/Data/iNaturalist.html",
    "title": "iNaturalist (iNat)",
    "section": "",
    "text": "iNaturalist (iNat) is one of the largest community-contributed biodiversity datasets, with millions of photos of plants, animals, and fungi submitted by citizen scientists around the world. These images are accompanied by rich metadata — including time, location, and species ID — and are verified through community consensus.\nThe iNat2024 (iNat24) subset, used in benchmarks like INQUIRE, includes over 5 million images and provides a real-world testbed for evaluating species classification, visual reasoning, and multimodal retrieval tasks.\n\n\nMost large-scale image datasets are curated or scraped from the internet. In contrast, iNat data reflects real-world ecological observations: images are often messy, off-center, and diverse in setting, but rich in scientific value. This makes iNat an ideal source for developing and testing robust machine learning models.\nResearchers use iNat to:\n\nTrain and evaluate models on fine-grained species classification\n\nBenchmark retrieval tasks involving complex visual and contextual cues\n\nExplore zero-shot generalization using CLIP or other vision-language models\n\nSupport conservation efforts and ecological research at scale\n\n\n\n\niNat images have powered major ML benchmarks and tools, including the INQUIRE benchmark, which uses iNat24 to evaluate retrieval models on expert-level ecological queries\n\n\n\niNat data is accessible through the iNaturalist API, the Global Biodiversity Information Facility (GBIF), and various competition archives. Data licensing follows Creative Commons guidelines, and attribution to individual observers is required."
  },
  {
    "objectID": "Toolbox/Data/iNaturalist.html#about-this-resource",
    "href": "Toolbox/Data/iNaturalist.html#about-this-resource",
    "title": "iNaturalist (iNat)",
    "section": "",
    "text": "iNaturalist (iNat) is one of the largest community-contributed biodiversity datasets, with millions of photos of plants, animals, and fungi submitted by citizen scientists around the world. These images are accompanied by rich metadata — including time, location, and species ID — and are verified through community consensus.\nThe iNat2024 (iNat24) subset, used in benchmarks like INQUIRE, includes over 5 million images and provides a real-world testbed for evaluating species classification, visual reasoning, and multimodal retrieval tasks.\n\n\nMost large-scale image datasets are curated or scraped from the internet. In contrast, iNat data reflects real-world ecological observations: images are often messy, off-center, and diverse in setting, but rich in scientific value. This makes iNat an ideal source for developing and testing robust machine learning models.\nResearchers use iNat to:\n\nTrain and evaluate models on fine-grained species classification\n\nBenchmark retrieval tasks involving complex visual and contextual cues\n\nExplore zero-shot generalization using CLIP or other vision-language models\n\nSupport conservation efforts and ecological research at scale\n\n\n\n\niNat images have powered major ML benchmarks and tools, including the INQUIRE benchmark, which uses iNat24 to evaluate retrieval models on expert-level ecological queries\n\n\n\niNat data is accessible through the iNaturalist API, the Global Biodiversity Information Facility (GBIF), and various competition archives. Data licensing follows Creative Commons guidelines, and attribution to individual observers is required."
  },
  {
    "objectID": "Toolbox/Data/iNaturalist.html#questions",
    "href": "Toolbox/Data/iNaturalist.html#questions",
    "title": "iNaturalist (iNat)",
    "section": "Questions?",
    "text": "Questions?\nIf you’re using iNat or want to ask about its structure or use cases, feel free to post in the ML+X Nexus Q&A forum."
  },
  {
    "objectID": "Toolbox/Data/iNaturalist.html#see-also",
    "href": "Toolbox/Data/iNaturalist.html#see-also",
    "title": "iNaturalist (iNat)",
    "section": "See also",
    "text": "See also\n\nData: INQUIRE Benchmark: Built on iNat24, this benchmark supports multimodal ecological retrieval tasks.\nTalk: Automating Scientific Discovery: Learn how iNat data is being used to train AI systems that support ecological research."
  },
  {
    "objectID": "Toolbox/Data/index.html",
    "href": "Toolbox/Data/index.html",
    "title": "Data",
    "section": "",
    "text": "Explore popular datasets that you can leverage for your next machine learning project.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\niNaturalist (iNat)\n\n\n\nData\n\nImage data\n\nBiology\n\nEcology\n\nComputer vision\n\nMultimodal learning\n\nBenchmarking\n\nCitizen science\n\nCLIP\n\nZero-shot learning\n\n\n\n\n\n\n\nChris Endemann\n\n\n2025-04-03\n\n\n\n\n\n\n\n\n\n\n\n\n\nINQUIRE\n\n\n\nData\n\nMultimodal data\n\nComputer vision\n\nRetrieval\n\nZero-shot learning\n\nBenchmarking\n\nMultimodal learning\n\nViT\n\nLLM\n\nBiology\n\nEcology\n\nImage data\n\nHugging Face\n\n\n\n\n\n\n\nChris Endemann\n\n\n2025-03-26\n\n\n\n\n\n\n\n\n\n\n\n\n\nCIFAR Dataset\n\n\n\nData\n\nImage data\n\nComputer vision\n\nImage classification\n\nCNN\n\nCIFAR\n\n\n\n\n\n\n\nAidan O’Brien\n\n\n2025-03-03\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeaf Vein Dataset (LVD2021)\n\n\n\nData\n\nImage data\n\nPlant phenotyping\n\nImage segmentation\n\nComputer vision\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-10-18\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject Gutenberg: Text & Audio Books\n\n\n\nData\n\nText data\n\nAudio data\n\nMultimodal data\n\nNLP\n\nText analysis\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-10-14\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Toolbox",
      "Data"
    ]
  },
  {
    "objectID": "Toolbox/Compute/index.html",
    "href": "Toolbox/Compute/index.html",
    "title": "Compute",
    "section": "",
    "text": "Explore compute resources (high-throughput, GPUs, etc.) for your next ML project!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to AWS SageMaker for Predictive ML/AI\n\n\n\nWorkshops\n\nCode-along\n\nCarpentries\n\nCompute\n\nAWS\n\nGPU\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-11-07\n\n\n\n\n\n\n\n\n\n\n\n\n\nCenter for High Throughput Computing (CHTC)\n\n\n\nCompute\n\nGPU\n\nCHTC\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-06-25\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Toolbox",
      "Compute"
    ]
  },
  {
    "objectID": "Toolbox/Libraries/MONAI.html",
    "href": "Toolbox/Libraries/MONAI.html",
    "title": "MONAI: Medical Open Network for AI",
    "section": "",
    "text": "MONAI (Medical Open Network for AI) is an open-source, community-supported framework for deep learning in healthcare imaging. Built on top of PyTorch, it provides specialized functionality for medical imaging tasks, including pre-trained models, data loaders, and evaluation metrics tailored for the medical domain. MONAI is particularly valuable for researchers and practitioners looking to leverage deep learning for medical imaging applications due to its flexibility, performance, and ease of integration with existing workflows.\n\n\n\nIntroduction to Deep Learning with PyTorch"
  },
  {
    "objectID": "Toolbox/Libraries/MONAI.html#about-this-resource",
    "href": "Toolbox/Libraries/MONAI.html#about-this-resource",
    "title": "MONAI: Medical Open Network for AI",
    "section": "",
    "text": "MONAI (Medical Open Network for AI) is an open-source, community-supported framework for deep learning in healthcare imaging. Built on top of PyTorch, it provides specialized functionality for medical imaging tasks, including pre-trained models, data loaders, and evaluation metrics tailored for the medical domain. MONAI is particularly valuable for researchers and practitioners looking to leverage deep learning for medical imaging applications due to its flexibility, performance, and ease of integration with existing workflows.\n\n\n\nIntroduction to Deep Learning with PyTorch"
  },
  {
    "objectID": "Toolbox/Libraries/MONAI.html#questions",
    "href": "Toolbox/Libraries/MONAI.html#questions",
    "title": "MONAI: Medical Open Network for AI",
    "section": "Questions?",
    "text": "Questions?\nIf you have any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Toolbox/Libraries/MONAI.html#see-also",
    "href": "Toolbox/Libraries/MONAI.html#see-also",
    "title": "MONAI: Medical Open Network for AI",
    "section": "See also",
    "text": "See also\n\nMONAI Tutorials\nMIDeL — Medical Image Deep Learning\nMONAI Bootcamp 2021 (Video Series)"
  },
  {
    "objectID": "Toolbox/Libraries/index.html",
    "href": "Toolbox/Libraries/index.html",
    "title": "Libraries",
    "section": "",
    "text": "Explore popular libraries and frameworks that you can leverage for your next AI/ML project.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPytesseract: OCR with Tesseract (LSTM) in Python\n\n\n\nLibraries\n\nOCR\n\nNLP\n\nComputer vision\n\nText extraction\n\nMultilingual\n\nDeep learning\n\nLSTM\n\n\n\n\n\n\n\nChris Endemann\n\n\n2025-04-05\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeepForest: A Toolkit of Models for Tree and Wildlife Detection in Aerial Imagery\n\n\n\nModels\n\nModel exploration\n\nObject detection\n\nRemote sensing\n\nEcology\n\nForest monitoring\n\nGeospatial data\n\nComputer vision\n\nDeep learning\n\nCNN\n\n\n\n\n\n\n\nChris Endemann\n\n\n2025-04-04\n\n\n\n\n\n\n\n\n\n\n\n\n\nAI Fairness 360 (AIF360)\n\n\n\nLibraries\n\nAIF360\n\nFairness\n\nEthical AI\n\nTrustworthy AI\n\nBias\n\nSklearn\n\nTabular\n\nComputer vision\n\nNLP\n\nText analysis\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-12-03\n\n\n\n\n\n\n\n\n\n\n\n\n\nPyTorch-OOD\n\n\n\nLibraries\n\nPyTorch-OOD\n\nOOD detection\n\nPyTorch\n\nAnomaly detection\n\nNovelty detection\n\nOpen-set recognition\n\nTrustworthy AI\n\nEthical AI\n\nDeep learning\n\nComputer vision\n\nNLP\n\nTabular\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-12-03\n\n\n\n\n\n\n\n\n\n\n\n\n\nKornia\n\n\n\nLibraries\n\nModel exploration\n\nComputer vision\n\nDeep learning\n\nPyTorch\n\nImage preprocessing\n\nImage segmentation\n\nViT\n\nSAM\n\nLoFTR\n\nRT-DETR\n\n\n\n\n\n\n\nRadi Akbar\n\n\n2024-10-29\n\n\n\n\n\n\n\n\n\n\n\n\n\nMONAI: Medical Open Network for AI\n\n\n\nLibraries\n\nDeep learning\n\nPyTorch\n\nMedical imaging\n\nModel exploration\n\n\n\n\n\n\n\nAlan McMillan\n\n\n2024-08-14\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Machine Learning with Sklearn\n\n\n\nWorkshops\n\nLibraries\n\nClassical ML\n\nBoosting\n\nDecision trees\n\nSVM\n\nClustering\n\nRegression\n\nSklearn\n\nCode-along\n\nCarpentries\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-17\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Deep Learning with Keras\n\n\n\nWorkshops\n\nLibraries\n\nDeep learning\n\nKeras\n\nCode-along\n\nCarpentries\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-16\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Deep Learning with PyTorch\n\n\n\nWorkshops\n\nLibraries\n\nDeep learning\n\nPyTorch\n\nUdacity\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-15\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Toolbox",
      "Libraries"
    ]
  },
  {
    "objectID": "Toolbox/Libraries/PyTorch-OOD.html",
    "href": "Toolbox/Libraries/PyTorch-OOD.html",
    "title": "PyTorch-OOD",
    "section": "",
    "text": "PyTorch-OOD is an open-source Python library designed for out-of-distribution (OOD) detection and related tasks like anomaly detection, novelty detection, and open-set recognition. It provides a flexible framework for training, testing, and benchmarking OOD detection methods with PyTorch models. With a standardized workflow, the library enables reproducible research and easy integration into existing deep learning pipelines.\nPyTorch-OOD supports a range of detectors, datasets, and benchmarks, allowing researchers to experiment with various OOD methods across multiple domains, including image classification, segmentation, and text classification.\n\n\n\nComprehensive detector support:\n\nProbability-based: Maximum Softmax Probability (MSP), Monte Carlo Dropout (MCD).\nLogit-based: Energy-Based OOD detection (EBO), Temperature Scaling.\nFeature-based: Mahalanobis Distance (MD), k-Nearest Neighbors (kNN).\nActivation pruning: ReAct, Activation Shaping (ASH).\n\nDataset integration: Auto-downloadable access to popular OOD datasets (e.g., CIFAR10-C, ImageNet-O, iNaturalist).\nBenchmarking tools: Evaluate OOD detectors on standardized benchmarks for classification, segmentation, and text data.\nUnified API: Common interface for all detectors with fit and predict methods, ensuring consistency and ease of use.\nExtensibility: Modular design allows adding new datasets, detectors, and workflows."
  },
  {
    "objectID": "Toolbox/Libraries/PyTorch-OOD.html#about-this-library",
    "href": "Toolbox/Libraries/PyTorch-OOD.html#about-this-library",
    "title": "PyTorch-OOD",
    "section": "",
    "text": "PyTorch-OOD is an open-source Python library designed for out-of-distribution (OOD) detection and related tasks like anomaly detection, novelty detection, and open-set recognition. It provides a flexible framework for training, testing, and benchmarking OOD detection methods with PyTorch models. With a standardized workflow, the library enables reproducible research and easy integration into existing deep learning pipelines.\nPyTorch-OOD supports a range of detectors, datasets, and benchmarks, allowing researchers to experiment with various OOD methods across multiple domains, including image classification, segmentation, and text classification.\n\n\n\nComprehensive detector support:\n\nProbability-based: Maximum Softmax Probability (MSP), Monte Carlo Dropout (MCD).\nLogit-based: Energy-Based OOD detection (EBO), Temperature Scaling.\nFeature-based: Mahalanobis Distance (MD), k-Nearest Neighbors (kNN).\nActivation pruning: ReAct, Activation Shaping (ASH).\n\nDataset integration: Auto-downloadable access to popular OOD datasets (e.g., CIFAR10-C, ImageNet-O, iNaturalist).\nBenchmarking tools: Evaluate OOD detectors on standardized benchmarks for classification, segmentation, and text data.\nUnified API: Common interface for all detectors with fit and predict methods, ensuring consistency and ease of use.\nExtensibility: Modular design allows adding new datasets, detectors, and workflows."
  },
  {
    "objectID": "Toolbox/Libraries/PyTorch-OOD.html#integration-and-compatibility",
    "href": "Toolbox/Libraries/PyTorch-OOD.html#integration-and-compatibility",
    "title": "PyTorch-OOD",
    "section": "Integration and compatibility",
    "text": "Integration and compatibility\nPyTorch-OOD is fully compatible with PyTorch models and workflows. It provides a common interface for initializing and evaluating OOD detectors, making it a seamless addition to deep learning projects.\n\nFrameworks supported: PyTorch\nInstallation instructions:\n\nFor basic installation: pip install pytorch-ood\nFor the latest development branch:\npip install git+ssh://git@github.com/kkirchheim/pytorch-ood.git@dev"
  },
  {
    "objectID": "Toolbox/Libraries/PyTorch-OOD.html#extending-pytorch-ood-for-various-domains",
    "href": "Toolbox/Libraries/PyTorch-OOD.html#extending-pytorch-ood-for-various-domains",
    "title": "PyTorch-OOD",
    "section": "Extending PyTorch-OOD for various domains",
    "text": "Extending PyTorch-OOD for various domains\nPyTorch-OOD’s modular design and dataset integration make it versatile across different domains:\n\nComputer vision: OOD detection for image classification and segmentation tasks using datasets like TinyImageNet and LSUN. Feature-based methods like Mahalanobis Distance can enhance robustness.\nNatural language processing (NLP): Evaluate text models for OOD detection using datasets like 20 Newsgroups and Reuters. Text embeddings and logit-based detectors (e.g., Temperature Scaling) can be used for effective OOD classification.\nTabular data: Simplify OOD detection in structured datasets by leveraging feature-based or probability-based detectors."
  },
  {
    "objectID": "Toolbox/Libraries/PyTorch-OOD.html#why-use-pytorch-ood",
    "href": "Toolbox/Libraries/PyTorch-OOD.html#why-use-pytorch-ood",
    "title": "PyTorch-OOD",
    "section": "Why use PyTorch-OOD?",
    "text": "Why use PyTorch-OOD?\n\nStandardized workflows: Provides a clear three-step process: train a model, initialize an OOD detector, and evaluate performance.\nReproducibility: Benchmark datasets and unified APIs ensure reproducible research.\nVersatility: Applicable across multiple domains, including computer vision, NLP, and tabular data.\nEducational value: An excellent resource for teaching OOD concepts and methods in machine learning."
  },
  {
    "objectID": "Toolbox/Libraries/PyTorch-OOD.html#use-cases",
    "href": "Toolbox/Libraries/PyTorch-OOD.html#use-cases",
    "title": "PyTorch-OOD",
    "section": "Use cases",
    "text": "Use cases\n\nHealthcare: Detecting anomalies in medical images.\nFinance: Identifying novel fraud patterns in transaction datasets.\nAutonomous driving: Segmenting unexpected obstacles in road scenarios.\nNLP: Evaluating robustness of sentiment analysis models to out-of-vocabulary inputs.\nMany others: Use the “Improve this page” button to add a use-case!"
  },
  {
    "objectID": "Toolbox/Libraries/PyTorch-OOD.html#tutorials-and-resources",
    "href": "Toolbox/Libraries/PyTorch-OOD.html#tutorials-and-resources",
    "title": "PyTorch-OOD",
    "section": "Tutorials and resources",
    "text": "Tutorials and resources\n\nGetting started guide: PyTorch-OOD Documentation"
  },
  {
    "objectID": "Toolbox/Libraries/PyTorch-OOD.html#questions",
    "href": "Toolbox/Libraries/PyTorch-OOD.html#questions",
    "title": "PyTorch-OOD",
    "section": "Questions?",
    "text": "Questions?\nFor more assistance, visit the GitHub repository or join the community for discussions and support."
  },
  {
    "objectID": "Toolbox/Libraries/PyTorch-OOD.html#see-also",
    "href": "Toolbox/Libraries/PyTorch-OOD.html#see-also",
    "title": "PyTorch-OOD",
    "section": "See also",
    "text": "See also\n\nTalk: Intro to Out-of-Distribution Detection: Learn more about the pervasive problem of out-of-distribution data, and techniques available to mitigate this problem.\nTalk: Trustworthy LLMs & Ethical AI: Learn how DeTox can be used to remove toxic embeddings in LLMs.\nWorkshop: Trustworthy AI: Explainability, Bias, Fairness, and Safety: A beginner-friendly workshop on Trustworthy AI/ML concepts and mitigation tools, including AIF360, OOD detection, and explainability methods.\nLibrary: AI Fairness 360 (AIF360): AI Fairness 360 (AIF360) is a scikit-learn-compatible open-source Python library designed to detect and mitigate bias in machine learning models"
  },
  {
    "objectID": "Toolbox/index.html",
    "href": "Toolbox/index.html",
    "title": "Toolbox",
    "section": "",
    "text": "Explore popular pretrained & foundation models, useful scripts/libraries, and datasets that you can leverage for your next ML project. Learn about their features, how to use them effectively, and see examples of them in action.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPytesseract: OCR with Tesseract (LSTM) in Python\n\n\n\nLibraries\n\nOCR\n\nNLP\n\nComputer vision\n\nText extraction\n\nMultilingual\n\nDeep learning\n\nLSTM\n\n\n\n\n\n\n\nChris Endemann\n\n\n2025-04-05\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeepForest: A Toolkit of Models for Tree and Wildlife Detection in Aerial Imagery\n\n\n\nModels\n\nModel exploration\n\nObject detection\n\nRemote sensing\n\nEcology\n\nForest monitoring\n\nGeospatial data\n\nComputer vision\n\nDeep learning\n\nCNN\n\n\n\n\n\n\n\nChris Endemann\n\n\n2025-04-04\n\n\n\n\n\n\n\n\n\n\n\n\n\niNaturalist (iNat)\n\n\n\nData\n\nImage data\n\nBiology\n\nEcology\n\nComputer vision\n\nMultimodal learning\n\nBenchmarking\n\nCitizen science\n\nCLIP\n\nZero-shot learning\n\n\n\n\n\n\n\nChris Endemann\n\n\n2025-04-03\n\n\n\n\n\n\n\n\n\n\n\n\n\nINQUIRE\n\n\n\nData\n\nMultimodal data\n\nComputer vision\n\nRetrieval\n\nZero-shot learning\n\nBenchmarking\n\nMultimodal learning\n\nViT\n\nLLM\n\nBiology\n\nEcology\n\nImage data\n\nHugging Face\n\n\n\n\n\n\n\nChris Endemann\n\n\n2025-03-26\n\n\n\n\n\n\n\n\n\n\n\n\n\nCIFAR Dataset\n\n\n\nData\n\nImage data\n\nComputer vision\n\nImage classification\n\nCNN\n\nCIFAR\n\n\n\n\n\n\n\nAidan O’Brien\n\n\n2025-03-03\n\n\n\n\n\n\n\n\n\n\n\n\n\nUW Generative AI Services & Policies\n\n\n\nGenAI\n\nMicrosoft Copilot\n\nGemini\n\nWebex\n\nZoom\n\nUW-Madison\n\nLLM\n\nFoundation models\n\n\n\n\n\n\n\nChris Endemann\n\n\n2025-01-07\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotebookLM: A GenAI Summarization Tool\n\n\n\nGenAI\n\nNLP\n\nSummarization\n\nGemini\n\nLLM\n\nFoundation models\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-12-09\n\n\n\n\n\n\n\n\n\n\n\n\n\nAI Fairness 360 (AIF360)\n\n\n\nLibraries\n\nAIF360\n\nFairness\n\nEthical AI\n\nTrustworthy AI\n\nBias\n\nSklearn\n\nTabular\n\nComputer vision\n\nNLP\n\nText analysis\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-12-03\n\n\n\n\n\n\n\n\n\n\n\n\n\nPyTorch-OOD\n\n\n\nLibraries\n\nPyTorch-OOD\n\nOOD detection\n\nPyTorch\n\nAnomaly detection\n\nNovelty detection\n\nOpen-set recognition\n\nTrustworthy AI\n\nEthical AI\n\nDeep learning\n\nComputer vision\n\nNLP\n\nTabular\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-12-03\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to AWS SageMaker for Predictive ML/AI\n\n\n\nWorkshops\n\nCode-along\n\nCarpentries\n\nCompute\n\nAWS\n\nGPU\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-11-07\n\n\n\n\n\n\n\n\n\n\n\n\n\nKornia\n\n\n\nLibraries\n\nModel exploration\n\nComputer vision\n\nDeep learning\n\nPyTorch\n\nImage preprocessing\n\nImage segmentation\n\nViT\n\nSAM\n\nLoFTR\n\nRT-DETR\n\n\n\n\n\n\n\nRadi Akbar\n\n\n2024-10-29\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeaf Vein Dataset (LVD2021)\n\n\n\nData\n\nImage data\n\nPlant phenotyping\n\nImage segmentation\n\nComputer vision\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-10-18\n\n\n\n\n\n\n\n\n\n\n\n\n\nXGBoost: Tree-Based Gradient Boosting for Tabular Data\n\n\n\nModels\n\nBoosting\n\nXGBoost\n\nDecision trees\n\nTabular\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-10-17\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject Gutenberg: Text & Audio Books\n\n\n\nData\n\nText data\n\nAudio data\n\nMultimodal data\n\nNLP\n\nText analysis\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-10-14\n\n\n\n\n\n\n\n\n\n\n\n\n\nU-Net: Convolutional Networks for Biomedical Image Segmentation\n\n\n\nModels\n\nDeep learning\n\nMedical imaging\n\nImage segmentation\n\nCNN\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-09-16\n\n\n\n\n\n\n\n\n\n\n\n\n\nMONAI: Medical Open Network for AI\n\n\n\nLibraries\n\nDeep learning\n\nPyTorch\n\nMedical imaging\n\nModel exploration\n\n\n\n\n\n\n\nAlan McMillan\n\n\n2024-08-14\n\n\n\n\n\n\n\n\n\n\n\n\n\nCenter for High Throughput Computing (CHTC)\n\n\n\nCompute\n\nGPU\n\nCHTC\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-06-25\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Toolbox"
    ]
  },
  {
    "objectID": "Toolbox/GenAI/NotebookLM.html",
    "href": "Toolbox/GenAI/NotebookLM.html",
    "title": "NotebookLM: A GenAI Summarization Tool",
    "section": "",
    "text": "NotebookLM is a generative AI tool designed to assist with understanding dense, complex texts, including research papers in machine learning and AI. Developed by Google Labs and powered by the Gemini 1.5 Pro large language model (LLM), NotebookLM leverages advanced AI capabilities to provide summaries, answer questions, and generate audio versions of summaries. This makes it an invaluable resource for practitioners looking to dive into highly technical material.\n\n\nUW–Madison faculty, staff, students, and affiliates are required to follow campus policies relevant to AI use. Uses of generative AI that are explicitly prohibited by policy include, but are not limited to, the following:\n\nEntering any sensitive, restricted or otherwise protected institutional data – including hard-coded passwords – into any generative AI tool or service;\nUsing AI-generated code for institutional IT systems or services without review by a human to verify the absence of malicious elements;\nUsing generative AI to violate laws; institutional policies, rules or guidelines; or agreements or contracts.\n\n\n\n\n\nSummarization (Text and Audio): Quickly generate concise textual summaries of dense ML/AI papers and convert them into a podcast-like audio summary. As an example, check out this audio summary of the paper, Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters, by researchers at DeepMind.\nInteractive Q&A: Ask specific questions about uploaded papers and receive detailed answers tailored to the content of the document.\nGemini 1.5 Pro’s strengths: Provides advanced contextual understanding and reasoning across diverse content, making it well-suited for technical and research documents.\nGenerative flexibility: Capable of handling complex queries and summarization tasks without requiring fine-tuning for specific domains.\n\n\n\n\n\nIterative interaction: Don’t hesitate to refine questions or dig deeper into concepts for thorough understanding.\nOver-reliance on summaries: While summaries are helpful, always cross-check details in the original document for accuracy.\nData privacy: Avoid uploading proprietary or sensitive documents, as NotebookLM’s privacy practices are tied to Google’s terms of use."
  },
  {
    "objectID": "Toolbox/GenAI/NotebookLM.html#about-this-resource",
    "href": "Toolbox/GenAI/NotebookLM.html#about-this-resource",
    "title": "NotebookLM: A GenAI Summarization Tool",
    "section": "",
    "text": "NotebookLM is a generative AI tool designed to assist with understanding dense, complex texts, including research papers in machine learning and AI. Developed by Google Labs and powered by the Gemini 1.5 Pro large language model (LLM), NotebookLM leverages advanced AI capabilities to provide summaries, answer questions, and generate audio versions of summaries. This makes it an invaluable resource for practitioners looking to dive into highly technical material.\n\n\nUW–Madison faculty, staff, students, and affiliates are required to follow campus policies relevant to AI use. Uses of generative AI that are explicitly prohibited by policy include, but are not limited to, the following:\n\nEntering any sensitive, restricted or otherwise protected institutional data – including hard-coded passwords – into any generative AI tool or service;\nUsing AI-generated code for institutional IT systems or services without review by a human to verify the absence of malicious elements;\nUsing generative AI to violate laws; institutional policies, rules or guidelines; or agreements or contracts.\n\n\n\n\n\nSummarization (Text and Audio): Quickly generate concise textual summaries of dense ML/AI papers and convert them into a podcast-like audio summary. As an example, check out this audio summary of the paper, Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters, by researchers at DeepMind.\nInteractive Q&A: Ask specific questions about uploaded papers and receive detailed answers tailored to the content of the document.\nGemini 1.5 Pro’s strengths: Provides advanced contextual understanding and reasoning across diverse content, making it well-suited for technical and research documents.\nGenerative flexibility: Capable of handling complex queries and summarization tasks without requiring fine-tuning for specific domains.\n\n\n\n\n\nIterative interaction: Don’t hesitate to refine questions or dig deeper into concepts for thorough understanding.\nOver-reliance on summaries: While summaries are helpful, always cross-check details in the original document for accuracy.\nData privacy: Avoid uploading proprietary or sensitive documents, as NotebookLM’s privacy practices are tied to Google’s terms of use."
  },
  {
    "objectID": "Toolbox/GenAI/NotebookLM.html#questions",
    "href": "Toolbox/GenAI/NotebookLM.html#questions",
    "title": "NotebookLM: A GenAI Summarization Tool",
    "section": "Questions?",
    "text": "Questions?\nHave questions about using NotebookLM? Join the discussion on the ML+X Nexus Q&A."
  },
  {
    "objectID": "Toolbox/GenAI/NotebookLM.html#see-also",
    "href": "Toolbox/GenAI/NotebookLM.html#see-also",
    "title": "NotebookLM: A GenAI Summarization Tool",
    "section": "See also",
    "text": "See also\n\nUW Generative AI Services & Policies: Learn about enterprise genAI tools supported by DoIT/UW-Madison."
  }
]