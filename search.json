[
  {
    "objectID": "Applications/Blogs/index.html",
    "href": "Applications/Blogs/index.html",
    "title": "Blogs",
    "section": "",
    "text": "Share your ML story!\nAre you currently immersed in an exciting ML project? We want to hear about it! Share your insights, challenges, and successes by contributing a blog post to Nexus, the ML+X resource sharing platform.\nWhether you’re exploring ML applications in biology, engineering, social sciences, or any other field, your unique perspective is invaluable. Showcase your innovation, research, and creativity to inspire others in the ML+X community.\nGet started by posting a brief summary of your blog idea as an Issue on GitHub! The Nexus development team will follow-up with further instructions.\n\n\nExplore blogs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat Tune Is That? A Humanities Application of Deep Learning\n\n\n\n\n\n\nBlogs\n\n\nDeep learning\n\n\nConformer\n\n\nTransformer\n\n\nCNN\n\n\nHumanities\n\n\nAudio\n\n\nMusic\n\n\nCSI\n\n\nTime-series\n\n\n\n\n\n\n\n\n\n2024-09-11\n\n\nAlan Ng\n\n\n14 min\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Applications",
      "Blogs"
    ]
  },
  {
    "objectID": "Applications/index.html",
    "href": "Applications/index.html",
    "title": "Applications",
    "section": "",
    "text": "Discover a curated collection of blogs, papers, and talks which dive into ML applications and lessons learned by practitioners.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring the Titanic Dataset\n\n\n\n\n\n\nEDA\n\n\nTabular\n\n\n\n\n\n\n\n\n\n2024-10-07\n\n\nChris Endemann\n\n\n\n\n\n\n\n\n\n\n\n\nVision, Language, and Vision-Language Modeling in Radiology\n\n\n\n\n\n\nVideos\n\n\nML4MI\n\n\nMedical imaging\n\n\nVLM\n\n\nViT\n\n\nUNET\n\n\nLLaVA\n\n\nComputer vision\n\n\nCNN\n\n\nLLM\n\n\nDeep learning\n\n\nMultimodal learning\n\n\n\nIn this ML4MI seminar, Tyler Bradshaw highlights the history and current use of vision (e.g., UNET), language, and vision-language models in medical imaging.\n\n\n\n\n\n2024-09-16\n\n\nTyler Bradshaw, PhD\n\n\n\n\n\n\n\n\n\n\n\n\nWhat Tune Is That? A Humanities Application of Deep Learning\n\n\n\n\n\n\nBlogs\n\n\nDeep learning\n\n\nConformer\n\n\nTransformer\n\n\nCNN\n\n\nHumanities\n\n\nAudio\n\n\nMusic\n\n\nCSI\n\n\nTime-series\n\n\n\n\n\n\n\n\n\n2024-09-11\n\n\nAlan Ng\n\n\n\n\n\n\n\n\n\n\n\n\nA Biophysics-based Protein Language Model for Protein Engineering\n\n\n\n\n\n\nVideos\n\n\nCross Labs AI\n\n\nTransfer learning\n\n\nBiophysics\n\n\nProtein language models\n\n\nFoundation models\n\n\nLLM\n\n\nDeep learning\n\n\nProtein engineering\n\n\nSimulations\n\n\n\nWe introduce Mutational Effect Transfer Learning (METL), a specialized protein language model that bridges the gap between traditional biophysics-based and machine learning approaches by incorporating synthetic data from molecular simulations.\n\n\n\n\n\n2024-06-18\n\n\nSam Gelman, PhD\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Large Language Models for Meteorological Fact Finding\n\n\n\n\n\n\nVideos\n\n\nLLM\n\n\nMeteorology\n\n\n\nThis talk demonstrates harnessing the power of AI to open new avenues in data analysis, including for meteorological fact-finding. Discover how cutting-edge large language models (LLMs) like OpenAI’s ChatGPT 3.5 and 4.0 hold are poised to help the field of meteorological data analysis. \n\n\n\n\n\n2024-05-30\n\n\nZekai Otles\n\n\n\n\n\n\n\n\n\n\n\n\nAdvancing Healthcare and Agriculture through Computer Vision\n\n\n\n\n\n\nVideos\n\n\nML+X\n\n\nComputer vision\n\n\nUltrasound\n\n\nMedical imaging\n\n\nAgriculture\n\n\nLSTM\n\n\nCNN-LSTM\n\n\nCNN\n\n\nDeep learning\n\n\n\n1. An ultrasound-based method to measure knee kinematics enabled by deep learning 2. Plant breeding in the age of computer vision \n\n\n\n\n\n2024-04-09\n\n\nMatthew Blomquist, Will de la Bretonne\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Model Sharing in the Age of Foundation Models\n\n\n\n\n\n\nVideos\n\n\nML+X\n\n\nMultimodal learning\n\n\nFoundation models\n\n\nModel sharing\n\n\nHugging Face\n\n\nLLM\n\n\nLMM\n\n\nLLaVA\n\n\nDeep learning\n\n\n\n1. Model sharing and reproducible ML 2. LLaVA-NeXT and model sharing \n\n\n\n\n\n2024-03-12\n\n\nChris Endemann, Haotian Liu, PhD\n\n\n\n\n\n\n\n\n\n\n\n\nNavigating Gravitational Waves with AI Insights\n\n\n\n\n\n\nVideos\n\n\nML+X\n\n\nPhysics\n\n\nSimulations\n\n\n\n1. Welcome and small group discussions 2. Classifying gravitational wave modes from core-collapse supernovae \n\n\n\n\n\n2024-02-13\n\n\nChris Endemann, Bella Finkel\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Science Communication and Drug Synergy Analysis using GPT\n\n\n\n\n\n\nVideos\n\n\nML+X\n\n\nScience communication\n\n\nHealthcare\n\n\nDrug synergy\n\n\nLLM\n\n\nText mining\n\n\n\n1. GPT for Science Communication: User-Interface and Developer Pipeline Approaches 2. Advancing Biomedical Research with GPT-4: A Novel Approach to Drug Synergy Analysis using Text Mining and Classification \n\n\n\n\n\n2023-12-12\n\n\nBen Rush, PhD, Jack Freeman\n\n\n\n\n\n\n\n\n\n\n\n\nWorld Knowledge in the Time of Large Models\n\n\n\n\n\n\nVideos\n\n\nSILO\n\n\nVLM\n\n\nLLM\n\n\nLMM\n\n\nMultimodal learning\n\n\nFoundation models\n\n\n\nThis talk will discuss the massive shift that has come about in the vision and ML community as a result of the large pre-trained language and language and vision models such as Flamingo, GPT-4, and other models.\n\n\n\n\n\n2023-11-22\n\n\nKenneth Marino, PhD\n\n\n\n\n\n\n\n\n\n\n\n\nLLMS in Genomic and Health Coaching\n\n\n\n\n\n\nVideos\n\n\nML+X\n\n\nHealthcare\n\n\nClustering\n\n\nDeep learning\n\n\nLLM\n\n\nGenomics\n\n\n\n1. Clustering of genomic sequences of mycoviruses using deep learning 2. Spurring self-improvement and intrinsic motivation using LLMs and reinforcement learning \n\n\n\n\n\n2023-11-07\n\n\nRohan Sontahlia, Michael Roytman\n\n\n\n\n\n\n\n\n\n\n\n\nTime-Series Analysis\n\n\n\n\n\n\nVideos\n\n\nML+X\n\n\nTime-series\n\n\nGenomics\n\n\nHealthcare\n\n\n\n1. Computational Methods for Comparative Time Clocks in Early Development and Tissue Regeneration 2. Controlled Differential Equations on Long Sequences via Non-standard Wavelets \n\n\n\n\n\n2023-10-10\n\n\nPeng Jiang, PhD, Sourav Pal\n\n\n\n\n\n\n\n\n\n\n\n\nMultimodal Learning\n\n\n\n\n\n\nVideos\n\n\nML+X\n\n\nMultimodal learning\n\n\nDeep learning\n\n\nComputer vision\n\n\nHealthcare\n\n\nGenomics\n\n\n\n1. Multimodal learning and analysis for understanding single-cell functional genomics in brains and brain diseases 2. Transforming healthcare: AI-enhanced disease quantification with vision-language models 3. The benefits of early fusion: deeply integrated audio-visual representation learning \n\n\n\n\n\n2023-09-19\n\n\nDaifeng Wang, PhD, Zachary Huemann, Pedro Morgado, PhD\n\n\n\n\n\n\n\n\n\n\n\n\nExploring AI at UW-Madison\n\n\n\n\n\n\nMultidisciplinary\n\n\nUW-Madison\n\n\nPlaylists\n\n\n\nA summer 2023 webinar series sponsored by the Division of Information Technology and the Data Science Institute.\n\n\n\n\n\n2023-06-23\n\n\nChris Endemann\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Applications"
    ]
  },
  {
    "objectID": "Applications/EDA/Titanic-Dataset.html#step-0-looking-up-each-feature",
    "href": "Applications/EDA/Titanic-Dataset.html#step-0-looking-up-each-feature",
    "title": "Exploring the Titanic Dataset",
    "section": "Step 0: Looking Up Each Feature",
    "text": "Step 0: Looking Up Each Feature\nBefore diving into the analysis, it’s important to understand what each feature in the dataset represents. This ensures we’re interpreting the data correctly and allows us to make informed decisions during the analysis.\n\nTitanic Dataset Features:\n\nsurvived: Whether the passenger survived (0 = No, 1 = Yes).\npclass: Ticket class (1 = 1st, 2 = 2nd, 3 = 3rd).\nsex: Gender of the passenger.\nage: Age of the passenger in years. Some values are missing.\nsibsp: Number of siblings/spouses aboard the Titanic.\nparch: Number of parents/children aboard the Titanic.\nfare: Passenger fare.\nembarked: Port of embarkation (C = Cherbourg; Q = Queenstown; S = Southampton).\nclass: Duplicate of ‘pclass’ (used for plotting by Seaborn).\nwho: Describes whether the passenger is a man, woman, or child.\nadult_male: Indicates whether the passenger is an adult male (True/False).\ndeck: The deck the passenger was on (missing for many passengers).\nembark_town: The name of the town where the passenger boarded.\nalive: Indicator of whether the passenger survived (Yes/No, derived from ‘survived’).\nalone: Indicates whether the passenger was traveling alone (True/False)."
  },
  {
    "objectID": "Applications/EDA/Titanic-Dataset.html#step-1-visualizing-data-in-its-rawest-form",
    "href": "Applications/EDA/Titanic-Dataset.html#step-1-visualizing-data-in-its-rawest-form",
    "title": "Exploring the Titanic Dataset",
    "section": "Step 1: Visualizing data in its rawest form",
    "text": "Step 1: Visualizing data in its rawest form\nLet’s take a look at a small sample of the dataset to understand the raw data we’re working with. This gives us a chance to spot obvious issues or patterns.\n\nImport libraries and load dataset\n\n# Import necessary libraries\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load the Titanic dataset from seaborn\ndf = sns.load_dataset('titanic')\n\n# Display the first few rows of the dataset\ndf.head()\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n\n\n\n\n\n\n# Show a small random sample of the data\nprint(\"Sample of 30 passengers:\")\ndf.sample(30)\n\nSample of 30 passengers:\n\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n454\n0\n3\nmale\nNaN\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n768\n0\n3\nmale\nNaN\n1\n0\n24.1500\nQ\nThird\nman\nTrue\nNaN\nQueenstown\nno\nFalse\n\n\n663\n0\n3\nmale\n36.0\n0\n0\n7.4958\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n169\n0\n3\nmale\n28.0\n0\n0\n56.4958\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n85\n1\n3\nfemale\n33.0\n3\n0\n15.8500\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nFalse\n\n\n503\n0\n3\nfemale\n37.0\n0\n0\n9.5875\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nno\nTrue\n\n\n848\n0\n2\nmale\n28.0\n0\n1\n33.0000\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n309\n1\n1\nfemale\n30.0\n0\n0\n56.9292\nC\nFirst\nwoman\nFalse\nE\nCherbourg\nyes\nTrue\n\n\n547\n1\n2\nmale\nNaN\n0\n0\n13.8625\nC\nSecond\nman\nTrue\nNaN\nCherbourg\nyes\nTrue\n\n\n767\n0\n3\nfemale\n30.5\n0\n0\n7.7500\nQ\nThird\nwoman\nFalse\nNaN\nQueenstown\nno\nTrue\n\n\n561\n0\n3\nmale\n40.0\n0\n0\n7.8958\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n590\n0\n3\nmale\n35.0\n0\n0\n7.1250\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n582\n0\n2\nmale\n54.0\n0\n0\n26.0000\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n615\n1\n2\nfemale\n24.0\n1\n2\n65.0000\nS\nSecond\nwoman\nFalse\nNaN\nSouthampton\nyes\nFalse\n\n\n272\n1\n2\nfemale\n41.0\n0\n1\n19.5000\nS\nSecond\nwoman\nFalse\nNaN\nSouthampton\nyes\nFalse\n\n\n640\n0\n3\nmale\n20.0\n0\n0\n7.8542\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n186\n1\n3\nfemale\nNaN\n1\n0\n15.5000\nQ\nThird\nwoman\nFalse\nNaN\nQueenstown\nyes\nFalse\n\n\n463\n0\n2\nmale\n48.0\n0\n0\n13.0000\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n230\n1\n1\nfemale\n35.0\n1\n0\n83.4750\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n101\n0\n3\nmale\nNaN\n0\n0\n7.8958\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n203\n0\n3\nmale\n45.5\n0\n0\n7.2250\nC\nThird\nman\nTrue\nNaN\nCherbourg\nno\nTrue\n\n\n371\n0\n3\nmale\n18.0\n1\n0\n6.4958\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n662\n0\n1\nmale\n47.0\n0\n0\n25.5875\nS\nFirst\nman\nTrue\nE\nSouthampton\nno\nTrue\n\n\n472\n1\n2\nfemale\n33.0\n1\n2\n27.7500\nS\nSecond\nwoman\nFalse\nNaN\nSouthampton\nyes\nFalse\n\n\n165\n1\n3\nmale\n9.0\n0\n2\n20.5250\nS\nThird\nchild\nFalse\nNaN\nSouthampton\nyes\nFalse\n\n\n878\n0\n3\nmale\nNaN\n0\n0\n7.8958\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n231\n0\n3\nmale\n29.0\n0\n0\n7.7750\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n461\n0\n3\nmale\n34.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n801\n1\n2\nfemale\n31.0\n1\n1\n26.2500\nS\nSecond\nwoman\nFalse\nNaN\nSouthampton\nyes\nFalse\n\n\n666\n0\n2\nmale\n25.0\n0\n0\n13.0000\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n\n\n\n\n\n\n\nInsights\n\nWe see various features such as age, sex, class, fare, and whether the passenger survived.\nThis helps us get a quick overview of what kind of data we’re working with, including potential issues like missing values.\nAppears we have some redundant columns.\nSome NaNs clearly visible in deck and age.\n\n\nRemove redundant columns\nIt looks like alive and survived are identical. Same for embarked and emback_town. Let’s run a check to see if these columns are truly identical and remove them, if so.\n\n# Check if the two columns are identical\nare_identical = df['survived'].equals(df['alive'].apply(lambda x: 1 if x == 'yes' else 0))\n\n# Print the result\nprint(f\"Are 'survived' and 'alive' identical? {are_identical}\")\n\nAre 'survived' and 'alive' identical? True\n\n\n\n# Check unique values in both columns\nprint(\"Unique values in 'embarked':\", df['embarked'].unique())\nprint(\"Unique values in 'embark_town':\", df['embark_town'].unique())\n\n# Map 'embarked' codes to 'embark_town' names\nembarked_mapping = {'S': 'Southampton', 'C': 'Cherbourg', 'Q': 'Queenstown'}\n\n# Apply the mapping to the 'embarked' column\ndf['embarked_mapped'] = df['embarked'].map(embarked_mapping)\n\n# Check if the mapped 'embarked' column is identical to 'embark_town'\nare_identical = df['embarked_mapped'].equals(df['embark_town'])\n\n# Print the result\nprint(f\"Are 'embarked' and 'embark_town' identical? {are_identical}\")\n\nUnique values in 'embarked': ['S' 'C' 'Q' nan]\nUnique values in 'embark_town': ['Southampton' 'Cherbourg' 'Queenstown' nan]\nAre 'embarked' and 'embark_town' identical? True\n\n\n\ndf.drop('alive',axis=1,inplace=True)\ndf.drop('embark_town',axis=1,inplace=True)\ndf.drop('embarked_mapped',axis=1,inplace=True)\ndf.drop('class',axis=1,inplace=True) #explicitly equiv. from documentation"
  },
  {
    "objectID": "Applications/EDA/Titanic-Dataset.html#step-2-check-data-types",
    "href": "Applications/EDA/Titanic-Dataset.html#step-2-check-data-types",
    "title": "Exploring the Titanic Dataset",
    "section": "Step 2: Check data types",
    "text": "Step 2: Check data types\n\ndf.dtypes\n\nsurvived         int64\npclass           int64\nsex             object\nage            float64\nsibsp            int64\nparch            int64\nfare           float64\nembarked        object\nwho             object\nadult_male        bool\ndeck          category\nalone             bool\ndtype: object"
  },
  {
    "objectID": "Applications/EDA/Titanic-Dataset.html#step-3-basic-statistics",
    "href": "Applications/EDA/Titanic-Dataset.html#step-3-basic-statistics",
    "title": "Exploring the Titanic Dataset",
    "section": "Step 3: Basic Statistics",
    "text": "Step 3: Basic Statistics\nNow we’ll summarize the numerical and categorical columns to better understand the central tendencies, variability, and potential missing data.\nWhen exploring numerical data:\n\nMean vs Median: Compare the mean and median. If the mean is much higher or lower than the median, this suggests skewness in the data, possibly due to outliers.\nMin and Max: Look at the minimum and maximum values to detect extreme outliers or potential data entry errors.\nStandard deviation: A high standard deviation indicates that the data points are spread out over a wide range of values, while a low standard deviation suggests that data points are clustered around the mean.\n\nWhen exploring categorical data:\n\nUnique counts: Check the number of unique categories. For example, the ‘sex’ column has two unique categories (‘male’ and ‘female’), while ‘pclass’ has three.\nMode: The mode (most frequent category) helps us understand which category dominates the dataset. For example, if most passengers are male or most belong to a particular class, this would inform our analysis.\nFrequency distribution: Look at the frequency of each category to identify potential imbalances. For example, are there more passengers from a particular class or embarkation point? Imbalanced categories can bias the model if not handled properly.\nMissing or rare categories: If a category has very few occurrences, this might suggest noise or anomalies in the data.\n\n\n# Summarize the numerical and categorical columns\nprint(\"\\nBasic statistics for numerical columns:\")\ndf.describe(include='all') # include='all' ensures both numeric and binary features are described\n\n\nBasic statistics for numerical columns:\n\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nwho\nadult_male\ndeck\nalone\n\n\n\n\ncount\n891.000000\n891.000000\n891\n714.000000\n891.000000\n891.000000\n891.000000\n889\n891\n891\n203\n891\n\n\nunique\nNaN\nNaN\n2\nNaN\nNaN\nNaN\nNaN\n3\n3\n2\n7\n2\n\n\ntop\nNaN\nNaN\nmale\nNaN\nNaN\nNaN\nNaN\nS\nman\nTrue\nC\nTrue\n\n\nfreq\nNaN\nNaN\n577\nNaN\nNaN\nNaN\nNaN\n644\n537\n537\n59\n537\n\n\nmean\n0.383838\n2.308642\nNaN\n29.699118\n0.523008\n0.381594\n32.204208\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nstd\n0.486592\n0.836071\nNaN\n14.526497\n1.102743\n0.806057\n49.693429\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nmin\n0.000000\n1.000000\nNaN\n0.420000\n0.000000\n0.000000\n0.000000\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n25%\n0.000000\n2.000000\nNaN\n20.125000\n0.000000\n0.000000\n7.910400\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n50%\n0.000000\n3.000000\nNaN\n28.000000\n0.000000\n0.000000\n14.454200\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n75%\n1.000000\n3.000000\nNaN\n38.000000\n1.000000\n0.000000\n31.000000\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nmax\n1.000000\n3.000000\nNaN\n80.000000\n8.000000\n6.000000\n512.329200\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\nInsights\n\nAge: The mean age is around 30, but with missing values and some extreme values (min = 0.42, max = 80). The mean and median are close, suggesting a fairly symmetric distribution for most passengers.\nFare: The fare has a wide range, from 0 to 512. This large range and the difference between mean and median (mean &gt; median) suggest the presence of outliers, with some passengers paying much higher fares.\nParch and SibSp: Most passengers had few or no relatives onboard, with a median of 0 for both columns.\nPclass: Most passengers are in 3rd class (mode: 3).\n\nThe df.describe(include=‘object’) part of the code is used to generate summary statistics specifically for categorical (or object) columns in a DataFrame.\n\nprint(\"\\nBasic statistics for categorical columns:\")\ndf.describe(include='object')\n\n\nBasic statistics for categorical columns:\n\n\n\n\n\n\n\n\n\nsex\nembarked\nwho\n\n\n\n\ncount\n891\n889\n891\n\n\nunique\n2\n3\n3\n\n\ntop\nmale\nS\nman\n\n\nfreq\n577\n644\n537\n\n\n\n\n\n\n\n\n\nInsights\n\nA significant portion of passengers did not survive (survival rate is less than 50%).\nThere are more males than females in this data, with adult males being the most common (vs boys)\nSouthampton (S) is the most common embarkation point (out of three options)\n\nWhile this summary provides useful information, it does not reveal the distribution of all categories, especially the rare categories. For instance, it only shows the most frequent category (the mode) and its frequency, but it doesn’t show how the other categories are distributed, especially if there are categories with very few occurrences."
  },
  {
    "objectID": "Applications/EDA/Titanic-Dataset.html#step-2.1-identifying-rare-categories",
    "href": "Applications/EDA/Titanic-Dataset.html#step-2.1-identifying-rare-categories",
    "title": "Exploring the Titanic Dataset",
    "section": "Step 2.1: Identifying Rare Categories",
    "text": "Step 2.1: Identifying Rare Categories\nTo detect rare categories in the dataset, we will examine the frequency distribution of each categorical column. This will help us understand if there are categories with very few occurrences, which could either be noise or anomalies.\nRare categories are important to identify because they can introduce bias or affect model performance if not handled properly.\n\n# Frequency distribution for categorical columns\ncategorical_cols = df.select_dtypes(include='object').columns\n\nfor col in categorical_cols:\n    print(f\"Value counts for {col}:\")\n    print(df[col].value_counts())\n    print(\"\\n\")\n\nValue counts for sex:\nsex\nmale      577\nfemale    314\nName: count, dtype: int64\n\n\nValue counts for embarked:\nembarked\nS    644\nC    168\nQ     77\nName: count, dtype: int64\n\n\nValue counts for who:\nwho\nman      537\nwoman    271\nchild     83\nName: count, dtype: int64\n\n\n\n\n\nInsights\n\nSex: No rare categories, with a fairly even distribution between males and females.\nPclass: Most passengers are in 3rd class, but no rare categories.\nEmbarked: The ‘C’ and ‘Q’ embarkation points are far less common than ‘S’ (Southampton)."
  },
  {
    "objectID": "Applications/EDA/Titanic-Dataset.html#step-3-counting-missing-values-nans",
    "href": "Applications/EDA/Titanic-Dataset.html#step-3-counting-missing-values-nans",
    "title": "Exploring the Titanic Dataset",
    "section": "Step 3: Counting Missing Values (NaNs)",
    "text": "Step 3: Counting Missing Values (NaNs)\nTo get a better understanding of where data is missing, we’ll count the number of NaN values in each column. This is important for understanding which features will need imputation or may need to be excluded from analysis.\n\n# Count the number of NaN values in each column\nprint(\"Number of NaNs per column:\")\nprint(df.isna().sum())\n\nNumber of NaNs per column:\nsurvived        0\npclass          0\nsex             0\nage           177\nsibsp           0\nparch           0\nfare            0\nembarked        2\nwho             0\nadult_male      0\ndeck          688\nalone           0\ndtype: int64"
  },
  {
    "objectID": "Applications/EDA/Titanic-Dataset.html#step-3.1-visualizing-missing-data",
    "href": "Applications/EDA/Titanic-Dataset.html#step-3.1-visualizing-missing-data",
    "title": "Exploring the Titanic Dataset",
    "section": "Step 3.1: Visualizing Missing Data",
    "text": "Step 3.1: Visualizing Missing Data\nVisualizing missing data helps us understand how much data is missing and where. This informs how we should handle missing values during preprocessing.\nThe ‘age’ column has many missing values, which could affect our analysis, especially when trying to assess survival rates across different age groups. One potential approach to deal with missing ‘age’ values is to use the ‘who’ feature, which categorizes passengers as men, women, or children. By looking at the distribution of age within each ‘who’ group, we could potentially im\n\nimport missingno as msno\n\n# Visualize missing data using missingno\nmsno.matrix(df)\nplt.show()\n\nmsno.bar(df)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights\n\nThe ‘age’ and ‘deck’ columns have a significant number of missing values.\n‘embarked’ has a couple of missing values\nWe’ll need to decide how to handle these NaNs before proceeding with modeling. The ‘deck’ column may have too many missing values to be useful without significant imputation.\n\n\n# remove deck column (not enough info here)\ndf.drop(['deck'],axis=1,inplace=True)\n\n\n# Let's explore the relationship between 'who' and 'age' to see if we can use 'who' for imputing missing ages\nsns.boxplot(x='who', y='age', data=df)\nplt.title(\"Age Distribution by Who (Men, Women, Children)\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nInsights\n\nThe ‘age’ column has many missing values, and visualizing the missing data shows that age is a significant feature with gaps. We need to address this issue to avoid biasing the model.\nBy plotting ‘age’ against ‘who’, we see distinct distributions: children tend to be younger, while men and women have overlapping but distinct age ranges.\nA potential strategy is to impute missing age values based on the ‘who’ category, filling in likely ages for children, women, and men based on these distributions.\nThis could provide a more accurate imputation than using the overall mean or median, especially in the case of children who are expected to have significantly lower ages.\n\n\n\nRemove NaNs\nImputing age could be a good stratgey here. For simplicity, we will just remove the rows where age has any NaNs, but keep in mind that this effectively tosses out ~20% of the data/information we have. In a real-world scenario, imputing is worth testing out.\n\n# Drop all rows containing NaN values\nprint(df.shape)\ndf_clean = df.dropna()\ndf_clean.shape\n\n(891, 11)\n\n\n(712, 11)\n\n\n\n# Count the number of NaN values in each column\nprint(\"Number of NaNs per column:\")\nprint(df_clean.isna().sum())\n\nNumber of NaNs per column:\nsurvived      0\npclass        0\nsex           0\nage           0\nsibsp         0\nparch         0\nfare          0\nembarked      0\nwho           0\nadult_male    0\nalone         0\ndtype: int64"
  },
  {
    "objectID": "Applications/EDA/Titanic-Dataset.html#step-4-identifying-outliers-across-multiple-features",
    "href": "Applications/EDA/Titanic-Dataset.html#step-4-identifying-outliers-across-multiple-features",
    "title": "Exploring the Titanic Dataset",
    "section": "Step 4: Identifying Outliers (Across Multiple Features)",
    "text": "Step 4: Identifying Outliers (Across Multiple Features)\nOutliers can distort model performance and influence the relationships between features. We will use boxplots to identify outliers across multiple numerical columns, including ‘age’, ‘fare’, ‘sibsp’, and ‘parch’.\n\nWhy Look for Outliers?\n\nAge: Extreme values (e.g., very young or old passengers) might influence survival predictions.\nFare: We’ve already identified skewness in the fare data, and high fares could represent wealthy individuals who had better chances of survival.\n\n\nimport seaborn as sns\n# Create boxplots for multiple numerical features to check for outliers\nnumerical_cols = ['age', 'fare']\n\nfor col in numerical_cols:\n    sns.boxplot(x=df_clean[col])\n    plt.title(f\"Boxplot of {col.capitalize()}\")\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights\n\nAge: Most passengers fall within a reasonable range, but there are a few extreme values for older passengers. These could be outliers that might need special attention during modeling.\nFare: The boxplot confirms the presence of outliers, with several passengers paying significantly more than the majority."
  },
  {
    "objectID": "Applications/EDA/Titanic-Dataset.html#step-5.1-probability-density-plot-for-fare",
    "href": "Applications/EDA/Titanic-Dataset.html#step-5.1-probability-density-plot-for-fare",
    "title": "Exploring the Titanic Dataset",
    "section": "Step 5.1: Probability Density Plot for Fare",
    "text": "Step 5.1: Probability Density Plot for Fare\nIn addition to the boxplot for identifying outliers, we can draw a probability density plot (PDF) to visualize the overall distribution of the ‘fare’ column.\nThis plot will show the likelihood of different fare values occurring, highlighting any skewness or concentration of values in certain ranges.\n\n# Plot the probability density plot (PDF) for fare\nsns.kdeplot(df_clean['fare'].dropna(), fill=True)\nplt.title(\"Probability Density Plot for Fare\")\nplt.xlabel(\"Fare\")\nplt.ylabel(\"Density\")\nplt.show()\n\n\n\n\n\n\n\n\n\nInsights\n\nThe PDF for the fare column shows a strong right skew, with most passengers paying lower fares, but a few passengers paying significantly higher fares.\nThis confirms the presence of outliers at the higher end, which we also observed in the boxplot.\nThe density plot helps visualize how fares are concentrated in lower ranges and taper off gradually toward the higher end."
  },
  {
    "objectID": "Applications/EDA/Titanic-Dataset.html#step-5.2-log-scaling-for-fare",
    "href": "Applications/EDA/Titanic-Dataset.html#step-5.2-log-scaling-for-fare",
    "title": "Exploring the Titanic Dataset",
    "section": "Step 5.2: Log Scaling for Fare",
    "text": "Step 5.2: Log Scaling for Fare\nTo deal with the strong skewness and outliers in the ‘fare’ column, we can apply a log transformation. This will compress the range of the fare values, reducing the influence of extreme outliers while keeping the relative differences intact.\nLog scaling is particularly useful for highly skewed distributions, making them more normal-like and easier for models to handle.\n\nimport numpy as np\n# Apply log scaling to the fare column (adding 1 to avoid log(0))\ndf_clean.loc[:, 'log_fare'] = df_clean['fare'].apply(lambda x: np.log(x + 1))\n\n# Plot the PDF for the log-transformed fare column\nsns.kdeplot(df_clean['log_fare'].dropna(), fill=True)\nplt.title(\"Probability Density Plot for Log-Scaled Fare\")\nplt.xlabel(\"Log(Fare + 1)\")\nplt.ylabel(\"Density\")\nplt.show()\n\n/tmp/ipykernel_2329/3551019692.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_clean.loc[:, 'log_fare'] = df_clean['fare'].apply(lambda x: np.log(x + 1))"
  },
  {
    "objectID": "Applications/EDA/Titanic-Dataset.html#step-6-exploring-correlations",
    "href": "Applications/EDA/Titanic-Dataset.html#step-6-exploring-correlations",
    "title": "Exploring the Titanic Dataset",
    "section": "Step 6: Exploring Correlations",
    "text": "Step 6: Exploring Correlations\nNext, we’ll check for correlations between numerical features. This helps us see whether some features are strongly related and could introduce multicollinearity.\nFor example: - Correlations close to 1 or -1 indicate a strong relationship between features. - Correlations close to 0 indicate little to no linear relationship between features."
  },
  {
    "objectID": "Applications/EDA/Titanic-Dataset.html#step-6.1-encode-categoriecal-data-as-numeric",
    "href": "Applications/EDA/Titanic-Dataset.html#step-6.1-encode-categoriecal-data-as-numeric",
    "title": "Exploring the Titanic Dataset",
    "section": "Step 6.1: Encode categoriecal data as numeric",
    "text": "Step 6.1: Encode categoriecal data as numeric\nEncoding the categorical data will allow us to measure correlations across different levels of our categorical variables. Encoded data is also needed for the modeling step. After encoding, you may want to visit some of the previous steps in this notebook to ensure there aren’t any problems with the encoded version of the data. Some people like to encode right after loading their data, but this can make the data unnecessarily complicated while we do some basic browsing of the data (e.g., check for redundnat columns, check for NaNs, check data types, etc.)\nCode explanation:\n\npd.get_dummies(df, drop_first=True): This one-hot encodes all categorical columns, converting them into binary columns. The drop_first=True argument prevents multicollinearity by removing one of the categories in each column (since they are mutually exclusive).\n\n\n# One-hot encode the categorical columns in the dataset\ndf_encoded = pd.get_dummies(df, drop_first=True)\ndf_encoded.head()\n\n\n\n\n\n\n\n\nsurvived\npclass\nage\nsibsp\nparch\nfare\nadult_male\nalone\nsex_male\nembarked_Q\nembarked_S\nwho_man\nwho_woman\n\n\n\n\n0\n0\n3\n22.0\n1\n0\n7.2500\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n1\n1\n38.0\n1\n0\n71.2833\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n2\n1\n3\n26.0\n0\n0\n7.9250\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\n\n\n3\n1\n1\n35.0\n1\n0\n53.1000\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\n\n\n4\n0\n3\n35.0\n0\n0\n8.0500\nTrue\nTrue\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n\n\n\n\n\nCode explanation.\n\ncorr_matrix_encoded = df_encoded.corr(): This computes the correlation matrix for both the numerical and newly one-hot encoded features.\nSeaborn heatmap: The heatmap will visualize correlations across all features, both numerical and categorical (now encoded).\n\n\n# Calculate the correlation matrix for all features (including one-hot encoded)\ncorr_matrix_encoded = df_encoded.corr()\n\n# Plot the heatmap of the correlation matrix\nplt.figure(figsize=(16, 12))\nsns.heatmap(corr_matrix_encoded, annot=False, cmap='coolwarm')\nplt.title(\"Correlation Matrix (Numerical and One-Hot Encoded Features)\")\nplt.show()\ncorr_matrix_encoded\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsurvived\npclass\nage\nsibsp\nparch\nfare\nadult_male\nalone\nsex_male\nembarked_Q\nembarked_S\nwho_man\nwho_woman\n\n\n\n\nsurvived\n1.000000\n-0.338481\n-0.077221\n-0.035322\n0.081629\n0.257307\n-0.557080\n-0.203367\n-0.543351\n0.003650\n-0.155660\n-0.557080\n0.506562\n\n\npclass\n-0.338481\n1.000000\n-0.369226\n0.083081\n0.018443\n-0.549500\n0.094035\n0.135207\n0.131900\n0.221009\n0.081720\n0.094035\n-0.177049\n\n\nage\n-0.077221\n-0.369226\n1.000000\n-0.308247\n-0.189119\n0.096067\n0.280328\n0.198270\n0.093254\n-0.022405\n-0.032523\n0.280328\n0.105081\n\n\nsibsp\n-0.035322\n0.083081\n-0.308247\n1.000000\n0.414838\n0.159651\n-0.253586\n-0.584471\n-0.114631\n-0.026354\n0.070941\n-0.253586\n0.047071\n\n\nparch\n0.081629\n0.018443\n-0.189119\n0.414838\n1.000000\n0.216225\n-0.349943\n-0.583398\n-0.245489\n-0.081228\n0.063036\n-0.349943\n0.150167\n\n\nfare\n0.257307\n-0.549500\n0.096067\n0.159651\n0.216225\n1.000000\n-0.182024\n-0.271832\n-0.182333\n-0.117216\n-0.166603\n-0.182024\n0.191243\n\n\nadult_male\n-0.557080\n0.094035\n0.280328\n-0.253586\n-0.349943\n-0.182024\n1.000000\n0.404744\n0.908578\n-0.076789\n0.112035\n1.000000\n-0.814281\n\n\nalone\n-0.203367\n0.135207\n0.198270\n-0.584471\n-0.583398\n-0.271832\n0.404744\n1.000000\n0.303646\n0.086464\n0.024929\n0.404744\n-0.211036\n\n\nsex_male\n-0.543351\n0.131900\n0.093254\n-0.114631\n-0.245489\n-0.182333\n0.908578\n0.303646\n1.000000\n-0.074115\n0.125722\n0.908578\n-0.896214\n\n\nembarked_Q\n0.003650\n0.221009\n-0.022405\n-0.026354\n-0.081228\n-0.117216\n-0.076789\n0.086464\n-0.074115\n1.000000\n-0.496624\n-0.076789\n0.100544\n\n\nembarked_S\n-0.155660\n0.081720\n-0.032523\n0.070941\n0.063036\n-0.166603\n0.112035\n0.024929\n0.125722\n-0.496624\n1.000000\n0.112035\n-0.119217\n\n\nwho_man\n-0.557080\n0.094035\n0.280328\n-0.253586\n-0.349943\n-0.182024\n1.000000\n0.404744\n0.908578\n-0.076789\n0.112035\n1.000000\n-0.814281\n\n\nwho_woman\n0.506562\n-0.177049\n0.105081\n0.047071\n0.150167\n0.191243\n-0.814281\n-0.211036\n-0.896214\n0.100544\n-0.119217\n-0.814281\n1.000000\n\n\n\n\n\n\n\n\nInsights\n\nThere is a strong correlation between ‘fare’ and ‘pclass’, which makes sense since higher-class tickets typically have higher fares.\n‘SibSp’ and ‘Parch’ have a weak positive correlation, indicating that larger families might be traveling together.\nThere aren’t many strong correlations with ‘survived’, suggesting that more advanced feature engineering might be needed to improve model performance.\nwho_man and adult_male are 100% correlated. We can remove one of these columns.\n\nPro-tip: The variance-inflation factor score can also be very helpful for assessing correlation. This measure looks at how well you can predict a given predictor (y) using all other predictors (X) as input variables to a linear regression model. The nice thing about it is that it gives you a different score for each predictor, which can be helpful when deciding which problematic features to remove.\n\ndf_encoded.drop('who_man',axis=1,inplace=True)"
  },
  {
    "objectID": "Applications/EDA/Titanic-Dataset.html#step-6.1-pairplot-for-visualizing-pairwise-relationships",
    "href": "Applications/EDA/Titanic-Dataset.html#step-6.1-pairplot-for-visualizing-pairwise-relationships",
    "title": "Exploring the Titanic Dataset",
    "section": "Step 6.1: Pairplot for Visualizing Pairwise Relationships",
    "text": "Step 6.1: Pairplot for Visualizing Pairwise Relationships\nWe’ll use Seaborn’s pairplot to visualize pairwise relationships between the numerical features, colored by whether the passenger survived.\nThis can help us identify any patterns or clusters that may inform our modeling decisions.\n\n# Pairplot to explore relationships between features, colored by 'survived'\nsns.pairplot(df_encoded, hue='survived', diag_kind='kde')\nplt.show()\n\n\n\n\n\n\n\n\n\nInsights\n\nThe pairplot shows some separation between survivors and non-survivors for certain features, such as ‘pclass’ and ‘adult_male’\nThe pairwise relationships between ‘age’, ‘fare’, and other numerical features are not perfectly linear, suggesting that non-linear models might perform better.\nVisualizing these relationships helps in identifying where additional feature engineering may be required to boost model performance."
  },
  {
    "objectID": "Applications/EDA/Titanic-Dataset.html#conclusion-next-steps-for-modeling-and-iterative-eda",
    "href": "Applications/EDA/Titanic-Dataset.html#conclusion-next-steps-for-modeling-and-iterative-eda",
    "title": "Exploring the Titanic Dataset",
    "section": "Conclusion: Next Steps for Modeling and Iterative EDA",
    "text": "Conclusion: Next Steps for Modeling and Iterative EDA\nNow that we’ve explored the Titanic dataset through extensive EDA, we’ve gained valuable insights that can guide our next steps in the modeling process. However, it’s important to remember that EDA is not a one-time process—it’s iterative and should continue as new patterns or issues arise during modeling.\n\nKey Takeaways:\n\nFeature Engineering:\n\nWe’ve identified that the ‘fare’ and ‘pclass’ columns are strongly correlated, suggesting that we might combine or transform these features for better model performance.\nLog scaling ‘fare’ has helped reduce skewness, making this feature more suitable for modeling. We can apply similar transformations to other skewed features as necessary.\nFeatures like ‘who’ and ‘age’ might benefit from imputation or interaction terms to capture deeper relationships with survival outcomes.\n\nHandling Missing Data:\n\n‘Age’ and ‘deck’ have substantial missing values. Imputing missing values based on insights from other features (e.g., using ‘who’ to impute ‘age’) could improve model robustness. Alternatively, we could explore more advanced techniques like multiple imputation or train models that handle missing data natively.\n\nAddressing Outliers:\n\nThe high outliers in ‘fare’ present a potential challenge for models like linear regression. In addition to log scaling, other techniques such as robust models or trimming/capping the extreme values could be useful.\n\nModel Selection:\n\nWith weak correlations between ‘survived’ and other numerical features, we may need to consider more complex, non-linear models like random forests, gradient boosting, or even deep learning methods that can capture non-linear patterns and interactions.\nThe insights from the pairplot suggest that non-linear relationships might exist between certain features, making tree-based models or ensemble methods a promising direction.\n\nIterative EDA:\n\nEDA doesn’t end here. As we start building models, we may encounter unexpected patterns or issues (e.g., poor model performance on certain subgroups, overfitting due to outliers). This will prompt us to revisit the EDA, iterating on feature engineering, transforming variables, or handling missing data more effectively.\nEvaluating model performance through techniques like cross-validation will provide additional insights, leading to further refinements in both data preprocessing and feature selection.\n\n\n\n\nInspirational Next Steps:\n\nBegin Modeling: Start by testing simple models (e.g., logistic regression) with the current feature set to get a baseline understanding of model performance. Use these models as a foundation for experimenting with more advanced methods.\nKeep Exploring: Stay curious and open to revisiting your EDA. As you iterate through feature engineering and model development, new questions will arise that could lead to even deeper insights.\nExperiment: Try different combinations of features, scaling techniques, and models. Use the insights from the EDA to inform these decisions but be prepared to experiment and validate your assumptions.\nIterate and Improve: Each iteration of modeling and EDA will bring you closer to a robust solution. Keep refining your approach as new patterns emerge and as your understanding of the dataset deepens.\n\nRemember, successful data science projects are not linear—they involve constant refinement, exploration, and learning. Keep iterating, keep questioning, and keep improving!\nGood luck on your journey from EDA to building powerful predictive models!"
  },
  {
    "objectID": "Applications/Videos/Forums/index.html",
    "href": "Applications/Videos/Forums/index.html",
    "title": "ML+X Forums",
    "section": "",
    "text": "Explore our library of ML+X forum recordings below! Each monthly ML+X forum highlights two ML applications that share a theme followed by communal discussions and project feedback.\n\nJoin the next live forum!\n\nWhere: Orchard View Room (rm. 3280), Discovery Building & via Zoom.\nWhen: Typically monthly on Tuesdays, 12-1pm CT. Join the Google group to receive a calendar invite (with Zoom link) and other updates. Please email us if you have any trouble joining.\n\n\n\nShare your work!\nWe encourage anyone who is using ML in their work to present at one of the ML+X forums. If interested, please fill out this brief presenter sign-up form.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvancing Healthcare and Agriculture through Computer Vision\n\n\n\n\n\n\nVideos\n\n\nML+X\n\n\nComputer vision\n\n\nUltrasound\n\n\nMedical imaging\n\n\nAgriculture\n\n\nLSTM\n\n\nCNN-LSTM\n\n\nCNN\n\n\nDeep learning\n\n\n\n1. An ultrasound-based method to measure knee kinematics enabled by deep learning 2. Plant breeding in the age of computer vision \n\n\n\n\n\nApril 9, 2024\n\n\nMatthew Blomquist, Will de la Bretonne\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Model Sharing in the Age of Foundation Models\n\n\n\n\n\n\nVideos\n\n\nML+X\n\n\nMultimodal learning\n\n\nFoundation models\n\n\nModel sharing\n\n\nHugging Face\n\n\nLLM\n\n\nLMM\n\n\nLLaVA\n\n\nDeep learning\n\n\n\n1. Model sharing and reproducible ML 2. LLaVA-NeXT and model sharing \n\n\n\n\n\nMarch 12, 2024\n\n\nChris Endemann, Haotian Liu, PhD\n\n\n\n\n\n\n\n\n\n\n\n\nNavigating Gravitational Waves with AI Insights\n\n\n\n\n\n\nVideos\n\n\nML+X\n\n\nPhysics\n\n\nSimulations\n\n\n\n1. Welcome and small group discussions 2. Classifying gravitational wave modes from core-collapse supernovae \n\n\n\n\n\nFebruary 13, 2024\n\n\nChris Endemann, Bella Finkel\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Science Communication and Drug Synergy Analysis using GPT\n\n\n\n\n\n\nVideos\n\n\nML+X\n\n\nScience communication\n\n\nHealthcare\n\n\nDrug synergy\n\n\nLLM\n\n\nText mining\n\n\n\n1. GPT for Science Communication: User-Interface and Developer Pipeline Approaches 2. Advancing Biomedical Research with GPT-4: A Novel Approach to Drug Synergy Analysis using Text Mining and Classification \n\n\n\n\n\nDecember 12, 2023\n\n\nBen Rush, PhD, Jack Freeman\n\n\n\n\n\n\n\n\n\n\n\n\nLLMS in Genomic and Health Coaching\n\n\n\n\n\n\nVideos\n\n\nML+X\n\n\nHealthcare\n\n\nClustering\n\n\nDeep learning\n\n\nLLM\n\n\nGenomics\n\n\n\n1. Clustering of genomic sequences of mycoviruses using deep learning 2. Spurring self-improvement and intrinsic motivation using LLMs and reinforcement learning \n\n\n\n\n\nNovember 7, 2023\n\n\nRohan Sontahlia, Michael Roytman\n\n\n\n\n\n\n\n\n\n\n\n\nTime-Series Analysis\n\n\n\n\n\n\nVideos\n\n\nML+X\n\n\nTime-series\n\n\nGenomics\n\n\nHealthcare\n\n\n\n1. Computational Methods for Comparative Time Clocks in Early Development and Tissue Regeneration 2. Controlled Differential Equations on Long Sequences via Non-standard Wavelets \n\n\n\n\n\nOctober 10, 2023\n\n\nPeng Jiang, PhD, Sourav Pal\n\n\n\n\n\n\n\n\n\n\n\n\nMultimodal Learning\n\n\n\n\n\n\nVideos\n\n\nML+X\n\n\nMultimodal learning\n\n\nDeep learning\n\n\nComputer vision\n\n\nHealthcare\n\n\nGenomics\n\n\n\n1. Multimodal learning and analysis for understanding single-cell functional genomics in brains and brain diseases 2. Transforming healthcare: AI-enhanced disease quantification with vision-language models 3. The benefits of early fusion: deeply integrated audio-visual representation learning \n\n\n\n\n\nSeptember 19, 2023\n\n\nDaifeng Wang, PhD, Zachary Huemann, Pedro Morgado, PhD\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Applications",
      "Videos",
      "Forums"
    ]
  },
  {
    "objectID": "Applications/Videos/Other/index.html",
    "href": "Applications/Videos/Other/index.html",
    "title": "Other",
    "section": "",
    "text": "Explore talks from other groups besides ML+X, ML4MI, and SILO; on campus and beyond.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Biophysics-based Protein Language Model for Protein Engineering\n\n\n\n\n\n\nVideos\n\n\nCross Labs AI\n\n\nTransfer learning\n\n\nBiophysics\n\n\nProtein language models\n\n\nFoundation models\n\n\nLLM\n\n\nDeep learning\n\n\nProtein engineering\n\n\nSimulations\n\n\n\nWe introduce Mutational Effect Transfer Learning (METL), a specialized protein language model that bridges the gap between traditional biophysics-based and machine learning approaches by incorporating synthetic data from molecular simulations.\n\n\n\n\n\nJune 18, 2024\n\n\nSam Gelman, PhD\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Large Language Models for Meteorological Fact Finding\n\n\n\n\n\n\nVideos\n\n\nLLM\n\n\nMeteorology\n\n\n\nThis talk demonstrates harnessing the power of AI to open new avenues in data analysis, including for meteorological fact-finding. Discover how cutting-edge large language models (LLMs) like OpenAI’s ChatGPT 3.5 and 4.0 hold are poised to help the field of meteorological data analysis. \n\n\n\n\n\nMay 30, 2024\n\n\nZekai Otles\n\n\n\n\n\n\n\n\n\n\n\n\nExploring AI at UW-Madison\n\n\n\n\n\n\nMultidisciplinary\n\n\nUW-Madison\n\n\nPlaylists\n\n\n\nA summer 2023 webinar series sponsored by the Division of Information Technology and the Data Science Institute.\n\n\n\n\n\nJune 23, 2023\n\n\nChris Endemann\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Applications",
      "Videos",
      "Other"
    ]
  },
  {
    "objectID": "Applications/Videos/Other/Exploring-AI-at-UW.html",
    "href": "Applications/Videos/Other/Exploring-AI-at-UW.html",
    "title": "Exploring AI at UW-Madison",
    "section": "",
    "text": "From the Exploring Artificial Integlligence @ UW-Madison webpage:\n\nFrom June 23 to September 29, 2023, we were privileged to present a thought-provoking artificial-intelligence (AI) webinar series tailored specifically for UW–Madison.\nIn today’s rapidly evolving world, the importance of AI in our academic environment cannot be overstated. As technological advancements continue to shape our society, we believe it is crucial to explore the opportunities and challenges AI brings to the forefront of higher education.\nThis webinar series aimed to provide a platform for experts and visionaries in the field of AI to share their insights, research and experiences in the classroom, research lab and wider academic community. By delving into topics such as AI ethics, cutting-edge machine learning algorithms, automation and human-machine collaboration, we hoped to foster a deeper understanding of AI’s transformative potential and its implications for higher education.\nThis was a captivating and engaging journey of discovery and innovation. We’re pleased to present recordings of the webinar sessions here.\n\nNote: To access the videos, you will need to be a UW-Madison affiliate with a netID."
  },
  {
    "objectID": "Applications/Videos/Other/Exploring-AI-at-UW.html#about-this-resource",
    "href": "Applications/Videos/Other/Exploring-AI-at-UW.html#about-this-resource",
    "title": "Exploring AI at UW-Madison",
    "section": "",
    "text": "From the Exploring Artificial Integlligence @ UW-Madison webpage:\n\nFrom June 23 to September 29, 2023, we were privileged to present a thought-provoking artificial-intelligence (AI) webinar series tailored specifically for UW–Madison.\nIn today’s rapidly evolving world, the importance of AI in our academic environment cannot be overstated. As technological advancements continue to shape our society, we believe it is crucial to explore the opportunities and challenges AI brings to the forefront of higher education.\nThis webinar series aimed to provide a platform for experts and visionaries in the field of AI to share their insights, research and experiences in the classroom, research lab and wider academic community. By delving into topics such as AI ethics, cutting-edge machine learning algorithms, automation and human-machine collaboration, we hoped to foster a deeper understanding of AI’s transformative potential and its implications for higher education.\nThis was a captivating and engaging journey of discovery and innovation. We’re pleased to present recordings of the webinar sessions here.\n\nNote: To access the videos, you will need to be a UW-Madison affiliate with a netID."
  },
  {
    "objectID": "Applications/Videos/Other/Exploring-AI-at-UW.html#questions",
    "href": "Applications/Videos/Other/Exploring-AI-at-UW.html#questions",
    "title": "Exploring AI at UW-Madison",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Applications/Videos/ML4MI/24-09-16_Vision-Language-and-VisionLanguage-Modeling-in-Radiology_Tyler-Bradshaw.html",
    "href": "Applications/Videos/ML4MI/24-09-16_Vision-Language-and-VisionLanguage-Modeling-in-Radiology_Tyler-Bradshaw.html",
    "title": "Vision, Language, and Vision-Language Modeling in Radiology",
    "section": "",
    "text": "In this talk from the Machine Learning for Medical Imaging (ML4MI) community, Tyler Bradshaw (PhD) discusses the historical context (e.g., CNN, VGG) leading up to the new era of multimodal learning (e.g., vision-language models), and explores how these models are currently being leveraged in the radiology field.\nVideo (requires UW netID): View 24-09-16 ML4MI Recording."
  },
  {
    "objectID": "Applications/Videos/ML4MI/24-09-16_Vision-Language-and-VisionLanguage-Modeling-in-Radiology_Tyler-Bradshaw.html#see-also",
    "href": "Applications/Videos/ML4MI/24-09-16_Vision-Language-and-VisionLanguage-Modeling-in-Radiology_Tyler-Bradshaw.html#see-also",
    "title": "Vision, Language, and Vision-Language Modeling in Radiology",
    "section": "See also",
    "text": "See also\n\nModel: UNET: Learn more about the UNET model."
  },
  {
    "objectID": "Applications/Videos/SILO/23-11-22_KennethMarino_ World-Knowledge-in-the-Time-of-Large-Models.html",
    "href": "Applications/Videos/SILO/23-11-22_KennethMarino_ World-Knowledge-in-the-Time-of-Large-Models.html",
    "title": "World Knowledge in the Time of Large Models",
    "section": "",
    "text": "Summary from SILO website\n\nBio: Kenneth Marino is Research Scientist at Google DeepMind in NYC, focusing on improving knowledge-based systems such as retrieval and information extraction as well as embodied reasoning with language. He graduated in 2021 from Carnegie Mellon University advised by Abhinav Gupta, where his thesis focused on incorporating knowledge into embodied systems. He has an adjunct appointment at Columbia University where he teaches a class focused on the impact of datasets on machine learning and how to collect good datasets. He received his undergraduate degree from the Georgia Institute of Technology where he studied Computer Engineering and Computer Science.\n\n\nAbstract: This talk will discuss the massive shift that has come about in the vision and ML community as a result of the large pre-trained language and language and vision models such as Flamingo, GPT-4, and other models. We begin by looking at the work on knowledge-based systems in CV and robotics before the large model revolution and discuss the impact it had. This impact can be broken down into three areas in which world knowledge should be studied in the context of these new models: evaluation, harnessing large models, and building outside knowledge. First, evaluating world knowledge is even more important as the large model revolution gives more easy access to world knowledge. Next, we discuss recent work in harnessing models such as Flamingo and Chinchilla for visual and procedural knowledge. Finally, the talk discusses how, by focusing on knowledge acquisition as an agent-centric problem, we can make developments in retrieving and collecting world knowledge.\n\n\n\nLinks\n\nAbout the Speaker → kennethmarino.com\nOK-VQA paper and dataset → okvqa.allenai.org/index.html\nKRISP paper → arxiv.org/abs/2012.11014\nSame Object, Different Grasps paper → arxiv.org/abs/2011.06431\nA-OKVQA dataset/GitHub → github.com/allenai/aokvqa\nDistilling Internet-Scale Vision-Language Models into Embodied Agents paper → arxiv.org/abs/2301.12507\n\n\n\n\nJump to section\n\n[0:00] Introducing Kenneth Marino\n[1:11] Begin presentation\n[1:37] What do we want from AI?\n[3:00] The old way: treating all tasks individually\n[3:50] Knowledge / priors matter\n[5:00] LMs have built-in knowledge\n[7:14] Prologue: Before the LLM/VLM revolution\n[8:15] Evaluating knowledge\n[9:30] Evaluating knowledge with OK-VQA\n[10:30] KRISP: Incorporating knowledge graphs\n[11:15] LLMs and VLMs: Accessible world knowledge\n[13:28] Evaluating knowledge in LLMs/VLMs\n[14:40] Many kinds of knowledge\n[16:17] Evaluating knowledge with A-OKVQA\n[22:38] From evaluating to using LLMs/VLMs\n[25:31] Extracting knoweldeg from LLMs\n[26:15] Bringing Flamingo’s knowledge into agents\n[35:48] The only constant is change\n[36:45] Inquisitive agents\n[40:11] Wikipedia navigation as a benchmark\n[53:00] Takeaways\n[53:55] Q&A"
  },
  {
    "objectID": "Contributor-templates/template_application-blog.html",
    "href": "Contributor-templates/template_application-blog.html",
    "title": "Title of Your Blog Post",
    "section": "",
    "text": "Provide a short introduction to your blog, including its main focus and why it’s relevant for the ML+X Nexus community. Highlight any key topics, challenges, or learnings you will discuss. Feel free to include a text embedded link to any related resources or references."
  },
  {
    "objectID": "Contributor-templates/template_application-blog.html#code-snippets-optional",
    "href": "Contributor-templates/template_application-blog.html#code-snippets-optional",
    "title": "Title of Your Blog Post",
    "section": "Code Snippets (Optional)",
    "text": "Code Snippets (Optional)\nIf you’re sharing code directly, include short snippets or explanations of the key components below. If you’d prefer to link to a repository, feel free to link your GitHub repo here.\n```python # Example code snippet import numpy as np print(“Hello ML+X Nexus!”)"
  },
  {
    "objectID": "Contributor-templates/template_application-blog.html#questions",
    "href": "Contributor-templates/template_application-blog.html#questions",
    "title": "Title of Your Blog Post",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Contributor-templates/template_application-blog.html#see-also",
    "href": "Contributor-templates/template_application-blog.html#see-also",
    "title": "Title of Your Blog Post",
    "section": "See also",
    "text": "See also\n\n\nRelated Resource 1: Brief description of related resource 1.\nRelated Resource 2: Brief description of related resource 2.\nRelated Resource 3: Brief description of related resource 3."
  },
  {
    "objectID": "Contributor-templates/template_learn.html",
    "href": "Contributor-templates/template_learn.html",
    "title": "Title/Topic of Resource",
    "section": "",
    "text": "Brief description of the resource, including a text embedded link in the first 1-2 sentences. Explain what the resource covers and its relevance. Mention any specific features, strengths, or weaknesses. This section should help potential users understand the value of the resource and what they can expect to learn or achieve by using it.\n\n\n\n\nPrerequisite Resource 1\nPrerequisite Resource 2"
  },
  {
    "objectID": "Contributor-templates/template_learn.html#about-this-resource",
    "href": "Contributor-templates/template_learn.html#about-this-resource",
    "title": "Title/Topic of Resource",
    "section": "",
    "text": "Brief description of the resource, including a text embedded link in the first 1-2 sentences. Explain what the resource covers and its relevance. Mention any specific features, strengths, or weaknesses. This section should help potential users understand the value of the resource and what they can expect to learn or achieve by using it.\n\n\n\n\nPrerequisite Resource 1\nPrerequisite Resource 2"
  },
  {
    "objectID": "Contributor-templates/template_learn.html#questions",
    "href": "Contributor-templates/template_learn.html#questions",
    "title": "Title/Topic of Resource",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Contributor-templates/template_learn.html#see-also",
    "href": "Contributor-templates/template_learn.html#see-also",
    "title": "Title/Topic of Resource",
    "section": "See also",
    "text": "See also\n\n\nRelated Resource 1: Brief description of related resource 1.\nRelated Resource 2: Brief description of related resource 2.\nRelated Resource 3: Brief description of related resource 3."
  },
  {
    "objectID": "Contributor-templates/template_learn-video.html",
    "href": "Contributor-templates/template_learn-video.html",
    "title": "Title/Topic of Resource",
    "section": "",
    "text": "Brief description of the resource, including a text embedded link in the first 1-2 sentences. Explain what the resource covers and its relevance. Mention any specific features, strengths, or weaknesses. This section should help potential users understand the value of the resource and what they can expect to learn or achieve by using it.\n\n\n\n\nPrerequisite Resource 1\nPrerequisite Resource 2"
  },
  {
    "objectID": "Contributor-templates/template_learn-video.html#about-this-resource",
    "href": "Contributor-templates/template_learn-video.html#about-this-resource",
    "title": "Title/Topic of Resource",
    "section": "",
    "text": "Brief description of the resource, including a text embedded link in the first 1-2 sentences. Explain what the resource covers and its relevance. Mention any specific features, strengths, or weaknesses. This section should help potential users understand the value of the resource and what they can expect to learn or achieve by using it.\n\n\n\n\nPrerequisite Resource 1\nPrerequisite Resource 2"
  },
  {
    "objectID": "Contributor-templates/template_learn-video.html#questions",
    "href": "Contributor-templates/template_learn-video.html#questions",
    "title": "Title/Topic of Resource",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Contributor-templates/template_learn-video.html#see-also",
    "href": "Contributor-templates/template_learn-video.html#see-also",
    "title": "Title/Topic of Resource",
    "section": "See also",
    "text": "See also\n\n\nRelated Resource 1: Brief description of related resource 1.\nRelated Resource 2: Brief description of related resource 2.\nRelated Resource 3: Brief description of related resource 3."
  },
  {
    "objectID": "Contributor-templates/template_toolbox-data.html",
    "href": "Contributor-templates/template_toolbox-data.html",
    "title": "[Dataset Name]",
    "section": "",
    "text": "The [Dataset Name] dataset contains [description of data: what it consists of, its size, and the variety of contents]. Researchers and students working on machine learning applications can use this dataset to explore tasks such as [relevant tasks: classification, regression, generation, etc.]. The dataset’s availability in [formats, e.g., text, image, tabular, audio, etc.] makes it suitable for [type of tasks, e.g., multimodal learning, NLP, image analysis, etc.] tasks as well.\n\n\n\n[Feature 1]: [Describe a key feature of the dataset, e.g., modality, format availability, data variety, etc.]\n\nFor text: Could include multilingual content, long-form text, etc.\nFor images: Could include resolution, labeled/annotated data, specific objects/subjects present, etc.\nFor tabular: Could include number of features, type of features (categorical, continuous), missing data, etc.\n\n[Feature 2]: [Describe another key feature]\n\nFor text: Possible audio pairings, language variety\nFor images: Pre-segmented images, spatial resolution, etc.\nFor tabular: Structured metadata, time-series elements, etc.\n\n[Feature 3]: [Highlight any distinguishing characteristics]\n\nFor text: Long-form, document-level annotations, etc.\nFor images: Bounding boxes, pixel-level segmentation, medical imaging, etc.\nFor tabular: Feature engineering potential, handling large-scale datasets, etc.\n\n\n\n\n\n\n[Application 1]: [Describe how the dataset can be used in a specific task, e.g., language modeling, classification]\n\nFor text: Language modeling, translation, classification\nFor images: Object detection, image classification, image segmentation\nFor tabular: Predictive modeling, classification, time-series forecasting\n\n[Application 2]: [Include more specific machine learning tasks the dataset is commonly applied to]\n\nFor text: Sentiment analysis, summarization\nFor images: Image generation, style transfer, medical diagnostics\nFor tabular: Forecasting, clustering, anomaly detection\n\n[Application 3]: [Cover any additional potential uses]\n\nFor text: Topic modeling, question answering\nFor images: Face detection, scene understanding\nFor tabular: Fraud detection, recommendation systems\n\n[Topic Modeling, if applicable]: [If relevant, explain how the dataset can be used to explore underlying themes or topics]\n\nApplies to text-based datasets primarily\n\n[Multimodal Learning, if applicable]: [If the dataset supports multimodal tasks, describe how it can be used in this context]\n\nE.g., paired image and text, or text and audio datasets\n\n[Transfer Learning, if applicable]: [Mention transfer learning use cases]\n\nFor all types: Fine-tuning pre-trained models for specific tasks using this dataset\n\n[Data Augmentation, if applicable]: [Explain how the dataset can be used to augment smaller datasets]\n\nText: Generating more training data via paraphrasing, etc.\nImages: Rotations, flips, color variations\nTabular: Synthetic data generation, imputation of missing values\n\n\n\n\n\n\n[Related Dataset 1]: [Provide a brief description of a related dataset or project]\n\nFor text: Could include other large corpora (e.g., Common Crawl)\nFor images: COCO, ImageNet, etc.\nFor tabular: UCI Machine Learning Repository, Kaggle datasets\n\n[Related Dataset 2]: [Include any other related projects or datasets that users may find useful]"
  },
  {
    "objectID": "Contributor-templates/template_toolbox-data.html#about-this-resource",
    "href": "Contributor-templates/template_toolbox-data.html#about-this-resource",
    "title": "[Dataset Name]",
    "section": "",
    "text": "The [Dataset Name] dataset contains [description of data: what it consists of, its size, and the variety of contents]. Researchers and students working on machine learning applications can use this dataset to explore tasks such as [relevant tasks: classification, regression, generation, etc.]. The dataset’s availability in [formats, e.g., text, image, tabular, audio, etc.] makes it suitable for [type of tasks, e.g., multimodal learning, NLP, image analysis, etc.] tasks as well.\n\n\n\n[Feature 1]: [Describe a key feature of the dataset, e.g., modality, format availability, data variety, etc.]\n\nFor text: Could include multilingual content, long-form text, etc.\nFor images: Could include resolution, labeled/annotated data, specific objects/subjects present, etc.\nFor tabular: Could include number of features, type of features (categorical, continuous), missing data, etc.\n\n[Feature 2]: [Describe another key feature]\n\nFor text: Possible audio pairings, language variety\nFor images: Pre-segmented images, spatial resolution, etc.\nFor tabular: Structured metadata, time-series elements, etc.\n\n[Feature 3]: [Highlight any distinguishing characteristics]\n\nFor text: Long-form, document-level annotations, etc.\nFor images: Bounding boxes, pixel-level segmentation, medical imaging, etc.\nFor tabular: Feature engineering potential, handling large-scale datasets, etc.\n\n\n\n\n\n\n[Application 1]: [Describe how the dataset can be used in a specific task, e.g., language modeling, classification]\n\nFor text: Language modeling, translation, classification\nFor images: Object detection, image classification, image segmentation\nFor tabular: Predictive modeling, classification, time-series forecasting\n\n[Application 2]: [Include more specific machine learning tasks the dataset is commonly applied to]\n\nFor text: Sentiment analysis, summarization\nFor images: Image generation, style transfer, medical diagnostics\nFor tabular: Forecasting, clustering, anomaly detection\n\n[Application 3]: [Cover any additional potential uses]\n\nFor text: Topic modeling, question answering\nFor images: Face detection, scene understanding\nFor tabular: Fraud detection, recommendation systems\n\n[Topic Modeling, if applicable]: [If relevant, explain how the dataset can be used to explore underlying themes or topics]\n\nApplies to text-based datasets primarily\n\n[Multimodal Learning, if applicable]: [If the dataset supports multimodal tasks, describe how it can be used in this context]\n\nE.g., paired image and text, or text and audio datasets\n\n[Transfer Learning, if applicable]: [Mention transfer learning use cases]\n\nFor all types: Fine-tuning pre-trained models for specific tasks using this dataset\n\n[Data Augmentation, if applicable]: [Explain how the dataset can be used to augment smaller datasets]\n\nText: Generating more training data via paraphrasing, etc.\nImages: Rotations, flips, color variations\nTabular: Synthetic data generation, imputation of missing values\n\n\n\n\n\n\n[Related Dataset 1]: [Provide a brief description of a related dataset or project]\n\nFor text: Could include other large corpora (e.g., Common Crawl)\nFor images: COCO, ImageNet, etc.\nFor tabular: UCI Machine Learning Repository, Kaggle datasets\n\n[Related Dataset 2]: [Include any other related projects or datasets that users may find useful]"
  },
  {
    "objectID": "Contributor-templates/template_toolbox-data.html#questions",
    "href": "Contributor-templates/template_toolbox-data.html#questions",
    "title": "[Dataset Name]",
    "section": "Questions?",
    "text": "Questions?\nIf you have any lingering questions about this resource, feel free to post them on the ML+X Nexus Q&A on GitHub. We will update this resource as new information or applications arise."
  },
  {
    "objectID": "Contributor-templates/template_toolbox-data.html#see-also",
    "href": "Contributor-templates/template_toolbox-data.html#see-also",
    "title": "[Dataset Name]",
    "section": "See also",
    "text": "See also\n\n[Related Workshop or Resource]: [Provide a link to any relevant workshop, tutorial, or resource that helps users get started with this dataset or similar datasets]"
  },
  {
    "objectID": "includes/common-resources-text.html",
    "href": "includes/common-resources-text.html",
    "title": "Nexus: Crowdsourced ML Resources",
    "section": "",
    "text": "Any content (original or external) that can help make the practice of ML more connected, accessible, efficient, and reproducible is welcome on the Nexus platform! This includes, but is not limited to…\n\n🧠 Educational materials: Explore a library of educational materials (workshops, guides, books, videos, etc.) covering a wide range of ML-related topics, tools, and workflows, from foundational concepts to advanced techniques. These materials offer clear explanations, practical examples, and actionable insights to help you navigate the complexities of ML with confidence.\n🛠 Models, code, and more: Learn about popular pretrained & foundation models, useful scripts, and datasets that you can leverage for your next ML project. Learn about their features, how to use them effectively, and see examples of them in action.\n🧬 Applications & stories: Discover a curated collection of blogs, papers, and talks which dive into real-world ML applications and lessons learned by practitioners. This section also includes exploratory data analysis (EDA) case studies, which demonstrate the technical and domain knowledge needed to explore data from various fields."
  },
  {
    "objectID": "Toolbox/Libraries/kornia.html",
    "href": "Toolbox/Libraries/kornia.html",
    "title": "Kornia",
    "section": "",
    "text": "Kornia is a differentiable library that allows classical computer vision to be integrated into deep learning models. Developed by E. Riba, D. Mishkin, D. Ponsa, E. Rublee and G. Bradski and introduced in 2020, it is built on top of PyTorch to offer tools for image processing and transformer-based computer vision models (e.g., SAM, ViT, LoFTR, and RT-DETR). It’s perfect for researchers and practitioners who want GPU-optimized tools for complex vision tasks without sacrificing classic techniques.\n\n\n\nImage augmentation: Standard image augmentation functionalities such as gaussian noise, random flips, and color jiggle.\nFeature detection: Classical computer vision algorithms such hessian blobs, Harris corner detector, and difference of gaussians. Deep learning-based algorithms such as Dexined, KeyNet, DISK, and DeDoDe.\nTransformer models: Optimized transformers-based computer vision models such as SAM, ViT, LoFTR, and RT-DETR\n\nSAM (Segment Anything Model): A state-of-the-art segmentation model from Meta that can segment objects in images without predefined categories using self-supervised learning.\nViT (Vision Transformer): Adapts the transformer architecture, traditionally used in NLP, to process images by dividing them into patches, leading to high performance on large datasets.\nLoFTR (Local Feature Transformer): A model for establishing correspondences between images through transformer-based attention, which is highly effective for tasks like image stitching and 3D reconstruction.\nRT-DETR (Real-Time Detection Transformer): An object detection model optimized for real-time processing, making it suitable for rapid detection tasks like video processing and autonomous navigation."
  },
  {
    "objectID": "Toolbox/Libraries/kornia.html#about-this-library",
    "href": "Toolbox/Libraries/kornia.html#about-this-library",
    "title": "Kornia",
    "section": "",
    "text": "Kornia is a differentiable library that allows classical computer vision to be integrated into deep learning models. Developed by E. Riba, D. Mishkin, D. Ponsa, E. Rublee and G. Bradski and introduced in 2020, it is built on top of PyTorch to offer tools for image processing and transformer-based computer vision models (e.g., SAM, ViT, LoFTR, and RT-DETR). It’s perfect for researchers and practitioners who want GPU-optimized tools for complex vision tasks without sacrificing classic techniques.\n\n\n\nImage augmentation: Standard image augmentation functionalities such as gaussian noise, random flips, and color jiggle.\nFeature detection: Classical computer vision algorithms such hessian blobs, Harris corner detector, and difference of gaussians. Deep learning-based algorithms such as Dexined, KeyNet, DISK, and DeDoDe.\nTransformer models: Optimized transformers-based computer vision models such as SAM, ViT, LoFTR, and RT-DETR\n\nSAM (Segment Anything Model): A state-of-the-art segmentation model from Meta that can segment objects in images without predefined categories using self-supervised learning.\nViT (Vision Transformer): Adapts the transformer architecture, traditionally used in NLP, to process images by dividing them into patches, leading to high performance on large datasets.\nLoFTR (Local Feature Transformer): A model for establishing correspondences between images through transformer-based attention, which is highly effective for tasks like image stitching and 3D reconstruction.\nRT-DETR (Real-Time Detection Transformer): An object detection model optimized for real-time processing, making it suitable for rapid detection tasks like video processing and autonomous navigation."
  },
  {
    "objectID": "Toolbox/Libraries/kornia.html#integration-and-compatibility",
    "href": "Toolbox/Libraries/kornia.html#integration-and-compatibility",
    "title": "Kornia",
    "section": "Integration and compatibility",
    "text": "Integration and compatibility\nKornia is compatible with PyTorch and any libraries built on top of PyTorch. In addition, all features from Kornia can utilize the GPU of the host machine.\n\nFrameworks Supported: PyTorch, PyTorch-Lightning, and Fastai\nInstallation Instructions: pip install kornia"
  },
  {
    "objectID": "Toolbox/Libraries/kornia.html#why-mix-traditional-methods-with-transformers",
    "href": "Toolbox/Libraries/kornia.html#why-mix-traditional-methods-with-transformers",
    "title": "Kornia",
    "section": "Why mix traditional methods with transformers?",
    "text": "Why mix traditional methods with transformers?\nBlending traditional computer vision techniques with transformer-based models offers a balanced approach that can improve model performance, flexibility, and efficiency. More specificically, this allow us to…\n\nLeverage established techniques: Many traditional methods, like edge detection and keypoint matching, have been fine-tuned over decades and are computationally efficient. They often work well as preprocessing steps or for augmenting transformer-based models with reliable, low-complexity features.\nReduce computational costs: Transformers are data- and computation-intensive, especially for complex tasks. By using traditional methods in early or intermediate stages, you can reduce the workload on transformers, enabling real-time processing and lowering hardware requirements.\nImprove generalization: Combining classical methods with transformers can provide models with a broader understanding of images, as they benefit from both deterministic feature engineering (traditional methods) and learned feature representations (transformers). This synergy often enhances the model’s generalization to varied datasets.\nTargeted feature enhancement: Traditional feature detectors can focus on specific areas, like edges or corners, enhancing input data that transformers process. This combination can lead to improved performance on tasks such as image matching, object detection, and segmentation."
  },
  {
    "objectID": "Toolbox/Libraries/kornia.html#use-cases",
    "href": "Toolbox/Libraries/kornia.html#use-cases",
    "title": "Kornia",
    "section": "Use cases",
    "text": "Use cases\nHere are some examples of how Kornia can be applied to different machine learning tasks.\n\nImage preprocessing: Apply the Shi-Tomasi cornerness function to preprocess images for 3D reconstruction.\nImage matching: Apply the KeyNet algorithm to match images from different perspectives/angles."
  },
  {
    "objectID": "Toolbox/Libraries/kornia.html#tutorials-and-resources",
    "href": "Toolbox/Libraries/kornia.html#tutorials-and-resources",
    "title": "Kornia",
    "section": "Tutorials and resources",
    "text": "Tutorials and resources\n\nOfficial tutorials: Link to several tutorials/guides on using their models: kornia.github.io/tutorials/"
  },
  {
    "objectID": "Toolbox/Libraries/kornia.html#questions",
    "href": "Toolbox/Libraries/kornia.html#questions",
    "title": "Kornia",
    "section": "Questions?",
    "text": "Questions?\nIf you have any lingering questions about this library, please post to the Nexus Q&A on GitHub."
  },
  {
    "objectID": "Toolbox/Libraries/kornia.html#see-also",
    "href": "Toolbox/Libraries/kornia.html#see-also",
    "title": "Kornia",
    "section": "See also",
    "text": "See also\n\nML4MI Seminar: Vision, Language, and Vision-Language Modeling in Radiology: In this talk from the Machine Learning for Medical Imaging (ML4MI) community, Tyler Bradshaw (PhD) discusses the historical context (e.g., CNN, VGG) leading up to the new era of multimodal learning (e.g., vision-language models), and explores how these models are currently being leveraged in the radiology field.\nWorkshop: Intro to Deep Learning with PyTorch: Explore the popular PyTorch deep learning framework."
  },
  {
    "objectID": "Toolbox/Libraries/MONAI.html",
    "href": "Toolbox/Libraries/MONAI.html",
    "title": "MONAI: Medical Open Network for AI",
    "section": "",
    "text": "MONAI (Medical Open Network for AI) is an open-source, community-supported framework for deep learning in healthcare imaging. Built on top of PyTorch, it provides specialized functionality for medical imaging tasks, including pre-trained models, data loaders, and evaluation metrics tailored for the medical domain. MONAI is particularly valuable for researchers and practitioners looking to leverage deep learning for medical imaging applications due to its flexibility, performance, and ease of integration with existing workflows.\n\n\n\nIntroduction to Deep Learning with PyTorch"
  },
  {
    "objectID": "Toolbox/Libraries/MONAI.html#about-this-resource",
    "href": "Toolbox/Libraries/MONAI.html#about-this-resource",
    "title": "MONAI: Medical Open Network for AI",
    "section": "",
    "text": "MONAI (Medical Open Network for AI) is an open-source, community-supported framework for deep learning in healthcare imaging. Built on top of PyTorch, it provides specialized functionality for medical imaging tasks, including pre-trained models, data loaders, and evaluation metrics tailored for the medical domain. MONAI is particularly valuable for researchers and practitioners looking to leverage deep learning for medical imaging applications due to its flexibility, performance, and ease of integration with existing workflows.\n\n\n\nIntroduction to Deep Learning with PyTorch"
  },
  {
    "objectID": "Toolbox/Libraries/MONAI.html#questions",
    "href": "Toolbox/Libraries/MONAI.html#questions",
    "title": "MONAI: Medical Open Network for AI",
    "section": "Questions?",
    "text": "Questions?\nIf you have any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Toolbox/Libraries/MONAI.html#see-also",
    "href": "Toolbox/Libraries/MONAI.html#see-also",
    "title": "MONAI: Medical Open Network for AI",
    "section": "See also",
    "text": "See also\n\nMONAI Tutorials\nMIDeL — Medical Image Deep Learning\nMONAI Bootcamp 2021 (Video Series)"
  },
  {
    "objectID": "Toolbox/Compute/index.html",
    "href": "Toolbox/Compute/index.html",
    "title": "Compute",
    "section": "",
    "text": "Explore compute resources (high-throughput, GPUs, etc.) for your next ML project!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to AWS SageMaker for Predictive ML/AI\n\n\n\nWorkshops\n\n\nCode-along\n\n\nCompute\n\n\nAWS SageMaker\n\n\nPredictive modeling\n\n\nMachine learning\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-11-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCenter for High Throughput Computing (CHTC)\n\n\n\nCompute\n\n\nGPU\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-06-25\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Toolbox",
      "Compute"
    ]
  },
  {
    "objectID": "Toolbox/Data/index.html",
    "href": "Toolbox/Data/index.html",
    "title": "Data",
    "section": "",
    "text": "Explore popular datasets that you can leverage for your next ML project.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeaf Vein Dataset (LVD2021)\n\n\n\nData\n\n\nImage\n\n\nPlant phenotyping\n\n\nImage segmentation\n\n\nComputer vision\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-10-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject Gutenberg: Text & Audio Books\n\n\n\nData\n\n\nText\n\n\nAudio\n\n\nMultimodal\n\n\nNLP\n\n\nText analysis\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-10-14\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Toolbox",
      "Data"
    ]
  },
  {
    "objectID": "Toolbox/Data/LVD2021_Leaf-Vein-Dataset.html",
    "href": "Toolbox/Data/LVD2021_Leaf-Vein-Dataset.html",
    "title": "Leaf Vein Dataset (LVD2021)",
    "section": "",
    "text": "The Leaf Vein Dataset 2021 (LVD2021), introduced in the paper Leaf vein segmentation with self-supervision, contains high-resolution images of leaves with pixel-wise annotations of vein structures. This dataset can be used to develop machine learning models for leaf vein segmentation and plant phenotyping. The dataset includes 4977 images across 36 different types of leaves, with over 100 images per type. The pixel-level annotations make it suitable for training models for fine-grained segmentation tasks. To request access to the Leaf Vein Dataset 2021 (LVD2021), please fill out the request form.\nThe paper’s code can also be accessed here: github.com/LeryLee/vein_segmentation?tab=readme-ov-filew.\n\n\n\nHigh-resolution images: Each image in the dataset is 2736x3648 pixels, providing detailed views of leaf structures.\nPixel-wise annotations: Each leaf image comes with annotations that precisely mark the vein architecture, enabling fine-grained segmentation.\nVariety of leaf types: The dataset covers a wide range of plant species, allowing for diverse research opportunities in botany and plant phenotyping.\n\n\n\n\n\nLeaf vein segmentation: Develop models to extract vein features such as vein continuity, branching, and intersections. Models like U-Net can be adapted for this task.\nPlant phenotyping: Analyze leaf vein characteristics, which are important for understanding water and nutrient transport in plants. This can be useful for ecological and environmental monitoring.\nSelf-supervised learning: The dataset’s fine annotations make it suitable for self-supervised learning techniques like pseudo-labeling, which allows models to be trained with only a small number of labeled samples.\nSegmentation in noisy environments: Develop segmentation models that can handle complex leaf images with indistinguishable mesophyll, breakpoints, and blurred vein boundaries.\n\n\n\n\n\nQuantitative Plant Dataset: A resource for phenotyping plant structures that can be used alongside LVD2021 for plant-related research.\nUW-Madison Infected Tomato Leaf - Vein Segmentation: A Kaggle competition that focuses on segmenting primary and secondary vein structures from tomato leaf images at varying stages of disease progression."
  },
  {
    "objectID": "Toolbox/Data/LVD2021_Leaf-Vein-Dataset.html#about-this-resource",
    "href": "Toolbox/Data/LVD2021_Leaf-Vein-Dataset.html#about-this-resource",
    "title": "Leaf Vein Dataset (LVD2021)",
    "section": "",
    "text": "The Leaf Vein Dataset 2021 (LVD2021), introduced in the paper Leaf vein segmentation with self-supervision, contains high-resolution images of leaves with pixel-wise annotations of vein structures. This dataset can be used to develop machine learning models for leaf vein segmentation and plant phenotyping. The dataset includes 4977 images across 36 different types of leaves, with over 100 images per type. The pixel-level annotations make it suitable for training models for fine-grained segmentation tasks. To request access to the Leaf Vein Dataset 2021 (LVD2021), please fill out the request form.\nThe paper’s code can also be accessed here: github.com/LeryLee/vein_segmentation?tab=readme-ov-filew.\n\n\n\nHigh-resolution images: Each image in the dataset is 2736x3648 pixels, providing detailed views of leaf structures.\nPixel-wise annotations: Each leaf image comes with annotations that precisely mark the vein architecture, enabling fine-grained segmentation.\nVariety of leaf types: The dataset covers a wide range of plant species, allowing for diverse research opportunities in botany and plant phenotyping.\n\n\n\n\n\nLeaf vein segmentation: Develop models to extract vein features such as vein continuity, branching, and intersections. Models like U-Net can be adapted for this task.\nPlant phenotyping: Analyze leaf vein characteristics, which are important for understanding water and nutrient transport in plants. This can be useful for ecological and environmental monitoring.\nSelf-supervised learning: The dataset’s fine annotations make it suitable for self-supervised learning techniques like pseudo-labeling, which allows models to be trained with only a small number of labeled samples.\nSegmentation in noisy environments: Develop segmentation models that can handle complex leaf images with indistinguishable mesophyll, breakpoints, and blurred vein boundaries.\n\n\n\n\n\nQuantitative Plant Dataset: A resource for phenotyping plant structures that can be used alongside LVD2021 for plant-related research.\nUW-Madison Infected Tomato Leaf - Vein Segmentation: A Kaggle competition that focuses on segmenting primary and secondary vein structures from tomato leaf images at varying stages of disease progression."
  },
  {
    "objectID": "Toolbox/Data/LVD2021_Leaf-Vein-Dataset.html#questions",
    "href": "Toolbox/Data/LVD2021_Leaf-Vein-Dataset.html#questions",
    "title": "Leaf Vein Dataset (LVD2021)",
    "section": "Questions?",
    "text": "Questions?\nIf you have any lingering questions about this resource, feel free to post them on the ML+X Nexus Q&A on GitHub. We will update this resource as new information or applications arise."
  },
  {
    "objectID": "Toolbox/Data/LVD2021_Leaf-Vein-Dataset.html#see-also",
    "href": "Toolbox/Data/LVD2021_Leaf-Vein-Dataset.html#see-also",
    "title": "Leaf Vein Dataset (LVD2021)",
    "section": "See also",
    "text": "See also\n\nModels: U-Net: Explore one of the most popular segmentation models."
  },
  {
    "objectID": "Toolbox/Models/UNET.html",
    "href": "Toolbox/Models/UNET.html",
    "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
    "section": "",
    "text": "U-Net is a convolutional neural network architecture designed for biomedical image segmentation. Introduced in 2015 by Ronneberger and colleagues in the paper, “U-Net: Convolutional Networks for Biomedical Image Segmentation”, U-Net’s encoder-decoder architecture, combined with skip connections, allows for high accuracy in pixel-wise classification tasks. It remains one of the most widely used models for segmentation across various domains, from medical imaging to satellite image analysis.\n\n\n\nEncoder-Decoder Architecture: U-Net utilizes a contracting path (encoder) for context and an expansive path (decoder) for localization, making it effective in segmentation tasks.\nSkip Connections: These connections between encoder and decoder layers allow for the preservation of spatial information, leading to more accurate segmentation.\nData Efficiency: U-Net is effective even with relatively small datasets, a common scenario in medical and specialized imaging tasks."
  },
  {
    "objectID": "Toolbox/Models/UNET.html#about-this-resource",
    "href": "Toolbox/Models/UNET.html#about-this-resource",
    "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
    "section": "",
    "text": "U-Net is a convolutional neural network architecture designed for biomedical image segmentation. Introduced in 2015 by Ronneberger and colleagues in the paper, “U-Net: Convolutional Networks for Biomedical Image Segmentation”, U-Net’s encoder-decoder architecture, combined with skip connections, allows for high accuracy in pixel-wise classification tasks. It remains one of the most widely used models for segmentation across various domains, from medical imaging to satellite image analysis.\n\n\n\nEncoder-Decoder Architecture: U-Net utilizes a contracting path (encoder) for context and an expansive path (decoder) for localization, making it effective in segmentation tasks.\nSkip Connections: These connections between encoder and decoder layers allow for the preservation of spatial information, leading to more accurate segmentation.\nData Efficiency: U-Net is effective even with relatively small datasets, a common scenario in medical and specialized imaging tasks."
  },
  {
    "objectID": "Toolbox/Models/UNET.html#timeline-context",
    "href": "Toolbox/Models/UNET.html#timeline-context",
    "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
    "section": "Timeline context",
    "text": "Timeline context\nU-Net has been pivotal in advancing image segmentation since its introduction in 2015. Here is a timeline placing U-Net in the broader context of computer vision model development.\n\nLeNet (1998): One of the first CNN architectures for digit recognition.\nAlexNet (2012): Significantly improved CNN performance using deep learning and GPUs for large-scale image classification.\nVGGNet (2014): Simplified CNN architecture by using small convolutional filters, deeper layers.\nFully Convolutional Networks (FCN) (2014): Pioneered fully convolutional networks for image segmentation.\nSegNet (2015): Encoder-decoder architecture optimized for road scene segmentation.\nU-Net (2015): Designed for biomedical image segmentation with an encoder-decoder architecture and skip connections.\nResNet (2015): Introduced residual learning to address vanishing gradient problems in deep networks.\nMask R-CNN (2017): Extended Faster R-CNN for pixel-level segmentation tasks.\nVision Transformer (ViT) (2020): Applied transformer models for image classification tasks.\nSwin Transformer (2021): Hierarchical transformer for vision tasks with improved efficiency.\nSegment Anything (SAM) (2023): A foundation model for segmentation, offering high generalization across image domains."
  },
  {
    "objectID": "Toolbox/Models/UNET.html#u-net-variants",
    "href": "Toolbox/Models/UNET.html#u-net-variants",
    "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
    "section": "U-Net variants",
    "text": "U-Net variants\n\nAttention U-Net: Introduces attention mechanisms to U-Net for more accurate segmentation.\n3D U-Net: Designed for 3D medical imaging tasks such as volumetric segmentation.\nResUnet: Combines U-Net with residual connections for enhanced performance in complex tasks.\nnnU-Net: A self-configuring, state-of-the-art variant for deep learning-based biomedical image segmentation. nnU-Net adapts automatically to a given dataset, optimizing network topology, preprocessing, and postprocessing. Widely used in biomedical challenges and competitions, it serves as both a strong baseline and a development framework for researchers."
  },
  {
    "objectID": "Toolbox/Models/UNET.html#model-playground",
    "href": "Toolbox/Models/UNET.html#model-playground",
    "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
    "section": "Model playground",
    "text": "Model playground\n\nTutorials and Getting Started Notebooks\n\nnnU-Net: Scroll down on the nnU-Net GitHub README for documentation on installing, finetuning, and more.\n\n\n\nHigh-level tips for effective use\n\nPre-trained Encoders: Consider using pre-trained encoders from models like ResNet or EfficientNet to improve performance.\nRegularization Techniques: Apply dropout, early stopping, or weight decay to prevent overfitting, especially on small datasets.\nData Augmentation: Employ data augmentation techniques when working with small datasets to improve model generalization.\nOptimizing Loss Function: Use specialized loss functions such as Dice coefficient or Intersection over Union (IoU) for pixel-wise optimization.\nArchitectural Adjustments: Depending on your dataset size, experiment with deeper or shallower architectures to balance overfitting and underfitting risks.\n\n\n\nRelated datasets & Kaggle challenges\n\nMedical Decathlon Dataset: A popular benchmark dataset for biomedical image segmentation.\nUW Madison GI Tract Image Segmentation:Track healthy organs in medical scans to improve cancer treatment.\nISIC Skin Cancer Segmentation: Dataset and challenges for skin lesion segmentation."
  },
  {
    "objectID": "Toolbox/Models/UNET.html#questions",
    "href": "Toolbox/Models/UNET.html#questions",
    "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
    "section": "Questions?",
    "text": "Questions?\nIf you have any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Toolbox/Models/UNET.html#see-also",
    "href": "Toolbox/Models/UNET.html#see-also",
    "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
    "section": "See also",
    "text": "See also\n\nPlaylists: ML4MI Seminar. Biomedial applications of ML (especially computer vision) at UW-Madison.\nVideo: Vision, Language, and Vision-Language Modeling in Radiology: In this ML4MI seminar, Tyler Bradshaw highlights the history and current use of vision (e.g., UNET), language, and vision-language models in medical imaging.\nModel hub: MONAI - Medical Open Network for AI. An open-source, community-supported framework for deep learning in healthcare imaging\nWorkshop: Introduction to Deep Learning with PyTorch. Learn how to use PyTorch to build and train deep learning models."
  },
  {
    "objectID": "Learn/index.html",
    "href": "Learn/index.html",
    "title": "Learn",
    "section": "",
    "text": "Explore a library of educational materials (workshops, guides, books, videos, etc.) covering a wide range of ML-related topics, tools, and workflows. From foundational concepts to advanced techniques, these materials offer clear explanations, practical examples, and actionable insights to help you navigate the complexities of ML with confidence.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to AWS SageMaker for Predictive ML/AI\n\n\n\nWorkshops\n\n\nCode-along\n\n\nCompute\n\n\nAWS SageMaker\n\n\nPredictive modeling\n\n\nMachine learning\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-11-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrokking\n\n\n\nVideos\n\n\nDeep learning\n\n\nEmpirical patterns\n\n\nGrokking\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Python (Carpentries)\n\n\n\nWorkshops\n\n\nPython\n\n\nCarpentries\n\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Machine Learning with Sklearn (Carpentries)\n\n\n\nWorkshops\n\n\nLibraries\n\n\nClassical ML\n\n\nSklearn\n\n\nCarpentries\n\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Deep Learning with Keras (Carpentries)\n\n\n\nWorkshops\n\n\nLibraries\n\n\nDeep learning\n\n\nKeras\n\n\nCarpentries\n\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Deep Learning with PyTorch (Udacity)\n\n\n\nWorkshops\n\n\nLibraries\n\n\nDeep learning\n\n\nPyTorch\n\n\nUdacity\n\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-15\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Deep Learning\n\n\n\nBooks\n\n\nDeep learning\n\n\nPyTorch\n\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Text Analysis / NLP (Carpentries)\n\n\n\nWorkshops\n\n\nDeep learning\n\n\nHugging Face\n\n\nText analysis\n\n\nNLP\n\n\nLLM\n\n\nCarpentries\n\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-13\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview of Reproducibility Lecture\n\n\n\nVideos\n\n\nReproducibility\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVersion Control with GitHub Desktop\n\n\n\nGuides\n\n\nReproducibility\n\n\nGit/GitHub\n\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVersion Control with Git and GitHub (Carpentries)\n\n\n\nWorkshops\n\n\nVideos\n\n\nReproducibility\n\n\nGit/GitHub\n\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOut-of-Distribution Detection\n\n\n\nVideos\n\n\nOOD detection\n\n\nTrustworthy ML\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Contribute?\n\n\n\nGuides\n\n\nContribute\n\n\n\n\n\n\n\nML+X\n\n\n2024-06-24\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Learn"
    ]
  },
  {
    "objectID": "Learn/Workshops/Intro-TextAnalysis_Python.html",
    "href": "Learn/Workshops/Intro-TextAnalysis_Python.html",
    "title": "Intro to Text Analysis / NLP (Carpentries)",
    "section": "",
    "text": "The Intro to Text Analysis workshop introduces the field of Natural Language Processing (NLP) and how to gain insights from collections of text data (i.e., a corpus). This includes a hands-on, step-by-step guide on how to source and prepare a corpus for analysis, generate text (document/sentence/word) embeddings, perform topic modeling, deploy common models (e.g., Word2Vec and large language models using Hugging Face), and ethical considerations. Students and researchers working with text data (especially the digital humanities!) are encouraged to take this workshop!\n\n\nLearners are expected to have basic Python programming skills and familiarity with the Pandas package. If you need a refresher, the Introductory Python lesson materials are available for independent study.\n\n\n\nThis workshop takes approximately 16 hours to complete.\n\n\n\nThe Carpentries is a global organization of researchers who volunteer their time and effort to create workshops that teach software engineering and data analysis skills to other researchers. UW-Madison has its own local Carpentries community which is actively engaged in developing new ML/AI workshops. To be notified of upcoming workshops offered by the Carpentries, make sure to subscribe to the Data Science @ UW Newsletter.\n\n\n\nAll Carpentries lessons are published as open source educational materials. You are welcome and encouraged to visit the lesson materials to work through them on your own. If you are involved with a research lab at UW-Madison campus, you may attend Coding Meetup (Tue/Thur, 2:30-4:30pm) to get help working through the materials.\n\n\n\n\nWorkshop: Intro to Deep Learning with Keras: Explore deep learning concepts in greater detail. This will help you better understand the technology (neural networks) needed for Word2Vec and large language models.\nBook: Understanding Deep Learning - Simon J.D. Prince: This free textbook is a good modern overview of deep learning (and machine learning in general), and provides colab notebooks to explore deep learning concepts and implementations. The book uses PyTorch as its framework of choice. You may find additional details in this book that the workshop only briefly touches on."
  },
  {
    "objectID": "Learn/Workshops/Intro-TextAnalysis_Python.html#about-this-resource",
    "href": "Learn/Workshops/Intro-TextAnalysis_Python.html#about-this-resource",
    "title": "Intro to Text Analysis / NLP (Carpentries)",
    "section": "",
    "text": "The Intro to Text Analysis workshop introduces the field of Natural Language Processing (NLP) and how to gain insights from collections of text data (i.e., a corpus). This includes a hands-on, step-by-step guide on how to source and prepare a corpus for analysis, generate text (document/sentence/word) embeddings, perform topic modeling, deploy common models (e.g., Word2Vec and large language models using Hugging Face), and ethical considerations. Students and researchers working with text data (especially the digital humanities!) are encouraged to take this workshop!\n\n\nLearners are expected to have basic Python programming skills and familiarity with the Pandas package. If you need a refresher, the Introductory Python lesson materials are available for independent study.\n\n\n\nThis workshop takes approximately 16 hours to complete.\n\n\n\nThe Carpentries is a global organization of researchers who volunteer their time and effort to create workshops that teach software engineering and data analysis skills to other researchers. UW-Madison has its own local Carpentries community which is actively engaged in developing new ML/AI workshops. To be notified of upcoming workshops offered by the Carpentries, make sure to subscribe to the Data Science @ UW Newsletter.\n\n\n\nAll Carpentries lessons are published as open source educational materials. You are welcome and encouraged to visit the lesson materials to work through them on your own. If you are involved with a research lab at UW-Madison campus, you may attend Coding Meetup (Tue/Thur, 2:30-4:30pm) to get help working through the materials.\n\n\n\n\nWorkshop: Intro to Deep Learning with Keras: Explore deep learning concepts in greater detail. This will help you better understand the technology (neural networks) needed for Word2Vec and large language models.\nBook: Understanding Deep Learning - Simon J.D. Prince: This free textbook is a good modern overview of deep learning (and machine learning in general), and provides colab notebooks to explore deep learning concepts and implementations. The book uses PyTorch as its framework of choice. You may find additional details in this book that the workshop only briefly touches on."
  },
  {
    "objectID": "Learn/Workshops/index.html",
    "href": "Learn/Workshops/index.html",
    "title": "Workshops",
    "section": "",
    "text": "UW-Madison has its own local Carpentries community which is actively engaged in developing and teaching new ML/AI workshops. To be notified of upcoming workshops offered by the Carpentries and other organizations at UW-Madison, make sure to subscribe to the Data Science @ UW Newsletter.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to AWS SageMaker for Predictive ML/AI\n\n\n\nWorkshops\n\n\nCode-along\n\n\nCompute\n\n\nAWS SageMaker\n\n\nPredictive modeling\n\n\nMachine learning\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-11-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Python (Carpentries)\n\n\n\nWorkshops\n\n\nPython\n\n\nCarpentries\n\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Machine Learning with Sklearn (Carpentries)\n\n\n\nWorkshops\n\n\nLibraries\n\n\nClassical ML\n\n\nSklearn\n\n\nCarpentries\n\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Deep Learning with Keras (Carpentries)\n\n\n\nWorkshops\n\n\nLibraries\n\n\nDeep learning\n\n\nKeras\n\n\nCarpentries\n\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Deep Learning with PyTorch (Udacity)\n\n\n\nWorkshops\n\n\nLibraries\n\n\nDeep learning\n\n\nPyTorch\n\n\nUdacity\n\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-15\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Text Analysis / NLP (Carpentries)\n\n\n\nWorkshops\n\n\nDeep learning\n\n\nHugging Face\n\n\nText analysis\n\n\nNLP\n\n\nLLM\n\n\nCarpentries\n\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-13\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVersion Control with Git and GitHub (Carpentries)\n\n\n\nWorkshops\n\n\nVideos\n\n\nReproducibility\n\n\nGit/GitHub\n\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-11\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Learn",
      "Workshops"
    ]
  },
  {
    "objectID": "Learn/Workshops/Intro-Deeplearning_Keras.html",
    "href": "Learn/Workshops/Intro-Deeplearning_Keras.html",
    "title": "Intro to Deep Learning with Keras (Carpentries)",
    "section": "",
    "text": "The Intro to Deep Learning with Keras workshop from the Carpentries will walk you through introductory deep learning concepts as well as how to build a neural networks in Keras. Keras is high-level wrapper framework (uses PyTorch or Tensorflow in the backend) which allows you to train and evaluate neural networks in just a few lines of code. It may take slightly longer to train a Keras model (compared to PyTorch and Tensorflow), but the difference in performance is often negligible for those that only need to train a few models. The ability to quickly build and test models is the primary selling point of Keras.\n\n\nLearners are expected to have the following knowledge:\n\nBasic Python programming skills and familiarity with the Pandas package. If you need a refresher, these Introductory Python lesson materials are available for independent study.\nBasic knowledge on machine learning, including the following concepts: Data cleaning, train & test split, type of problems (regression, classification), overfitting & underfitting, metrics (accuracy, recall, etc.).The Intro to Machine Learning with Sklearn lesson materials are a good option for those that need a refresher on machine learning fundamentals.\n\n\n\n\nThis workshop takes approximately 15 hours to complete.\n\n\n\nThe Carpentries is a global organization of researchers who volunteer their time and effort to create workshops that teach software engineering and data analysis skills to other researchers. UW-Madison has its own local Carpentries community which is actively engaged in developing new ML/AI workshops. To be notified of upcoming workshops offered by the Carpentries, make sure to subscribe to the Data Science @ UW Newsletter. The Intro Deep Learning workshop is typically taught in May each year.\n\n\n\nAll Carpentries lessons are published as open source educational materials. You are welcome and encouraged to visit the lesson materials to work through them on your own. If you are involved with a research lab at UW-Madison campus, you may attend Coding Meetup (Tue/Thur, 2:30-4:30pm) to get help working through the materials."
  },
  {
    "objectID": "Learn/Workshops/Intro-Deeplearning_Keras.html#about-this-resource",
    "href": "Learn/Workshops/Intro-Deeplearning_Keras.html#about-this-resource",
    "title": "Intro to Deep Learning with Keras (Carpentries)",
    "section": "",
    "text": "The Intro to Deep Learning with Keras workshop from the Carpentries will walk you through introductory deep learning concepts as well as how to build a neural networks in Keras. Keras is high-level wrapper framework (uses PyTorch or Tensorflow in the backend) which allows you to train and evaluate neural networks in just a few lines of code. It may take slightly longer to train a Keras model (compared to PyTorch and Tensorflow), but the difference in performance is often negligible for those that only need to train a few models. The ability to quickly build and test models is the primary selling point of Keras.\n\n\nLearners are expected to have the following knowledge:\n\nBasic Python programming skills and familiarity with the Pandas package. If you need a refresher, these Introductory Python lesson materials are available for independent study.\nBasic knowledge on machine learning, including the following concepts: Data cleaning, train & test split, type of problems (regression, classification), overfitting & underfitting, metrics (accuracy, recall, etc.).The Intro to Machine Learning with Sklearn lesson materials are a good option for those that need a refresher on machine learning fundamentals.\n\n\n\n\nThis workshop takes approximately 15 hours to complete.\n\n\n\nThe Carpentries is a global organization of researchers who volunteer their time and effort to create workshops that teach software engineering and data analysis skills to other researchers. UW-Madison has its own local Carpentries community which is actively engaged in developing new ML/AI workshops. To be notified of upcoming workshops offered by the Carpentries, make sure to subscribe to the Data Science @ UW Newsletter. The Intro Deep Learning workshop is typically taught in May each year.\n\n\n\nAll Carpentries lessons are published as open source educational materials. You are welcome and encouraged to visit the lesson materials to work through them on your own. If you are involved with a research lab at UW-Madison campus, you may attend Coding Meetup (Tue/Thur, 2:30-4:30pm) to get help working through the materials."
  },
  {
    "objectID": "Learn/Workshops/Intro-Deeplearning_Keras.html#questions",
    "href": "Learn/Workshops/Intro-Deeplearning_Keras.html#questions",
    "title": "Intro to Deep Learning with Keras (Carpentries)",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Learn/Workshops/Intro-Deeplearning_Keras.html#see-also",
    "href": "Learn/Workshops/Intro-Deeplearning_Keras.html#see-also",
    "title": "Intro to Deep Learning with Keras (Carpentries)",
    "section": "See also",
    "text": "See also\n\nWorkshop: Intro to Deep Learning with PyTorch: Explore PyTorch as an alternative deep learning framework.\nBook: Understanding Deep Learning - Simon J.D. Prince: This free textbook is a good modern overview of deep learning, and provides colab notebooks to explore deep learning concepts and implementations. The book uses PyTorch as its framework of choice. You may find additional details in this book that the workshop only briefly touches on."
  },
  {
    "objectID": "Learn/Books/index.html",
    "href": "Learn/Books/index.html",
    "title": "Books",
    "section": "",
    "text": "Understanding Deep Learning\n\n\n\nBooks\n\n\nDeep learning\n\n\nPyTorch\n\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-14\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Learn",
      "Books"
    ]
  },
  {
    "objectID": "Learn/Guides/index.html",
    "href": "Learn/Guides/index.html",
    "title": "Guides",
    "section": "",
    "text": "Version Control with GitHub Desktop\n\n\n\nGuides\n\n\nReproducibility\n\n\nGit/GitHub\n\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Contribute?\n\n\n\nGuides\n\n\nContribute\n\n\n\n\n\n\n\nML+X\n\n\n2024-06-24\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Learn",
      "Guides"
    ]
  },
  {
    "objectID": "Learn/Guides/How-to-contribute.html",
    "href": "Learn/Guides/How-to-contribute.html",
    "title": "How to Contribute?",
    "section": "",
    "text": "We want Nexus to serve also as a place where members of the community can share their knowledge. This guide answers the question, how to contribute to Nexus?",
    "crumbs": [
      "How to contribute?"
    ]
  },
  {
    "objectID": "Learn/Guides/How-to-contribute.html#what-kinds-of-resources-are-hosted-on-nexus",
    "href": "Learn/Guides/How-to-contribute.html#what-kinds-of-resources-are-hosted-on-nexus",
    "title": "How to Contribute?",
    "section": "What kinds of resources are hosted on Nexus?",
    "text": "What kinds of resources are hosted on Nexus?\nAny content (original or external) that can help make the practice of ML more connected, accessible, efficient, and reproducible is welcome on the Nexus platform! This includes, but is not limited to…\n\n🧠 Educational materials: Explore a library of educational materials (workshops, guides, books, videos, etc.) covering a wide range of ML-related topics, tools, and workflows, from foundational concepts to advanced techniques. These materials offer clear explanations, practical examples, and actionable insights to help you navigate the complexities of ML with confidence.\n🛠 Models, code, and more: Learn about popular pretrained & foundation models, useful scripts, and datasets that you can leverage for your next ML project. Learn about their features, how to use them effectively, and see examples of them in action.\n🧬 Applications & stories: Discover a curated collection of blogs, papers, and talks which dive into real-world ML applications and lessons learned by practitioners. This section also includes exploratory data analysis (EDA) case studies, which demonstrate the technical and domain knowledge needed to explore data from various fields.",
    "crumbs": [
      "How to contribute?"
    ]
  },
  {
    "objectID": "Learn/Guides/How-to-contribute.html#need-inspiration-for-a-good-topic-to-post-about",
    "href": "Learn/Guides/How-to-contribute.html#need-inspiration-for-a-good-topic-to-post-about",
    "title": "How to Contribute?",
    "section": "Need inspiration for a good topic to post about?",
    "text": "Need inspiration for a good topic to post about?\nAn ever-expanding list of requested resources can be found on the Issues page (on GitHub). Search for open issues that have the “Resource” label to check out some of our top priorities. If you’d like to tackle a given issue, please comment on the issue to let others know.",
    "crumbs": [
      "How to contribute?"
    ]
  },
  {
    "objectID": "Learn/Guides/How-to-contribute.html#what-makes-a-good-post",
    "href": "Learn/Guides/How-to-contribute.html#what-makes-a-good-post",
    "title": "How to Contribute?",
    "section": "What makes a good post?",
    "text": "What makes a good post?\nCreating a useful and engaging post for the ML+X Nexus involves a few key elements to ensure it is beneficial for the community. Here are some general guidelines to follow:\n\nClear and concise title\nThe title should accurately reflect the content and main focus of the post. It should be engaging and specific, allowing readers to quickly understand what they can expect.\n\n\nDetailed description\nProvide a comprehensive overview of the resource or topic. This should include:\n\nPurpose and scope: Clearly state what the resource covers and its main objectives. Explain why the content is valuable and how it can help practitioners.\nKey features: Highlight the unique aspects or strengths of the resource. This could include practical examples, interactive elements, or real-world applications.\nStrengths and weaknesses: Provide a balanced view by discussing both the strengths and any potential limitations of the resource. This helps users make informed decisions about whether the resource is right for them.\n\n\n\nPrerequisites\nList any necessary background knowledge or skills required to fully benefit from the resource. This helps set expectations and ensures that users are adequately prepared. Include links to additional resources or tutorials that can help users gain the required knowledge.\n\n\nEstimated time to complete\nOffer an estimate of the time commitment needed to complete the resource. This helps users plan their learning activities and manage their time effectively.\n\n\nAccessibility and usability\nEnsure the resource is easy to access and use. We want the majority of resources on Nexus to be free and open source (with possibly a few rare exceptions for tools/resources in high-demand). Provide clear instructions on how to navigate and utilize the content. If the resource is hosted externally, include a direct link and any necessary login or access information.\n\n\nAdditional related resources\nInclude links to related materials or further readings that can enhance the user’s understanding and provide more in-depth knowledge on the topic. This can include books, articles, other workshops, or case studies. When possible, link to any relevant materials which are already hosted on the Nexus platform.\n\n\nExamples of good posts\nPlease see below for a list of resources that meet our platform’s standards. You can use these examples in conjunction with the template files provided in the next section to create your post.\n\nExternal content\n\nLearn: Workshop\nLearn: Book\nLearn: Video\nApplication: Video\nToolbox: Data\n\n\n\nOriginal content\n\nLearn: Guide\nApplication: Blog\nApplication: EDA",
    "crumbs": [
      "How to contribute?"
    ]
  },
  {
    "objectID": "Learn/Guides/How-to-contribute.html#how-to-make-a-new-post-with-github",
    "href": "Learn/Guides/How-to-contribute.html#how-to-make-a-new-post-with-github",
    "title": "How to Contribute?",
    "section": "How to make a new post with GitHub?",
    "text": "How to make a new post with GitHub?\n\nGitHub collaboration model\nWe follow GitHub’s collaboration model, so the general idea to make a post or edit a document is the same. The high-level steps include:\n\nCreate an issue announcing your plan to add a resource — see ML+X Nexus Issues\nFork the ML+X-Nexus repository\nClone the forked repository onto your local machine\nCreate a new branch\nWrite the post, commit and push the changes\nMake a pull request\n\nIf you don’t know how to use Git / GitHub already, it can be a little intimidating at first. A friendlier alternative could be to download GitHub desktop and add your post using the instructions provided below. If you’d like to learn more about GitHub Desktop, check out the Version Control with GitHub Desktop guide on Nexus. If you need additional help (and work in a research lab at UW-Madison), you may also seek help at the Data Science Hub’s office hours (“Coding Meetup”).\n\n1. Get Started with GitHub Desktop\n\nGo to the GitHub Desktop website and download the application for your operating system. Install GitHub Desktop by following the on-screen instructions.\nOpen GitHub Desktop and sign in with your GitHub account. If you don’t have one, you will need to create your GitHub account first.\n\n\n\n2. Create an Issue on Nexus GitHub\nBefore you start writing your content, create an issue on the Nexus GitHub to announce your intended addition. This helps the Nexus development team keep track of new contributions and provides an opportunity for feedback.\n\nGo to the Nexus GitHub Issues page\nClick on the New Issue button.\nTitle the issue with the name of your resource, and add a “Resource” label/tag (found on right side panel of issue post).\nDescribe why you think this resource should be included on the Nexus platform.\nWait for feedback: Wait for one of the Nexus developers to provide feedback or comments on your issue before proceeding.\n\n\n\n3. Fork the Repository\n\nGo to the ML+X-Nexus repository on GitHub.\nClick the Fork button at the top-right corner of the page. This will create a copy of the repository under your GitHub account.\n\n\n\n4. Clone the Repository to Your System to Your Local System\n\nFrom your new forked version of the repo on GitHub, click the green Code button to copy the HTTPS URL of the fork\nIn GitHub Desktop, click on File &gt; Clone Repository.\nPaste the URL and click Clone\n\n\n\n5. Create a New Branch\n\nIn GitHub Desktop, click on Branch &gt; New Branch.\nName your new branch descriptively based on the resource type and name (e.g., workshop-introDL, video-NeurIPS2024, etc.).\n\n\n\n6. Write Your Post\n\nOpen your favorite text editor or IDE (e.g., Visual Studio Code, Sublime Text).\nWrite your post in the appropriate format. Follow the guidelines in the next section for writing a good post, making use the template file provided.\n\n\n\n7. Commit and Push Changes\n\nIn GitHub Desktop, you should see your changes listed under Changes.\nWrite a descriptive commit message (e.g., Add new resource: workshop-introDL).\nClick Commit to your-branch-name.\nClick on Repository &gt; Push to push your changes to GitHub.\n\n\n\n8. Make a Pull Request\n\nGo to your forked repository on GitHub.\nClick on the Compare & pull request button.\nEnsure you are merging into the “main” branch of the ML+X-Nexus repository.\nWrite a descriptive title and comment for your pull request.\nClick Create pull request.\n\n\n\n9. Wait for Review\n\nOne of the Nexus developers will review your pull request. They may provide feedback or request changes.\nAddress any feedback and push additional commits as needed.\n\n\n\n\nHow to write the post? Start with a template!\nIf applicable, start with one of the relevant template files linked below. There are comments in each template that will help you make the appropriate edits for your resource. You can also check out how other posts have been formatted by clicking “Improve this page” from a given post’s webpage. This will bring you directly to the qmd file for that post.\n\nEducational - video: lecture, demo, recorded workshop, etc.\nEducational - general: workshop materials, book, guide, etc.\nApplication - video\nApplication - blog\nToolbox - data\nToolbox - model\n\n\n\nWhere to locate your post?\nWe want the site to be constantly evolving with the community, and our intention is to keep the contributions to the site as free as possible. However, we added some sections to structure the site a little bit:\n\n├── Applications\n│   ├── Blogs\n│   ├── EDA\n│   ├── Videos\n│   ├── Papers\n│   ├── Playlists\n├── Learn\n│   ├── Books\n│   ├── Guides\n│   ├── Videos\n│   ├── Workshops\n├── Toolbox\n│   ├── Code\n│   ├── Compute\n│   ├── Data \n│   ├── Models \nNote: Some subfolders may not exist yet (e.g., Code, Data) since no one has contributed a resource from one of those categories yet, and git doesn’t allow empty folders. Feel free to start one of the missing folders, if applicable. If your resource doesn’t belong to one of the categories listed above, you may add a new one. We’ll discuss this new category when we review your initial “Issue” announcing the resource addition.",
    "crumbs": [
      "How to contribute?"
    ]
  },
  {
    "objectID": "Learn/Guides/How-to-contribute.html#how-to-improve-an-existing-post",
    "href": "Learn/Guides/How-to-contribute.html#how-to-improve-an-existing-post",
    "title": "How to Contribute?",
    "section": "How to improve an existing post?",
    "text": "How to improve an existing post?\nWant to add a code-long exercise to an existing post, add your perspective, or correct a typo? Anyone is welcome and encouraged to suggest improvements to existing materials hosted on Nexus! The most straightforward way to do this is to click “Improve this page” from the post’s webpage on Nexus to suggest your edits. The below steps will walk you through this process.\n\nSteps to improve a post on Nexus via GitHub (no software installation needed):\n\nClick “Improve this page”: This will redirect you to the GitHub repository where the content of the post is stored.\nEdit the file directly on GitHub:\n\nOn the GitHub page, click the pencil icon (✏️) at the top right of the file to edit the content directly in your browser. No need to install any git software!\nMake your changes in the text editor. You can add a code-long exercise, share your perspective, or correct any typos.\n\nCommit your changes:\n\nAfter you have made your edits, scroll down to the “Commit changes” section.\nWrite a brief description of what you changed.\nChoose the option to “Create a new branch for this commit and start a pull request.”\n\nCreate a pull request:\n\nClick the “Propose changes” button.\nYou will be taken to a new page where you can review your changes.\nClick “Create pull request” to submit your changes for review.\n\n\nCongratulations! You have suggested an improvement to a Nexus post. The repository maintainers will review your pull request, and if everything looks good, your changes will be merged into the post.\nNote: While you can make these edits directly on GitHub without any software installation, we recommend all ML practitioners learn git! Check out our git category search resources.",
    "crumbs": [
      "How to contribute?"
    ]
  },
  {
    "objectID": "Learn/Guides/How-to-contribute.html#questions",
    "href": "Learn/Guides/How-to-contribute.html#questions",
    "title": "How to Contribute?",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions on how to contribute, please feel free to post to the Nexus Q&A on GitHub. We will improve this guide based on additional questions/comments we receive.",
    "crumbs": [
      "How to contribute?"
    ]
  },
  {
    "objectID": "Learn/Videos/OOD-detection.html",
    "href": "Learn/Videos/OOD-detection.html",
    "title": "Out-of-Distribution Detection",
    "section": "",
    "text": "The below tutorial from Sharon Li, an Assistant Professor in the Department of Computer Sciences at the University of Wisconsin-Madison, introduces a pervasive problem faced by many machine learning systems deployed in the wild — out-of-distribution data.\nOut-of-distribution data, often overlooked but immensely consequential, poses a significant threat to the reliability and efficacy of machine learning models. Through Sharon’s presentation, viewers gain a comprehensive understanding of this complex phenomenon and its potential ramifications on predictive accuracy.\nCheck out the video below to learn more about this problem and the cutting-edge methods you can equip yourself with to prevent inaccurate model predictions.\n\n\nDetails, slides and videos from other talks at ICCV 2023: abursuc.github.io/many-faces-reliability/\n\n\n\n\nCheck this page again soon for worked examples and exercises (code provided)!"
  },
  {
    "objectID": "Learn/Videos/OOD-detection.html#about-this-resource",
    "href": "Learn/Videos/OOD-detection.html#about-this-resource",
    "title": "Out-of-Distribution Detection",
    "section": "",
    "text": "The below tutorial from Sharon Li, an Assistant Professor in the Department of Computer Sciences at the University of Wisconsin-Madison, introduces a pervasive problem faced by many machine learning systems deployed in the wild — out-of-distribution data.\nOut-of-distribution data, often overlooked but immensely consequential, poses a significant threat to the reliability and efficacy of machine learning models. Through Sharon’s presentation, viewers gain a comprehensive understanding of this complex phenomenon and its potential ramifications on predictive accuracy.\nCheck out the video below to learn more about this problem and the cutting-edge methods you can equip yourself with to prevent inaccurate model predictions.\n\n\nDetails, slides and videos from other talks at ICCV 2023: abursuc.github.io/many-faces-reliability/\n\n\n\n\nCheck this page again soon for worked examples and exercises (code provided)!"
  },
  {
    "objectID": "Learn/Videos/OOD-detection.html#questions",
    "href": "Learn/Videos/OOD-detection.html#questions",
    "title": "Out-of-Distribution Detection",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Learn/Videos/index.html",
    "href": "Learn/Videos/index.html",
    "title": "Videos",
    "section": "",
    "text": "Grokking\n\n\n\nVideos\n\n\nDeep learning\n\n\nEmpirical patterns\n\n\nGrokking\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview of Reproducibility Lecture\n\n\n\nVideos\n\n\nReproducibility\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVersion Control with Git and GitHub (Carpentries)\n\n\n\nWorkshops\n\n\nVideos\n\n\nReproducibility\n\n\nGit/GitHub\n\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOut-of-Distribution Detection\n\n\n\nVideos\n\n\nOOD detection\n\n\nTrustworthy ML\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-11\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Learn",
      "Videos"
    ]
  },
  {
    "objectID": "Learn/Videos/Reproducibility-overview.html",
    "href": "Learn/Videos/Reproducibility-overview.html",
    "title": "Overview of Reproducibility Lecture",
    "section": "",
    "text": "Dr. Sarah Stevens’ lecture highlights the critical importance of reproducibility in computational and data science projects. She also shares best practices to ensure reproducible results including:\n\nHow to organize your project\nGood names for files/folders\nDocumenting your work and README files\nHow to organize data in spreadsheets and use data dictionaries\nAutomation with scripts\nVersion control\nLicensing\nCreating backups"
  },
  {
    "objectID": "Learn/Videos/Reproducibility-overview.html#about-this-resource",
    "href": "Learn/Videos/Reproducibility-overview.html#about-this-resource",
    "title": "Overview of Reproducibility Lecture",
    "section": "",
    "text": "Dr. Sarah Stevens’ lecture highlights the critical importance of reproducibility in computational and data science projects. She also shares best practices to ensure reproducible results including:\n\nHow to organize your project\nGood names for files/folders\nDocumenting your work and README files\nHow to organize data in spreadsheets and use data dictionaries\nAutomation with scripts\nVersion control\nLicensing\nCreating backups"
  },
  {
    "objectID": "Learn/Videos/Reproducibility-overview.html#questions",
    "href": "Learn/Videos/Reproducibility-overview.html#questions",
    "title": "Overview of Reproducibility Lecture",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Learn/Videos/Reproducibility-overview.html#see-also",
    "href": "Learn/Videos/Reproducibility-overview.html#see-also",
    "title": "Overview of Reproducibility Lecture",
    "section": "See also",
    "text": "See also\n\nGuide: Version Control with GitHub Desktop: GitHub Desktop is a graphical user interface (GUI) application that simplifies the use of Git and GitHub. It is designed for users who prefer not to use the command line interface, offering a more intuitive and visual approach to version control. With GitHub Desktop, you can easily perform common Git tasks such as committing changes, creating branches, and resolving merge conflicts, all within a user-friendly interface.\nWorkshop: Intro to Version Control with Git: If you’re curious to learn how to use Git via shell commands (or just want to become more fluent with Git), check out this YouTube playlist from the Data Science Hub!"
  },
  {
    "objectID": "Learn/Videos/Intro-git-github.html",
    "href": "Learn/Videos/Intro-git-github.html",
    "title": "Version Control with Git and GitHub (Carpentries)",
    "section": "",
    "text": "The below video (and the 7 subsequent videos in the workshop playlist) will walk you through this introductory Git workshop from the Carpentries: Version Control with Git/GitHub. Git is a free and open source version control system that has become the #1 choice for software developers both in research and industry. Unlike centralized version control systems where there is a single central repository, Git allows every user to have a full copy of the entire project history on their own machine. This distributed nature enables multiple people to work on a project simultaneously without interfering with each other’s work. Git stores the history of changes in a project, enabling users to track progress, revert to previous states, and manage branches for different features or versions of a project. GitHub is a web-based platform that uses Git for version control. It provides a collaborative environment where users can host and review code, manage projects, and build software alongside millions of other developers. GitHub also offers additional features such as issue tracking, project management tools, and continuous integration workflows.\n\n\n\nIn this lesson we use Git from the Unix Shell. Some previous experience with the shell is expected, but isn’t mandatory. For help with Unix Shell, check out the Intro to Unix Shell workshop (Carpentries).\n\n\n\n\n\nThis workshop takes rough 3-4 hours to complete.\n\n\n\n\nThe Carpentries is a global organization of researchers who volunteer their time and effort to create workshops that teach software engineering and data analysis skills to other researchers. UW-Madison has its own local Carpentries community which is actively engaged in developing new ML/AI workshops. To be notified of upcoming workshops offered by the Carpentries, make sure to subscribe to the Data Science @ UW Newsletter."
  },
  {
    "objectID": "Learn/Videos/Intro-git-github.html#about-this-resource",
    "href": "Learn/Videos/Intro-git-github.html#about-this-resource",
    "title": "Version Control with Git and GitHub (Carpentries)",
    "section": "",
    "text": "The below video (and the 7 subsequent videos in the workshop playlist) will walk you through this introductory Git workshop from the Carpentries: Version Control with Git/GitHub. Git is a free and open source version control system that has become the #1 choice for software developers both in research and industry. Unlike centralized version control systems where there is a single central repository, Git allows every user to have a full copy of the entire project history on their own machine. This distributed nature enables multiple people to work on a project simultaneously without interfering with each other’s work. Git stores the history of changes in a project, enabling users to track progress, revert to previous states, and manage branches for different features or versions of a project. GitHub is a web-based platform that uses Git for version control. It provides a collaborative environment where users can host and review code, manage projects, and build software alongside millions of other developers. GitHub also offers additional features such as issue tracking, project management tools, and continuous integration workflows.\n\n\n\nIn this lesson we use Git from the Unix Shell. Some previous experience with the shell is expected, but isn’t mandatory. For help with Unix Shell, check out the Intro to Unix Shell workshop (Carpentries).\n\n\n\n\n\nThis workshop takes rough 3-4 hours to complete.\n\n\n\n\nThe Carpentries is a global organization of researchers who volunteer their time and effort to create workshops that teach software engineering and data analysis skills to other researchers. UW-Madison has its own local Carpentries community which is actively engaged in developing new ML/AI workshops. To be notified of upcoming workshops offered by the Carpentries, make sure to subscribe to the Data Science @ UW Newsletter."
  },
  {
    "objectID": "Learn/Videos/Intro-git-github.html#questions",
    "href": "Learn/Videos/Intro-git-github.html#questions",
    "title": "Version Control with Git and GitHub (Carpentries)",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Learn/Videos/Intro-git-github.html#see-also",
    "href": "Learn/Videos/Intro-git-github.html#see-also",
    "title": "Version Control with Git and GitHub (Carpentries)",
    "section": "See also",
    "text": "See also\n\nGuide: Version Control with GitHub Desktop: GitHub Desktop is a graphical user interface (GUI) application that simplifies the use of Git and GitHub. It is designed for users who prefer not to use the command line interface, offering a more intuitive and visual approach to version control. With GitHub Desktop, you can easily perform common Git tasks such as committing changes, creating branches, and resolving merge conflicts, all within a user-friendly interface.\nVideo: Reproducibility Overview Lecture: If you’re curious to learn how to use Git via shell commands (or just want to become more fluent with Git), check out this YouTube playlist from the Data Science Hub!"
  },
  {
    "objectID": "Learn/Videos/Grokking.html",
    "href": "Learn/Videos/Grokking.html",
    "title": "Grokking",
    "section": "",
    "text": "The verb, “to grok”, was originally coined by Robert A. Heinlein in his 1961 science fiction novel “Stranger in a Strange Land,” where it meant to understand something so thoroughly that it becomes a part of oneself. In the context of machine learning, “grokking” refers to the phenomenon whereby a model, after extensive training, suddenly shifts from merely memorizing data to achieving a deep and intuitive understanding, allowing it to generalize effectively to new, unseen data. Observing this phenomenon requires a significantly large increase in training iterations, often far beyond the usual training duration expected for a model to reach acceptable performance. This prolonged training period initially shows no improvement in generalization, making the eventual transition to grokking both surprising and significant.\nThe grokking phenomenon was recently investigated in a notable paper from OpenAI (Power et al., 2021). The video you’ll be watching will explain OpenAI’s findings on this transition to generalization, highlighting the dramatic increase in iterations needed and providing a deeper understanding of the process and its implications for developing more robust and reliable machine learning models."
  },
  {
    "objectID": "Learn/Videos/Grokking.html#about-this-resource",
    "href": "Learn/Videos/Grokking.html#about-this-resource",
    "title": "Grokking",
    "section": "",
    "text": "The verb, “to grok”, was originally coined by Robert A. Heinlein in his 1961 science fiction novel “Stranger in a Strange Land,” where it meant to understand something so thoroughly that it becomes a part of oneself. In the context of machine learning, “grokking” refers to the phenomenon whereby a model, after extensive training, suddenly shifts from merely memorizing data to achieving a deep and intuitive understanding, allowing it to generalize effectively to new, unseen data. Observing this phenomenon requires a significantly large increase in training iterations, often far beyond the usual training duration expected for a model to reach acceptable performance. This prolonged training period initially shows no improvement in generalization, making the eventual transition to grokking both surprising and significant.\nThe grokking phenomenon was recently investigated in a notable paper from OpenAI (Power et al., 2021). The video you’ll be watching will explain OpenAI’s findings on this transition to generalization, highlighting the dramatic increase in iterations needed and providing a deeper understanding of the process and its implications for developing more robust and reliable machine learning models."
  },
  {
    "objectID": "Learn/Videos/Grokking.html#questions",
    "href": "Learn/Videos/Grokking.html#questions",
    "title": "Grokking",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Learn/Guides/Github-desktop.html",
    "href": "Learn/Guides/Github-desktop.html",
    "title": "Version Control with GitHub Desktop",
    "section": "",
    "text": "Navigating the world of version control systems like Git can initially feel daunting, especially for those new to programming or collaborative software development projects. However, with the right tools and guidance, anyone can quickly grasp the essentials and begin leveraging the power of Git for efficient project management and collaboration. While Git commands can be run via a Unix shell, there are alternatives which are more friendly for beginners. In this guide, we’ll explore GitHub Desktop as a convenient and accessible gateway to Git, along with a step-by-step walkthrough on essential Git terminology, setup procedures, tracking changes, collaboration workflows, and even managing Kaggle notebooks seamlessly. Whether you’re embarking on your first coding adventure or seeking to streamline your team’s development process, this guide aims to demystify Git and empower you with practical knowledge to navigate the Git landscape with confidence."
  },
  {
    "objectID": "Learn/Guides/Github-desktop.html#about-this-resource",
    "href": "Learn/Guides/Github-desktop.html#about-this-resource",
    "title": "Version Control with GitHub Desktop",
    "section": "",
    "text": "Navigating the world of version control systems like Git can initially feel daunting, especially for those new to programming or collaborative software development projects. However, with the right tools and guidance, anyone can quickly grasp the essentials and begin leveraging the power of Git for efficient project management and collaboration. While Git commands can be run via a Unix shell, there are alternatives which are more friendly for beginners. In this guide, we’ll explore GitHub Desktop as a convenient and accessible gateway to Git, along with a step-by-step walkthrough on essential Git terminology, setup procedures, tracking changes, collaboration workflows, and even managing Kaggle notebooks seamlessly. Whether you’re embarking on your first coding adventure or seeking to streamline your team’s development process, this guide aims to demystify Git and empower you with practical knowledge to navigate the Git landscape with confidence."
  },
  {
    "objectID": "Learn/Guides/Github-desktop.html#getting-started",
    "href": "Learn/Guides/Github-desktop.html#getting-started",
    "title": "Version Control with GitHub Desktop",
    "section": "Getting Started",
    "text": "Getting Started\n\nVersion Control\nVersion control is system that records changes to a file or set of files over time so that you can recall specific versions later. It helps in managing changes, keeping track of different versions, and collaborating with multiple people. Version control is an essential tool for reproducible science and software systems that can improve over time.\n\n\nGit\nGit is a free and open source version control system that has become the #1 choice for software developers both in research and industry. Unlike centralized version control systems where there is a single central repository, Git allows every user to have a full copy of the entire project history on their own machine. This distributed nature enables multiple people to work on a project simultaneously without interfering with each other’s work. Git stores the history of changes in a project, enabling users to track progress, revert to previous states, and manage branches for different features or versions of a project.\n\n\nGitHub\nGitHub is a web-based platform that uses Git for version control. It provides a collaborative environment where users can host and review code, manage projects, and build software alongside millions of other developers. GitHub also offers additional features such as issue tracking, project management tools, and continuous integration (CI) workflows, which automate the process of testing and integrating new code to ensure that changes don’t break the project. As the most popular Git-based platform, GitHub has a larger user base and ecosystem, often making it the go-to choice for development collaboration.\n\n\nOther platforms using Git\nWhile GitHub is the most widely used, other platforms also use Git for version control and may be preferred in certain contexts:\nGitLab: Known for its privacy and security features, GitLab is often favored for organization-wide code repositories, especially in enterprise environments that require tighter control over their codebase. While GitHub also offers robust CI tools, GitLab’s features make it a strong option for managing both public and private repositories across large organizations.\nBitbucket: Another Git-based repository hosting service that integrates well with Atlassian products like Jira. While less popular than GitHub, Bitbucket is commonly used by teams that rely on the Atlassian ecosystem and need strong project management integration.\n\n\nGitHub Desktop\nGitHub Desktop is a graphical user interface (GUI) application that simplifies the use of Git and GitHub. It is designed for users who prefer not to use the command line interface, offering a more intuitive and visual approach to version control. With GitHub Desktop, you can easily perform common Git tasks such as committing changes, creating branches, and resolving merge conflicts, all within a user-friendly interface.\n\n\nEssential Terminology\nBecoming familiar with version control terminology is half the battle in becoming fluent in Git/GitHub. Study the terms below to become better acquainted and revisit as needed. We’ll refer to these terms often throughout this guide.\n\nRepository == repo: A project that is tracked via git/GitHub\n\nRemote repo: A git project that is stored on GitHub\nLocal repo: A git project that has been downloaded to your local machine\n\nClone: Cloning is the process of making a copy of a remote repo on your local machine. This allows you to work on the project locally and perform tasks like commits, branches, and pulls.\nCommit: A git command that marks the completion of new work to a repo (e.g., add a new script, add a feature, fill out README). You can always recover previous versions of your work by loading up a previous commit.\nPush: A git command that sends local changes (commits) stored in your local repo to the remote repo.\nPull: A git command that allows you to update your local repo based on changes made to the remote repo (e.g., if your colleague pushes to the remote repo)\nBranch: A branch in Git is a parallel line of development that allows you to work on features, bug fixes, or experiments without affecting the main codebase. You can create and switch between branches to isolate your work.\nMain branch: The default branch in a git repository where the final, stable version of the project is maintained. It typically contains production-ready code, and new features or changes from other branches are merged into the main branch after being tested and reviewed. Historically, this branch was called master, but in recent years, there has been a shift to using main as the default name. This change reflects a move towards more inclusive language, as the term “master” can carry connotations of oppression, and many platforms, like GitHub, have adopted main to encourage more thoughtful and neutral terminology in coding environments.\nMerge: Merging is the process of integrating changes from one branch into another. This is typically done to combine the changes made in a feature branch with the main branch.\nPull Request (PR): A pull request is a feature provided by platforms like GitHub, GitLab, and Bitbucket. It’s a way to propose changes (commits) to a project. Others can review the changes, and once approved, they can be merged into the main branch. If it’s easier to remember, you can think of this as a “merge request”, which is actually the terminiology GitLab uses.\nFork: Forking a repository means creating a copy of someone else’s project in your GitHub account. This allows you to make changes independently and propose those changes back to the original project via pull requests. If everyone on your team has write-access to the repo, it’s best to use new branches instead of forks for pull requests.\nGitignore: A .gitignore file is used to specify which files and directories should be excluded from version control. It’s essential for preventing unnecessary or sensitive files (contains like API keys) from being included in the repository."
  },
  {
    "objectID": "Learn/Guides/Github-desktop.html#setup",
    "href": "Learn/Guides/Github-desktop.html#setup",
    "title": "Version Control with GitHub Desktop",
    "section": "Setup",
    "text": "Setup\nIn this section, we will walk you through setting up a GitHub repository for a collaborative software project. As an example case, imagine you and your team are preparing for a Kaggle competition and need a streamlined way to manage your code, track changes, and collaborate efficiently. By following the steps below, you’ll learn how to create a new repository, add collaborators, set up secure access, and clone the repository to your local machine, ensuring everyone on your team is ready to contribute seamlessly.\n\nInstall GitHub Desktop\n\nVisit https://desktop.github.com/ to install\n\nCreate new repository or “repo”\n\nVisit https://github.com/ and sign in to your GitHub account (or create an account)\nClick the green “new” button to create a new repo\nProvide a name for the project, e.g., “my_kaggle_project”\nGive a description: “Git repo for collaborating on Kaggle project for MLM24.”\nSet to private if you’re worried about having your work scooped. Otherwise set to public.\nAdd a README file: best practice is to include a README file that explains how to use your code/repo\nChoose a license: https://choosealicense.com/. MIT license is usually best for open-source projects.\n\nAdd collaborator(s)\n\nFrom your repo homepage on GitHub, click the settings tab\nClick on the “Collaborators” menu option shown in the left panel\nClick “Add people” and enter your collaborator’s username or GitHub email address\n\nSetup SSH key: SSH provides a secure way to authenticate and transfer data between your local machine and GitHub. You can also use HTTPS if you prefer. HTTPS avoids having to generate an SSH key, but you may need to enter your GitHub login credentials from time to time.\n\nOpen GitBash (windows) or terminal (Mac) and run the following commands replacing the example email with your GitHub email:\n\nssh-keygen -t ed25519 -C “your_github_email@address.com”\ncat ~/.ssh/id_ed25519.pub\nThe ssh-keygen produces private and public keys, and make sure to copy and paste the output from the command\ncat ~/.ssh/id_ed25519.pub\n\nPaste output (starts like ssh-ed25519) into the new SSH key under GitHub settings (SSH and GPG Keys) and save the key\n\nClone repo\n\nFrom your GitHub repo homepage, click the green “Code” button\nSelect SSH if you setup an SSH key or select HTTPS if you don’t have one setup. Copy the URL shown.\nOpen GitHub Desktop\nClick File → Clone repository → URL\nPaste the repo URL and pay attention to the destination folder path so you can access this folder later\nClick “Clone”"
  },
  {
    "objectID": "Learn/Guides/Github-desktop.html#tracking-changes",
    "href": "Learn/Guides/Github-desktop.html#tracking-changes",
    "title": "Version Control with GitHub Desktop",
    "section": "Tracking changes",
    "text": "Tracking changes\nNow that you have set up your collaborative Kaggle hackathon repository, it’s time to start working on your project and track the changes you make. In this section, we will guide you through the process of adding files to your local repository, viewing and committing changes, and pushing those changes to the remote repository on GitHub. By understanding how to track changes effectively, you and your team can ensure that all contributions are recorded, reviewed, and integrated smoothly into the project.\n\nAdd a blank text file to your local repo\n\nRight-click repo name in GitHub Desktop → show in explorer (show in Finder and go to the directory on Mac)\nCreate a new text file and add to local repo folder\nAdd a line of text to the file, e.g., “hello world” and save the file\n\nView local changes\n\nIn GitHub desktop, you can view this change under the “Changes” tab. Notice that we see the new file and added text under this tab.\n\nCommit the new file\n\nCommits mark a checkpoint in the progress you have made to your repo. Provide a short summary message and optionally provide more information in the “Description” box.\n\nView remote changes (or lack thereof)\n\nVisit GitHub and notice that the change is not yet reflected on GitHub\n\nPush the change to GitHub \nView remote changes\n\nVisit GitHub again and notice the change has now been transferred to GitHub. Your collaborators can now access your changes through the remote repo (the repo stored on GitHub)"
  },
  {
    "objectID": "Learn/Guides/Github-desktop.html#ignoring-.ipynb-files",
    "href": "Learn/Guides/Github-desktop.html#ignoring-.ipynb-files",
    "title": "Version Control with GitHub Desktop",
    "section": "Ignoring .ipynb files",
    "text": "Ignoring .ipynb files\nAs you collaborate on your Kaggle hackathon project, you may encounter challenges with tracking changes in Jupyter notebooks (.ipynb files) due to their complex JSON format. These files can include a lot of metadata that makes version control difficult and cluttered. In this section, we’ll show you how to use Jupytext to convert your Jupyter notebooks into a more manageable format and configure your repository to ignore .ipynb files. This approach will help you maintain a cleaner version history and focus on the actual code changes, making collaboration more efficient.\n\nAdd jupyter lab file to repo\n\nOpen anaconda prompt and cd into your local repo folder\nrun “jupyter lab” command to start a new jupyter lab instance\ncreate a new notebook, e.g., preprocess_data.ipynb\nadd a line of code, e.g., print(‘hello world’)\nsave the notebook and open GitHub desktop\n\nIn GitHub desktop, notice the changes being tracked are wildly confusing. \n\nJupyter files are stored in JSON format which includes a lot of metadata unrelated to the changes you made to your file. The solution? Use Jupytext!\n\nInstall jupytext\n\npip install jupytext\njupytext –set-formats ipynb,py *.ipynb # convert .ipynb files to .py\njupytext –set-formats py,ipynb *.py\n\nalternatively to convert just one specific file: jupytext –set-formats ipynb,py file_name.ipynb\n\n\ngit ignore .ipynb files\n\nright click one of the .ipynb files in GitHub Desktop\nignore all files of this type\n\ncommit changes\npush and view changes on GitHub"
  },
  {
    "objectID": "Learn/Guides/Github-desktop.html#pulling-updates-from-github",
    "href": "Learn/Guides/Github-desktop.html#pulling-updates-from-github",
    "title": "Version Control with GitHub Desktop",
    "section": "Pulling updates from GitHub",
    "text": "Pulling updates from GitHub\nAs your team collaborates on the Kaggle hackathon project, it’s essential to stay up-to-date with the latest changes made by your teammates. In this section, we’ll explain how to pull updates from the remote repository on GitHub to your local machine. This process ensures that you always have the most recent version of the project and can integrate your work with the contributions of others seamlessly. By regularly pulling updates, you can avoid conflicts and ensure smooth collaboration throughout the project.\n\nPretend you are a collaborator and visit GitHub to find your repo\nAdd a new file to the remote repo (the version stored on GitHub): Add file → create new file.\nCommit the file to the repo\nOpen your local repo folder and notice we don’t have this new file yet\nIn GitHub Desktop, click “Fetch origin” by “Pull origin”\n\nFetch origin will run and inform you of any changes made to the remote copy of the repo (the one stored on GitHub)\nIf changes have been made since you last pulled, you’ll see the Fetch button turn into a “Pull” option. Click this option to retrieve any updates from GitHub and pull them into the local version of your repo.\n\nCheck your local repo folder to verify the new file has been pulled from GitHub onto your machine"
  },
  {
    "objectID": "Learn/Guides/Github-desktop.html#reverting-to-a-previous-commit",
    "href": "Learn/Guides/Github-desktop.html#reverting-to-a-previous-commit",
    "title": "Version Control with GitHub Desktop",
    "section": "Reverting to a previous commit",
    "text": "Reverting to a previous commit\nDuring the course of your Kaggle hackathon project, there may be times when you need to revert to a previous version of your code. This could be due to a bug, an unwanted change, or simply the need to return to a stable state. In this section, we’ll guide you through the process of reverting to a previous commit using GitHub Desktop. Understanding how to revert to an earlier commit ensures that you can quickly and safely undo changes, helping your team maintain a stable and functional codebase throughout the competition.\n\nFind the Commit to Revert To\n\nOpen GitHub Desktop and navigate to the repository you are working on.\nClick on the “History” tab to view the commit history of your repository.\nScroll through the list of commits and locate the commit you want to revert to. Click on the specific commit to select it.\n\nCreate a New Branch from the Selected Commit\n\nWith the desired commit selected, click on the “Branch” menu at the top of GitHub Desktop.\nSelect “New Branch” from the dropdown menu.\nIn the dialog box that appears, name your new branch (e.g., “revert-to-commit”) and ensure that it is based on the selected commit. Click “Create Branch” to proceed.\n\nMake Necessary Changes in the New Branch\n\nSwitch to the newly created branch by clicking on the “Current Branch” dropdown menu and selecting your new branch.\nMake any necessary changes in this branch to resolve issues or implement desired modifications.\nUse your code editor or IDE to make and save these changes.\n\nCommit the Changes to the New Branch\n\nReturn to GitHub Desktop and navigate to the “Changes” tab.\nReview the changes you made and provide a commit message summarizing them.\nClick the “Commit to ” button to commit these changes to the new branch.\n\nPush the Changes to GitHub\n\nClick the “Publish branch” button in GitHub Desktop to push your changes to the remote repository on GitHub.\nWait for the push process to complete.\n\nCreate a Pull Request (Optional)\n\nIf you want to merge these changes back into the main branch, go to your repository on GitHub.\nClick on the “Pull requests” tab and then click the “New pull request” button.\nSelect your new branch as the source and the main branch as the destination. Review the changes and click “Create pull request.”\n\nReview and Merge (If Using a Pull Request)\n\nReviewers can now examine the pull request on GitHub. They can leave comments, request changes, or approve the pull request.\nOnce the changes are approved, the pull request can be merged into the main branch by clicking the “Merge pull request” button on GitHub."
  },
  {
    "objectID": "Learn/Guides/Github-desktop.html#using-pull-requests-to-review-each-others-work",
    "href": "Learn/Guides/Github-desktop.html#using-pull-requests-to-review-each-others-work",
    "title": "Version Control with GitHub Desktop",
    "section": "Using “pull requests” to review each other’s’ work",
    "text": "Using “pull requests” to review each other’s’ work\nPull requests are an essential feature of GitHub that facilitate collaborative development by allowing team members to propose changes to a codebase. They provide a structured way for team members to review, discuss, and approve changes before they are merged into the main branch. In this section, we will explore how to use pull requests effectively to ensure that your team’s work is consistently high-quality and integrated smoothly. By following best practices for creating, reviewing, and merging pull requests, you can maintain a clean and stable codebase while fostering a collaborative and transparent development process. These instructions are clear and structured well, but a few refinements can enhance clarity and flow. Here’s an improved version:\n\nCreate a New Branch:\n\nOpen GitHub Desktop and select your repository.\nClick the “Current Branch” dropdown.\nSelect “New Branch” and give it a descriptive name (e.g., “feature-branch” or “collaborator-feature”).\nChoose the base branch, typically the default branch like main or master, and click “Create Branch.”\n\nMake Changes in the New Branch:\n\nSwitch to the newly created branch by selecting it from the “Current Branch” dropdown.\nCollaborators can now make changes in this new branch. They can create, edit, or delete files as needed.\n\nCommit and Push Changes:\n\nAfter making changes, go to the “Changes” tab in GitHub Desktop.\nReview the changes, provide a meaningful commit message, and click “Commit to ”.\nClick “Push origin” to push the changes to the remote repository on GitHub.\n\nPreview Pull Request:\n\nIn GitHub Desktop, click on “Branch” in the menu bar.\nSelect “Create Pull Request” to open a preview. This will show which branch is being merged into the main code base.\n\nCreate Pull Request:\n\nAfter confirming that the preview is correct, click “Create Pull Request”.\nGitHub will open in your web browser. Fill out the details for the pull request, including a title and description.\nAssign reviewers (e.g., you and other collaborators) to review the changes, then click “Create Pull Request.”\n\nReview and Submit the Pull Request on GitHub:\n\nCollaborators should review the pull request on the GitHub website.\nThey can add comments, suggestions, or request changes directly in the pull request interface.\n\nReview the Pull Request in GitHub Desktop:\n\nReturn to GitHub Desktop to see the newly created pull request listed in the “Current Branch” dropdown.\nClick on the pull request to view the changes, comments, and review the code.\nRespond to any feedback or comments in the GitHub Desktop interface.\n\nAccept or Request Changes:\n\nAfter reviewing the code, you and other collaborators can either accept the pull request if it’s ready to merge or request changes if there are issues to address.\nLeave comments, suggestions, and feedback in the pull request.\n\nCollaborators Make Changes:\n\nIf changes are requested, collaborators can make the necessary adjustments in their branch and push the updates.\nThe pull request will automatically update with the new commits.\n\nClose the Pull Request:\n\nOnce the pull request is approved and the changes have been successfully reviewed, merge the pull request into the main branch.\nAfter merging, you can delete the branch to keep the repository clean."
  },
  {
    "objectID": "Learn/Guides/Github-desktop.html#questions",
    "href": "Learn/Guides/Github-desktop.html#questions",
    "title": "Version Control with GitHub Desktop",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Learn/Guides/Github-desktop.html#see-also",
    "href": "Learn/Guides/Github-desktop.html#see-also",
    "title": "Version Control with GitHub Desktop",
    "section": "See also",
    "text": "See also\n\nWorkshop: Intro to Version Control with Git: If you’re curious to learn how to use Git via shell commands (or just want to become more fluent with Git), check out this YouTube playlist from the Data Science Hub!\nVideo: Reproducibility Overview Lecture: Without reproducibility, software systems become unreliable and difficult to improve upon over time. To learn more about best practices when conducting reproducible computational work, check out this talk! Reproducibility is essential not only in research, but for all software developers as well."
  },
  {
    "objectID": "Learn/Books/Intro-Deeplearning_SimonJDPrince.html",
    "href": "Learn/Books/Intro-Deeplearning_SimonJDPrince.html",
    "title": "Understanding Deep Learning",
    "section": "",
    "text": "Nowadays, nearly anyone can implement a deep learning model in a just a few lines of code. What separates the novices from the experts, however, is the ability to understand (or at least predict!) how these models work in different circumstances.\nSimon J.D. Prince’s free textbook, Understanding Deep Learning, provides a modern overview of deep learning (including newer topics like double descent and transformer models), and provides colab notebooks (!!!) to explore deep learning concepts and implementations. The book uses PyTorch as its framework of choice.\n\n\n\nThe title of this book is “Understanding Deep Learning” to distinguish it from volumes that cover coding and other practical aspects. This text is primarily about the ideas that underlie deep learning. The first part of the book introduces deep learning models and discusses how to train them, measure their performance, and improve this performance. The next part considers architectures that are specialized to images, text, and graph data. These chapters require only introductory linear algebra, calculus, and probability and should be accessible to any second-year undergraduate in a quantitative discipline. Subsequent parts of the book tackle generative models and reinforcement learning. These chapters require more knowledge of probability and calculus and target more advanced students. The title is also partly a joke — no-one really understands deep learning at the time of writing. Modern deep networks learn piecewise linear functions with more regions than there are atoms in the universe and can be trained with fewer data examples than model parameters. It is neither obvious that we should be able to fit these functions reliably nor that they should generalize well to new data. The penultimate chapter addresses these and other aspects that are not yet fully understood. Regardless, deep learning will change the world for better or worse. The final chapter discusses AI ethics and concludes with an appeal for practitioners to consider the moral implications of their work.\n\n\n\nLearners are expected to have the following knowledge:\n\nLinear algebra: linear algebra is the language of machine learning\nCalculus: recommended to understand gradient descent\nProbability theory: needed for reinforcement learning\nPyTorch: recommended for following along with Colab notebooks\n\n\n\n\nTBD: Use the Improve this page functionality to add your own estimate!"
  },
  {
    "objectID": "Learn/Books/Intro-Deeplearning_SimonJDPrince.html#about-this-resource",
    "href": "Learn/Books/Intro-Deeplearning_SimonJDPrince.html#about-this-resource",
    "title": "Understanding Deep Learning",
    "section": "",
    "text": "Nowadays, nearly anyone can implement a deep learning model in a just a few lines of code. What separates the novices from the experts, however, is the ability to understand (or at least predict!) how these models work in different circumstances.\nSimon J.D. Prince’s free textbook, Understanding Deep Learning, provides a modern overview of deep learning (including newer topics like double descent and transformer models), and provides colab notebooks (!!!) to explore deep learning concepts and implementations. The book uses PyTorch as its framework of choice.\n\n\n\nThe title of this book is “Understanding Deep Learning” to distinguish it from volumes that cover coding and other practical aspects. This text is primarily about the ideas that underlie deep learning. The first part of the book introduces deep learning models and discusses how to train them, measure their performance, and improve this performance. The next part considers architectures that are specialized to images, text, and graph data. These chapters require only introductory linear algebra, calculus, and probability and should be accessible to any second-year undergraduate in a quantitative discipline. Subsequent parts of the book tackle generative models and reinforcement learning. These chapters require more knowledge of probability and calculus and target more advanced students. The title is also partly a joke — no-one really understands deep learning at the time of writing. Modern deep networks learn piecewise linear functions with more regions than there are atoms in the universe and can be trained with fewer data examples than model parameters. It is neither obvious that we should be able to fit these functions reliably nor that they should generalize well to new data. The penultimate chapter addresses these and other aspects that are not yet fully understood. Regardless, deep learning will change the world for better or worse. The final chapter discusses AI ethics and concludes with an appeal for practitioners to consider the moral implications of their work.\n\n\n\nLearners are expected to have the following knowledge:\n\nLinear algebra: linear algebra is the language of machine learning\nCalculus: recommended to understand gradient descent\nProbability theory: needed for reinforcement learning\nPyTorch: recommended for following along with Colab notebooks\n\n\n\n\nTBD: Use the Improve this page functionality to add your own estimate!"
  },
  {
    "objectID": "Learn/Books/Intro-Deeplearning_SimonJDPrince.html#questions",
    "href": "Learn/Books/Intro-Deeplearning_SimonJDPrince.html#questions",
    "title": "Understanding Deep Learning",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Learn/Books/Intro-Deeplearning_SimonJDPrince.html#see-also",
    "href": "Learn/Books/Intro-Deeplearning_SimonJDPrince.html#see-also",
    "title": "Understanding Deep Learning",
    "section": "See also",
    "text": "See also\n\nWorkshop: Intro to Deep Learning with PyTorch: Learn how to use the PyTorch deep learning framework\nWorkshop: Intro to Deep Learning with Keras: Learn how to use the Keras deep learning framework"
  },
  {
    "objectID": "Learn/Workshops/Intro-Python_Gapminder.html",
    "href": "Learn/Workshops/Intro-Python_Gapminder.html",
    "title": "Intro to Python (Carpentries)",
    "section": "",
    "text": "The Plotting and Programming in Python workshop provides an introduction to programming in Python 3 for people with little or no previous programming experience. It uses plotting as its motivating example to learn Python fundamentals, including functions, conditional logic, loops, and popular packages (e.g., Pandas).\n\n\nNo previous experience with programming necessary.\n\n\n\nThis workshop takes approximately 8 hours to complete.\n\n\n\nThe Carpentries is a global organization of researchers who volunteer their time and effort to create workshops that teach software engineering and data analysis skills to other researchers. UW-Madison has its own local Carpentries community which is actively engaged in developing new ML/AI workshops. To be notified of upcoming workshops offered by the Carpentries, make sure to subscribe to the Data Science @ UW Newsletter.\n\n\n\nAll Carpentries lessons are published as open source educational materials. You are welcome and encouraged to visit the lesson materials to work through them on your own. If you are involved with a research lab at UW-Madison campus, you may attend Coding Meetup (Tue/Thur, 2:30-4:30pm) to get help working through the materials."
  },
  {
    "objectID": "Learn/Workshops/Intro-Python_Gapminder.html#about-this-resource",
    "href": "Learn/Workshops/Intro-Python_Gapminder.html#about-this-resource",
    "title": "Intro to Python (Carpentries)",
    "section": "",
    "text": "The Plotting and Programming in Python workshop provides an introduction to programming in Python 3 for people with little or no previous programming experience. It uses plotting as its motivating example to learn Python fundamentals, including functions, conditional logic, loops, and popular packages (e.g., Pandas).\n\n\nNo previous experience with programming necessary.\n\n\n\nThis workshop takes approximately 8 hours to complete.\n\n\n\nThe Carpentries is a global organization of researchers who volunteer their time and effort to create workshops that teach software engineering and data analysis skills to other researchers. UW-Madison has its own local Carpentries community which is actively engaged in developing new ML/AI workshops. To be notified of upcoming workshops offered by the Carpentries, make sure to subscribe to the Data Science @ UW Newsletter.\n\n\n\nAll Carpentries lessons are published as open source educational materials. You are welcome and encouraged to visit the lesson materials to work through them on your own. If you are involved with a research lab at UW-Madison campus, you may attend Coding Meetup (Tue/Thur, 2:30-4:30pm) to get help working through the materials."
  },
  {
    "objectID": "Learn/Workshops/Intro-Python_Gapminder.html#questions",
    "href": "Learn/Workshops/Intro-Python_Gapminder.html#questions",
    "title": "Intro to Python (Carpentries)",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Learn/Workshops/Intro-Python_Gapminder.html#see-also",
    "href": "Learn/Workshops/Intro-Python_Gapminder.html#see-also",
    "title": "Intro to Python (Carpentries)",
    "section": "See also",
    "text": "See also\n\nWorkshop: Intro to Machine Learning with Sklearn: Once you master Python fundamentals, start using the scikit-learn package to begin exploring “classical” ML methods (e.g., regression, clustering, decision trees)."
  },
  {
    "objectID": "Learn/Workshops/Intro-ML_Sklearn.html",
    "href": "Learn/Workshops/Intro-ML_Sklearn.html",
    "title": "Intro to Machine Learning with Sklearn (Carpentries)",
    "section": "",
    "text": "The Intro to Machine Learning with Sklearn workshop from the Carpentries will walk you through introductory machine learning concepts as well as how to implement common ML methods (e.g., regression, clustering, classication) using the popular scikit-learn (“sklearn”) package. Sklearn makes it possible to quickly fit and evaluate many models in just a few lines of code. It also comes with convenient functions needed for nearly all ML pipelines (e.g., train/test split, gridsearchcv). Note: Don’t use Sklearn for neural networks (it is the slowest option!). Instead, explore Keras or PyTorch.\n\n\nLearners are expected to have the following knowledge:\n\nBasic Python programming skills and familiarity with the Pandas package. If you need a refresher, these Introductory Python lesson materials are available for independent study.\n\n\n\n\nThis workshop takes approximately 8 hours to complete.\n\n\n\nThe Carpentries is a global organization of researchers who volunteer their time and effort to create workshops that teach software engineering and data analysis skills to other researchers. UW-Madison has its own local Carpentries community which is actively engaged in developing new ML/AI workshops. To be notified of upcoming workshops offered by the Carpentries, make sure to subscribe to the Data Science @ UW Newsletter.\n\n\n\nAll Carpentries lessons are published as open source educational materials. You are welcome and encouraged to visit the lesson materials to work through them on your own. If you are involved with a research lab at UW-Madison campus, you may attend Coding Meetup (Tue/Thur, 2:30-4:30pm) to get help working through the materials."
  },
  {
    "objectID": "Learn/Workshops/Intro-ML_Sklearn.html#about-this-resource",
    "href": "Learn/Workshops/Intro-ML_Sklearn.html#about-this-resource",
    "title": "Intro to Machine Learning with Sklearn (Carpentries)",
    "section": "",
    "text": "The Intro to Machine Learning with Sklearn workshop from the Carpentries will walk you through introductory machine learning concepts as well as how to implement common ML methods (e.g., regression, clustering, classication) using the popular scikit-learn (“sklearn”) package. Sklearn makes it possible to quickly fit and evaluate many models in just a few lines of code. It also comes with convenient functions needed for nearly all ML pipelines (e.g., train/test split, gridsearchcv). Note: Don’t use Sklearn for neural networks (it is the slowest option!). Instead, explore Keras or PyTorch.\n\n\nLearners are expected to have the following knowledge:\n\nBasic Python programming skills and familiarity with the Pandas package. If you need a refresher, these Introductory Python lesson materials are available for independent study.\n\n\n\n\nThis workshop takes approximately 8 hours to complete.\n\n\n\nThe Carpentries is a global organization of researchers who volunteer their time and effort to create workshops that teach software engineering and data analysis skills to other researchers. UW-Madison has its own local Carpentries community which is actively engaged in developing new ML/AI workshops. To be notified of upcoming workshops offered by the Carpentries, make sure to subscribe to the Data Science @ UW Newsletter.\n\n\n\nAll Carpentries lessons are published as open source educational materials. You are welcome and encouraged to visit the lesson materials to work through them on your own. If you are involved with a research lab at UW-Madison campus, you may attend Coding Meetup (Tue/Thur, 2:30-4:30pm) to get help working through the materials."
  },
  {
    "objectID": "Learn/Workshops/Intro-ML_Sklearn.html#questions",
    "href": "Learn/Workshops/Intro-ML_Sklearn.html#questions",
    "title": "Intro to Machine Learning with Sklearn (Carpentries)",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Learn/Workshops/Intro-ML_Sklearn.html#see-also",
    "href": "Learn/Workshops/Intro-ML_Sklearn.html#see-also",
    "title": "Intro to Machine Learning with Sklearn (Carpentries)",
    "section": "See also",
    "text": "See also\n\nWorkshop: Intro to Deep Learning with Keras: Once you master sklearn, start using Keras to build neural networks quickly.\nWorkshop: Intro to Deep Learning with PyTorch: Explore PyTorch as an alternative deep learning framework (faster but more verbose than Keras)\nBook: Understanding Deep Learning - Simon J.D. Prince: This free textbook is a good modern overview of deep learning, and provides colab notebooks to explore deep learning concepts and implementations. The book uses PyTorch as its framework of choice. You may find additional details in this book that the workshop only briefly touches on."
  },
  {
    "objectID": "Learn/Workshops/Intro-Deeplearning_PyTorch.html",
    "href": "Learn/Workshops/Intro-Deeplearning_PyTorch.html",
    "title": "Intro to Deep Learning with PyTorch (Udacity)",
    "section": "",
    "text": "The Intro to Deep Learning with PyTorch workshop from Udacity will walk you through introductory deep learning concepts as well as how to build a neural networks in PyTorch. PyTorch is one of the most popular deep learning frameworks. Known for its speed and more “Pythonic” feel, it is frequently the go-to choice for most researchers. The biggest downside of PyTorch, compared to a high-level framework like Keras, is that it is quite verbose. That is, you’ll need to write a couple hundred lines of code to train and evaluate your neural network. Keras is a great alternative for those who are just getting started with neural networks or those that don’t need to train many models, as you can train/evaluate in just a dozen or so lines of code.\n\n\nLearners are expected to have the following knowledge:\n\nBasic Python programming skills and familiarity with the Pandas package. If you need a refresher, these Introductory Python lesson materials are available for independent study.\n\n\n\n\nTBD: Use the Improve this page functionality to add your own estimate!"
  },
  {
    "objectID": "Learn/Workshops/Intro-Deeplearning_PyTorch.html#about-this-resource",
    "href": "Learn/Workshops/Intro-Deeplearning_PyTorch.html#about-this-resource",
    "title": "Intro to Deep Learning with PyTorch (Udacity)",
    "section": "",
    "text": "The Intro to Deep Learning with PyTorch workshop from Udacity will walk you through introductory deep learning concepts as well as how to build a neural networks in PyTorch. PyTorch is one of the most popular deep learning frameworks. Known for its speed and more “Pythonic” feel, it is frequently the go-to choice for most researchers. The biggest downside of PyTorch, compared to a high-level framework like Keras, is that it is quite verbose. That is, you’ll need to write a couple hundred lines of code to train and evaluate your neural network. Keras is a great alternative for those who are just getting started with neural networks or those that don’t need to train many models, as you can train/evaluate in just a dozen or so lines of code.\n\n\nLearners are expected to have the following knowledge:\n\nBasic Python programming skills and familiarity with the Pandas package. If you need a refresher, these Introductory Python lesson materials are available for independent study.\n\n\n\n\nTBD: Use the Improve this page functionality to add your own estimate!"
  },
  {
    "objectID": "Learn/Workshops/Intro-Deeplearning_PyTorch.html#questions",
    "href": "Learn/Workshops/Intro-Deeplearning_PyTorch.html#questions",
    "title": "Intro to Deep Learning with PyTorch (Udacity)",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Learn/Workshops/Intro-Deeplearning_PyTorch.html#see-also",
    "href": "Learn/Workshops/Intro-Deeplearning_PyTorch.html#see-also",
    "title": "Intro to Deep Learning with PyTorch (Udacity)",
    "section": "See also",
    "text": "See also\n\nWorkshop: Intro to Deep Learning with Keras: Explore Keras as an alternative deep learning framework\nBook: Understanding Deep Learning - Simon J.D. Prince: This free textbook is a good modern overview of deep learning, and provides colab notebooks to explore deep learning concepts and implementations. The book uses PyTorch as its framework of choice. You may find additional details in this book that the workshop only briefly touches on."
  },
  {
    "objectID": "Learn/Workshops/Intro-Amazon_SageMaker.html",
    "href": "Learn/Workshops/Intro-Amazon_SageMaker.html",
    "title": "Intro to AWS SageMaker for Predictive ML/AI",
    "section": "",
    "text": "This introductory AWS SageMaker workshop teaches core workflows for running predictive ML/AI models in AWS SageMaker, an AWS-managed machine learning environment. Participants will learn to set up data, configure SageMaker Notebooks, manage code repositories, train and tune models, and optimize resource costs effectively within AWS. While currently tailored for 2024 Machine Learning Marathon participants, a more general version of the materials will be released in the coming months. The lesson is in the pre-alpha phase, meaning it is under development and has not yet been taught. Users will benefit from tips on controlling AWS expenses and scaling models efficiently, with real-world guidance on choosing appropriate CPU and GPU resources.\n\n\nRunning through this workshop should cost approximately $10-$15 on AWS, assuming moderate usage of GPU instances and a few parallel jobs. For new AWS accounts, the AWS Free Tier may cover some of these costs, including 250 hours per month of the ml.t2.medium instance for the first two months, as well as some limited S3 storage. This means new users may be able to complete certain parts of the workshop for free or at a significantly reduced cost. We recommend monitoring usage through the AWS Billing Dashboard to stay within the free tier and manage any extra expenses effectively.\n\n\n\n\nIntro to Machine Learning\nBasic Python Programming\n\n\n\n\n3-5 hours: Based on running through training, tuning, and experimenting with example code setups."
  },
  {
    "objectID": "Learn/Workshops/Intro-Amazon_SageMaker.html#about-this-resource",
    "href": "Learn/Workshops/Intro-Amazon_SageMaker.html#about-this-resource",
    "title": "Intro to AWS SageMaker for Predictive ML/AI",
    "section": "",
    "text": "This introductory AWS SageMaker workshop teaches core workflows for running predictive ML/AI models in AWS SageMaker, an AWS-managed machine learning environment. Participants will learn to set up data, configure SageMaker Notebooks, manage code repositories, train and tune models, and optimize resource costs effectively within AWS. While currently tailored for 2024 Machine Learning Marathon participants, a more general version of the materials will be released in the coming months. The lesson is in the pre-alpha phase, meaning it is under development and has not yet been taught. Users will benefit from tips on controlling AWS expenses and scaling models efficiently, with real-world guidance on choosing appropriate CPU and GPU resources.\n\n\nRunning through this workshop should cost approximately $10-$15 on AWS, assuming moderate usage of GPU instances and a few parallel jobs. For new AWS accounts, the AWS Free Tier may cover some of these costs, including 250 hours per month of the ml.t2.medium instance for the first two months, as well as some limited S3 storage. This means new users may be able to complete certain parts of the workshop for free or at a significantly reduced cost. We recommend monitoring usage through the AWS Billing Dashboard to stay within the free tier and manage any extra expenses effectively.\n\n\n\n\nIntro to Machine Learning\nBasic Python Programming\n\n\n\n\n3-5 hours: Based on running through training, tuning, and experimenting with example code setups."
  },
  {
    "objectID": "Learn/Workshops/Intro-Amazon_SageMaker.html#questions",
    "href": "Learn/Workshops/Intro-Amazon_SageMaker.html#questions",
    "title": "Intro to AWS SageMaker for Predictive ML/AI",
    "section": "Questions?",
    "text": "Questions?\nFor any questions, please post to the Nexus Q&A on GitHub. Feedback is especially helpful in these early stages to improve workshop materials!"
  },
  {
    "objectID": "Learn/Workshops/Intro-Amazon_SageMaker.html#see-also",
    "href": "Learn/Workshops/Intro-Amazon_SageMaker.html#see-also",
    "title": "Intro to AWS SageMaker for Predictive ML/AI",
    "section": "See also",
    "text": "See also\n\nAWS Free Tier Guide: An overview of the AWS Free Tier, including limitations and expected costs for beginner users.\nCenter for High Throughput Computing (CHTC): CHTC at UW-Madison offers free compute resources to researchers."
  },
  {
    "objectID": "Toolbox/Models/index.html",
    "href": "Toolbox/Models/index.html",
    "title": "Models",
    "section": "",
    "text": "Explore popular model hubs, pretrained models, and foundation models.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nXGBoost: Tree-Based Gradient Boosting for Tabular Data\n\n\n\nModels\n\n\nGradient boosting\n\n\nDecision trees\n\n\nTabular data\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-10-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nU-Net: Convolutional Networks for Biomedical Image Segmentation\n\n\n\nModels\n\n\nDeep learning\n\n\nMedical imaging\n\n\nImage segmentation\n\n\nCNN\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-09-16\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Toolbox",
      "Models"
    ]
  },
  {
    "objectID": "Toolbox/Models/XGBoost.html",
    "href": "Toolbox/Models/XGBoost.html",
    "title": "XGBoost: Tree-Based Gradient Boosting for Tabular Data",
    "section": "",
    "text": "XGBoost, or eXtreme Gradient Boosting, is a machine learning algorithm built upon the foundation of decision trees, extending their power through boosting. Originally introduced by Tianqi Chen in 2016, XGBoost has revolutionized predictive modeling, especially for tabular data, thanks to its efficiency, scalability, and performance. It is particularly well-suited for tasks like regression, classification, and ranking, where interpretability, speed, and precision are key."
  },
  {
    "objectID": "Toolbox/Models/XGBoost.html#about-this-resource",
    "href": "Toolbox/Models/XGBoost.html#about-this-resource",
    "title": "XGBoost: Tree-Based Gradient Boosting for Tabular Data",
    "section": "",
    "text": "XGBoost, or eXtreme Gradient Boosting, is a machine learning algorithm built upon the foundation of decision trees, extending their power through boosting. Originally introduced by Tianqi Chen in 2016, XGBoost has revolutionized predictive modeling, especially for tabular data, thanks to its efficiency, scalability, and performance. It is particularly well-suited for tasks like regression, classification, and ranking, where interpretability, speed, and precision are key."
  },
  {
    "objectID": "Toolbox/Models/XGBoost.html#tree-based-learning-and-enhancements-in-xgboost",
    "href": "Toolbox/Models/XGBoost.html#tree-based-learning-and-enhancements-in-xgboost",
    "title": "XGBoost: Tree-Based Gradient Boosting for Tabular Data",
    "section": "Tree-based learning and enhancements in XGBoost",
    "text": "Tree-based learning and enhancements in XGBoost\nAt its core, XGBoost uses decision trees to model complex patterns in data, applying gradient boosting to improve model accuracy:\n\nAdditive learning: XGBoost builds trees sequentially, where each new tree focuses on the residuals (errors) of the previous trees.\nGradient boosting: By minimizing a loss function (using gradient descent), XGBoost creates an ensemble of decision trees that progressively improve predictions.\n\n\nEnhancements\n\nRegularization: XGBoost applies L1 (Lasso) and L2 (Ridge) regularization to control model complexity and reduce overfitting.\nPruning: Decision trees in XGBoost are pruned during training to avoid overfitting and maintain generalization ability.\nMissing data handling: XGBoost can automatically handle missing data, making it particularly robust for real-world datasets."
  },
  {
    "objectID": "Toolbox/Models/XGBoost.html#comparisons-to-deep-learning",
    "href": "Toolbox/Models/XGBoost.html#comparisons-to-deep-learning",
    "title": "XGBoost: Tree-Based Gradient Boosting for Tabular Data",
    "section": "Comparisons to deep learning",
    "text": "Comparisons to deep learning\nWhile deep learning excels at tasks like image recognition and natural language processing, XGBoost often outperforms deep learning models on tabular data, especially in the following scenarios:\n\nSmall to medium-sized datasets: Deep learning models require large amounts of data to perform well, whereas XGBoost can achieve high accuracy even with smaller datasets.\nMedical contexts: In medical datasets, where structured/tabular data is prevalent, XGBoost often outperforms deep learning due to its efficiency and the importance of feature engineering.\nFaster training Time: XGBoost is much faster to train compared to deep learning models, making it ideal for applications with limited computational resources or time constraints.\n\n\nInterpretability\nXGBoost offers greater interpretability than deep learning models, but it is less interpretable than simpler models like decision trees or linear regressions:\n\nFeature Importance: XGBoost provides feature importance scores, showing which features contribute the most to model accuracy.\nEnsemble Complexity: While individual trees in the XGBoost ensemble are interpretable, the combined model (with potentially hundreds of trees) can be difficult to fully explain.\nSHAP Values and LIME: These tools allow you to understand how each feature influences predictions both locally (for individual instances) and globally (across the whole model)."
  },
  {
    "objectID": "Toolbox/Models/XGBoost.html#timeline-context",
    "href": "Toolbox/Models/XGBoost.html#timeline-context",
    "title": "XGBoost: Tree-Based Gradient Boosting for Tabular Data",
    "section": "Timeline context",
    "text": "Timeline context\nXGBoost builds on a rich history of decision tree-based models. Here’s how it fits into the broader development of machine learning models that rely on trees:\n\nClassification and Regression Trees - CART (1984): The foundation of decision tree algorithms, focusing on binary splits of data based on feature values.\nRandom Forest (2001): Combines many independent decision trees for better accuracy and robustness by reducing variance through bootstrapping.\nXGBoost (2016): An optimized implementation of gradient boosting, which leverages tree-based models and introduces regularization, parallelism, and handling of missing values.\nCatBoost (2018): Introduced categorical feature handling directly in gradient boosting."
  },
  {
    "objectID": "Toolbox/Models/XGBoost.html#xgboost-variants",
    "href": "Toolbox/Models/XGBoost.html#xgboost-variants",
    "title": "XGBoost: Tree-Based Gradient Boosting for Tabular Data",
    "section": "XGBoost variants",
    "text": "XGBoost variants\n\nXGBoost with DART: Modifies the traditional tree boosting approach by dropping trees randomly during training, preventing overfitting and improving generalization.\nXGBoost with Linear Booster: Instead of building trees, this variant uses a linear model as the base learner, blending gradient boosting with linear regression or classification."
  },
  {
    "objectID": "Toolbox/Models/XGBoost.html#model-playground",
    "href": "Toolbox/Models/XGBoost.html#model-playground",
    "title": "XGBoost: Tree-Based Gradient Boosting for Tabular Data",
    "section": "Model playground",
    "text": "Model playground\n\nTutorials and Getting Started Notebooks\n\nXGBoost: Check out the XGBoost Python Documentation for installation, basic usage, and advanced tuning tips.\n\n\n\nHigh-level tips for effective sse\n\nHyperparameter tuning: Parameters like max depth and learning rate control the complexity and speed of the trees; tuning these is crucial for optimal performance.\nHandling missing data: XGBoost’s ability to handle missing values natively means there’s no need for complex imputation strategies.\nEarly stopping: Use early stopping with cross-validation to prevent overfitting, especially when training deep trees.\nImbalanced datasets: Adjust the scale_pos_weight parameter to manage class imbalance in datasets, like those seen in fraud detection or rare event classification.\n\n\n\nRelated datasets & Kaggle challenges\n\nAmes House Price Prediction: A regression task where XGBoost often outperforms deep learning due to the tabular nature of the data.\nDisaster Tweets Classification: A classification task that involves text, but XGBoost can excel with good feature engineering."
  },
  {
    "objectID": "Toolbox/Models/XGBoost.html#questions",
    "href": "Toolbox/Models/XGBoost.html#questions",
    "title": "XGBoost: Tree-Based Gradient Boosting for Tabular Data",
    "section": "Questions?",
    "text": "Questions?\nIf you have any lingering questions about this resource, feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Toolbox/Data/Gutenberg.html",
    "href": "Toolbox/Data/Gutenberg.html",
    "title": "Project Gutenberg: Text & Audio Books",
    "section": "",
    "text": "The Project Gutenberg dataset contains text from thousands of books, spanning a variety of genres and styles, and in some cases, corresponding audiobooks. Researchers and students working on machine learning applications can use this dataset to explore tasks such as language modeling, text classification, summarization, and speech synthesis. The dataset’s availability in both text and audio formats makes it suitable for multimodal learning tasks as well.\n\n\n\nText & audio: Many books in the Gutenberg collection have corresponding audiobooks (through Librivox), enabling both text and audio-based learning tasks.\nMultilingual content: While primarily in English, the dataset includes books in other languages such as French, German, and Spanish, providing opportunities for multilingual and cross-lingual research in NLP.\nLong-form text: The dataset includes full-length novels, short stories, and essays, making it ideal for tasks that require understanding context over longer sequences of text.\n\n\n\n\n\nLanguage modeling: With its vast variety of literary styles and genres, Gutenberg serves as a valuable resource for training and evaluating language models like GPT and BERT. Pre-training on Gutenberg’s diverse text corpus allows models to capture nuanced linguistic patterns, which can later be fine-tuned for more specific NLP tasks.\nText classification: The dataset can be applied to classification tasks such as genre classification or sentiment analysis. Researchers often use Gutenberg to train classifiers that distinguish between literary styles or detect emotional tone in texts.\nSummarization and translation: Due to the diversity in content, Gutenberg is commonly used to test summarization models (e.g., creating concise book summaries) and translation algorithms across different literary forms.\nTopic modeling: The diverse collection of texts allows for the exploration of underlying themes or topics through techniques like Latent Dirichlet Allocation (LDA) or Non-negative Matrix Factorization (NMF), enabling researchers to uncover hidden patterns in the literature.\nMultimodal learning: Paired with Librivox audiobooks, the Gutenberg dataset enables multimodal tasks like text-to-speech synthesis, speech recognition, and aligning spoken text with its written counterpart. This supports the development of models like Tacotron and Wav2Vec.\nTransfer learning: Researchers frequently fine-tune pre-trained language models on Gutenberg to test performance on literary and long-form text, often comparing results with models trained on broader corpora like Common Crawl.\nData augmentation: Gutenberg’s large-scale, structured text is ideal for augmenting smaller datasets and improving model robustness through data imputation or other generalization techniques.\n\n\n\n\n\nLibrivox Audiobook Collection: Provides audiobooks to accompany the texts available in Project Gutenberg.\nCommon Crawl: Large-scale web crawl dataset often used to pre-train language models. Gutenberg can provide a more structured and curated supplement to such datasets."
  },
  {
    "objectID": "Toolbox/Data/Gutenberg.html#about-this-resource",
    "href": "Toolbox/Data/Gutenberg.html#about-this-resource",
    "title": "Project Gutenberg: Text & Audio Books",
    "section": "",
    "text": "The Project Gutenberg dataset contains text from thousands of books, spanning a variety of genres and styles, and in some cases, corresponding audiobooks. Researchers and students working on machine learning applications can use this dataset to explore tasks such as language modeling, text classification, summarization, and speech synthesis. The dataset’s availability in both text and audio formats makes it suitable for multimodal learning tasks as well.\n\n\n\nText & audio: Many books in the Gutenberg collection have corresponding audiobooks (through Librivox), enabling both text and audio-based learning tasks.\nMultilingual content: While primarily in English, the dataset includes books in other languages such as French, German, and Spanish, providing opportunities for multilingual and cross-lingual research in NLP.\nLong-form text: The dataset includes full-length novels, short stories, and essays, making it ideal for tasks that require understanding context over longer sequences of text.\n\n\n\n\n\nLanguage modeling: With its vast variety of literary styles and genres, Gutenberg serves as a valuable resource for training and evaluating language models like GPT and BERT. Pre-training on Gutenberg’s diverse text corpus allows models to capture nuanced linguistic patterns, which can later be fine-tuned for more specific NLP tasks.\nText classification: The dataset can be applied to classification tasks such as genre classification or sentiment analysis. Researchers often use Gutenberg to train classifiers that distinguish between literary styles or detect emotional tone in texts.\nSummarization and translation: Due to the diversity in content, Gutenberg is commonly used to test summarization models (e.g., creating concise book summaries) and translation algorithms across different literary forms.\nTopic modeling: The diverse collection of texts allows for the exploration of underlying themes or topics through techniques like Latent Dirichlet Allocation (LDA) or Non-negative Matrix Factorization (NMF), enabling researchers to uncover hidden patterns in the literature.\nMultimodal learning: Paired with Librivox audiobooks, the Gutenberg dataset enables multimodal tasks like text-to-speech synthesis, speech recognition, and aligning spoken text with its written counterpart. This supports the development of models like Tacotron and Wav2Vec.\nTransfer learning: Researchers frequently fine-tune pre-trained language models on Gutenberg to test performance on literary and long-form text, often comparing results with models trained on broader corpora like Common Crawl.\nData augmentation: Gutenberg’s large-scale, structured text is ideal for augmenting smaller datasets and improving model robustness through data imputation or other generalization techniques.\n\n\n\n\n\nLibrivox Audiobook Collection: Provides audiobooks to accompany the texts available in Project Gutenberg.\nCommon Crawl: Large-scale web crawl dataset often used to pre-train language models. Gutenberg can provide a more structured and curated supplement to such datasets."
  },
  {
    "objectID": "Toolbox/Data/Gutenberg.html#loading-data-in-python",
    "href": "Toolbox/Data/Gutenberg.html#loading-data-in-python",
    "title": "Project Gutenberg: Text & Audio Books",
    "section": "Loading data in Python",
    "text": "Loading data in Python\nYou can easily load text data from Project Gutenberg in Python using the gutenbergpy or requests libraries. Here’s a basic example using gutenbergpy:\n\nInstall the gutenbergpy library:\n\n!pip install gutenbergpy\n\nLoad a book from Project Gutenberg You’ll need the Gutenberg Book ID, which you can find by searching the Gutenberg website for the book you want. The Book ID is the number found at the end of the book’s URL. For example, the URL https://www.gutenberg.org/ebooks/1342 corresponds to Pride and Prejudice, and the Book ID is 1342.\n\nfrom gutenbergpy.textget import get_text_by_id\nfrom gutenbergpy.textget import strip_headers\n\n# Replace '1342' with the ID of the book you want to download\nbook_id = 1342\nbook_text = get_text_by_id(book_id)\nbook_text_clean = strip_headers(book_text).strip()\n\n# Print the first 500 characters\nprint(book_text_clean[:500])"
  },
  {
    "objectID": "Toolbox/Data/Gutenberg.html#questions",
    "href": "Toolbox/Data/Gutenberg.html#questions",
    "title": "Project Gutenberg: Text & Audio Books",
    "section": "Questions?",
    "text": "Questions?\nIf you have any lingering questions about this resource, feel free to post them on the ML+X Nexus Q&A on GitHub. We will update this resource as new information or applications arise."
  },
  {
    "objectID": "Toolbox/Data/Gutenberg.html#see-also",
    "href": "Toolbox/Data/Gutenberg.html#see-also",
    "title": "Project Gutenberg: Text & Audio Books",
    "section": "See also",
    "text": "See also\n\nWorkshop: Intro to Text Analysis / NLP: A hands-on introduction to natural language processing and how to extract insights from text data."
  },
  {
    "objectID": "Toolbox/Compute/CHTC.html",
    "href": "Toolbox/Compute/CHTC.html",
    "title": "Center for High Throughput Computing (CHTC)",
    "section": "",
    "text": "Established in 2006, the Center for High Throughput Computing (CHTC) is committed to democratizing access to powerful computing resources across all research domains. High Throughput Computing (HTC) encompasses a set of principles and techniques designed to optimize computing resource utilization towards solving complex problems. When applied to scientific computing, HTC enhances resource efficiency, automation, and accelerates scientific breakthroughs, including those in machine learning.\nAre you a researcher at UW-Madison seeking to extend your computing capabilities beyond local resources, particularly for machine learning tasks? Request an account now to take advantage of the open computing services offered by the CHTC!"
  },
  {
    "objectID": "Toolbox/Compute/CHTC.html#about-this-resource",
    "href": "Toolbox/Compute/CHTC.html#about-this-resource",
    "title": "Center for High Throughput Computing (CHTC)",
    "section": "",
    "text": "Established in 2006, the Center for High Throughput Computing (CHTC) is committed to democratizing access to powerful computing resources across all research domains. High Throughput Computing (HTC) encompasses a set of principles and techniques designed to optimize computing resource utilization towards solving complex problems. When applied to scientific computing, HTC enhances resource efficiency, automation, and accelerates scientific breakthroughs, including those in machine learning.\nAre you a researcher at UW-Madison seeking to extend your computing capabilities beyond local resources, particularly for machine learning tasks? Request an account now to take advantage of the open computing services offered by the CHTC!"
  },
  {
    "objectID": "Toolbox/Compute/CHTC.html#chtc-recipes",
    "href": "Toolbox/Compute/CHTC.html#chtc-recipes",
    "title": "Center for High Throughput Computing (CHTC)",
    "section": "CHTC “Recipes”",
    "text": "CHTC “Recipes”\nVisit the CHTC Recipes Repository to discover a collection of common CHTC workflows or “recipes”, including those specifically geared towards machine learning tasks.\n\nGPU-based\nExplore our collection of templates tailored for high throughput compute (HTC) systems utilizing GPUs, ideal for accelerating machine learning workflows. These templates streamline the process of job submission, maximizing the utilization of GPU resources for your computational tasks in machine learning. Dive into efficient computing with our GPU-based templates available on GitHub: CHTC GPU Templates\n\n\nContainer Guides\nEmpower your machine learning research endeavors with containerization! CHTC’s guides on Docker and Apptainer for HTC empower researchers to encapsulate their machine learning workflows, dependencies, and environments efficiently. Seamlessly integrate containers into your machine learning computing workflow for enhanced reproducibility and scalability.\n\nDocker Jobs Guide\nApptainer HTC Guide\nPython container\nR container\nPyTorch container\nAlphafold container"
  },
  {
    "objectID": "Toolbox/index.html",
    "href": "Toolbox/index.html",
    "title": "Toolbox",
    "section": "",
    "text": "The Toolbox section aggregates resources related to popular pretrained & foundation models, useful scripts/libraries, and datasets that you can leverage for your next ML project. Learn about their features, how to use them effectively, and see examples of them in action.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKornia\n\n\n\nLibraries\n\n\nModel exploration\n\n\nComputer vision\n\n\nDeep learning\n\n\nPyTorch\n\n\nImage processing\n\n\nViT\n\n\nSAM\n\n\nLoFTR\n\n\nRT-DETR\n\n\n\n\n\n\n\nRadi Akbar\n\n\n2024-10-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeaf Vein Dataset (LVD2021)\n\n\n\nData\n\n\nImage\n\n\nPlant phenotyping\n\n\nImage segmentation\n\n\nComputer vision\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-10-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nXGBoost: Tree-Based Gradient Boosting for Tabular Data\n\n\n\nModels\n\n\nGradient boosting\n\n\nDecision trees\n\n\nTabular data\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-10-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject Gutenberg: Text & Audio Books\n\n\n\nData\n\n\nText\n\n\nAudio\n\n\nMultimodal\n\n\nNLP\n\n\nText analysis\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-10-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nU-Net: Convolutional Networks for Biomedical Image Segmentation\n\n\n\nModels\n\n\nDeep learning\n\n\nMedical imaging\n\n\nImage segmentation\n\n\nCNN\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-09-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMONAI: Medical Open Network for AI\n\n\n\nLibraries\n\n\nDeep learning\n\n\nPyTorch\n\n\nMedical imaging\n\n\nModel exploration\n\n\n\n\n\n\n\nAlan McMillan\n\n\n2024-08-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCenter for High Throughput Computing (CHTC)\n\n\n\nCompute\n\n\nGPU\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-06-25\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Toolbox"
    ]
  },
  {
    "objectID": "Toolbox/Libraries/index.html",
    "href": "Toolbox/Libraries/index.html",
    "title": "Libraries",
    "section": "",
    "text": "Explore popular libraries and frameworks that you can leverage for your next AI/ML project.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKornia\n\n\n\nLibraries\n\n\nModel exploration\n\n\nComputer vision\n\n\nDeep learning\n\n\nPyTorch\n\n\nImage processing\n\n\nViT\n\n\nSAM\n\n\nLoFTR\n\n\nRT-DETR\n\n\n\n\n\n\n\nRadi Akbar\n\n\n2024-10-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMONAI: Medical Open Network for AI\n\n\n\nLibraries\n\n\nDeep learning\n\n\nPyTorch\n\n\nMedical imaging\n\n\nModel exploration\n\n\n\n\n\n\n\nAlan McMillan\n\n\n2024-08-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Machine Learning with Sklearn (Carpentries)\n\n\n\nWorkshops\n\n\nLibraries\n\n\nClassical ML\n\n\nSklearn\n\n\nCarpentries\n\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Deep Learning with Keras (Carpentries)\n\n\n\nWorkshops\n\n\nLibraries\n\n\nDeep learning\n\n\nKeras\n\n\nCarpentries\n\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Deep Learning with PyTorch (Udacity)\n\n\n\nWorkshops\n\n\nLibraries\n\n\nDeep learning\n\n\nPyTorch\n\n\nUdacity\n\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-15\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Toolbox",
      "Libraries"
    ]
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "Glossary of Categories",
    "section": "",
    "text": "Glossary of Categories\n\n\n\n\n\n\n\n\nTerm\nMeaning\nDescription\n\n\n\n\nAgriculture\nN/A\nThe application of machine learning in agricultural practices, including crop prediction, pest detection, and yield optimization. Often involves satellite imaging and IoT data.\n\n\nBiophysics\nN/A\nThe study of biological processes through the methods of physics. Machine learning is increasingly applied in areas like protein structure prediction and molecular dynamics simulations.\n\n\nBooks\nN/A\nComprehensive resources for learning and reference. Often written by experts, these provide in-depth coverage of machine learning topics.\n\n\nCarpentries\nN/A\nHands-on, interactive learning sessions focusing on foundational coding and data skills. Facilitates skill acquisition through real-world examples and active learning.\n\n\nClassical ML\nClassical Machine Learning\nRefers to traditional ML algorithms like SVMs, decision trees, and k-means clustering. These methods are well-established, interpretable, and often less resource-intensive than deep learning.\n\n\nClustering\nN/A\nThe process of grouping a set of objects in such a way that objects in the same group (cluster) are more similar to each other than to those in other groups. Common algorithms include k-means and DBSCAN.\n\n\nCNN\nConvolutional Neural Network\nA class of deep learning models particularly well-suited for image processing tasks. Known for their ability to automatically learn spatial hierarchies of features. Common libraries include TensorFlow and Keras.\n\n\nCNN-LSTM\nConvolutional Neural Network - Long Short-Term Memory\nA hybrid model combining CNNs for feature extraction from images or sequences and LSTMs for handling time dependencies. Used in applications like video classification.\n\n\nCode-along\nN/A\nLive coding sessions where learners write code simultaneously with the instructor, which enhances learning through practice and immediate application of concepts.\n\n\nCompute\nN/A\nRefers to computational resources, often in the context of machine learning, such as CPUs, GPUs, and cloud computing. Critical for training models, especially deep learning models.\n\n\nComputer vision\nN/A\nA field of machine learning focused on enabling computers to interpret and make decisions based on visual data. Applications include image recognition, object detection, and facial recognition. Libraries include OpenCV, PyTorch, and TensorFlow.\n\n\nContribute\nN/A\nInvolves contributing to open-source projects, fostering collaboration, improving software quality, and providing community support.\n\n\nCSI\nCover Song Identification\nA task in music information retrieval that involves identifying whether one song is a cover version of another. Machine learning models for CSI often analyze harmonic, melodic, and rhythmic similarities across versions. Popular approaches include feature-based methods and deep learning models.\n\n\nCross Labs AI\nN/A\nRefers to interdisciplinary AI research and development across various labs or research groups, often involving collaboration between different fields such as computer science, biology, and engineering.\n\n\nDeep learning\nN/A\nA subset of machine learning involving neural networks with many layers, used for complex tasks like image recognition and natural language processing. Commonly used libraries include TensorFlow, PyTorch, and Keras.\n\n\nDrug synergy\nN/A\nThe study and identification of drug combinations that produce a greater effect together than individually. Machine learning is used to predict synergistic drug pairs.\n\n\nEmpirical patterns\nN/A\nObserved patterns in data identified through experimentation and analysis. These patterns help inform the development of models and algorithms.\n\n\nForums\nN/A\nOnline platforms where members of the machine learning community can ask questions, share knowledge, and collaborate on projects. Examples include Reddit, Stack Overflow, and specialized forums like Kaggle.\n\n\nFoundation models\nN/A\nLarge-scale pre-trained models that serve as a base for fine-tuning on specific tasks. They underpin many state-of-the-art NLP and computer vision systems. Examples include GPT-3, BERT, and CLIP.\n\n\nGenomics\nN/A\nThe study of genomes, often involving the analysis of DNA sequences. Machine learning aids in tasks like gene prediction, mutation analysis, and personalized medicine. Libraries include Scikit-learn, TensorFlow, and PyTorch.\n\n\nGit/GitHub\nN/A\nVersion control system (Git) and the associated platform (GitHub) for hosting and sharing code. Essential tools for collaboration and project management in software development, including ML projects.\n\n\nGrokking\nN/A\nA phenomenon in machine learning where a model unexpectedly generalizes well after many training iterations, often after initially performing poorly. Highlights the non-linear relationship between training time and model performance.\n\n\nGuides\nN/A\nDetailed instructions or explanations, often in the form of tutorials or documentation, aimed at helping users understand and apply specific concepts or tools.\n\n\nGPU\nGraphics Processing Unit\nA specialized processor that accelerates the creation of images in a frame buffer intended for output to a display. Widely used in deep learning for parallel processing capabilities.\n\n\nHealthcare\nN/A\nThe application of machine learning in the healthcare industry, including areas like medical imaging, diagnostics, and personalized treatment plans. Common challenges include data privacy and interpretability.\n\n\nHugging Face\nN/A\nA popular platform for sharing pre-trained models, datasets, and other machine learning resources, especially in NLP. Provides tools like the Transformers library for easy model deployment.\n\n\nIndustry applications\nN/A\nRefers to the use of machine learning across various industries such as finance, manufacturing, and logistics, for tasks like predictive maintenance, fraud detection, and supply chain optimization.\n\n\nKeras\nN/A\nA high-level neural networks API written in Python, capable of running on top of TensorFlow, Theano, or CNTK. It allows for easy and fast prototyping of deep learning models.\n\n\nKnowledge-based\nN/A\nRefers to models or systems that incorporate domain-specific knowledge, often encoded in rules or ontologies, to improve decision-making or interpretability. Examples include expert systems and knowledge graphs.\n\n\nLLaVA\nLarge Language and Vision Assistant\nA multimodal AI model that combines language and vision understanding, capable of processing and generating both text and images.\n\n\nLLM\nLarge Language Model\nA type of deep learning model that can process and generate human-like text by understanding context from vast amounts of data. Examples include GPT-3 and BERT.\n\n\nLMM\nLarge Multimodal Model\nA class of models that can process and generate content across different modalities such as text, image, and audio. Examples include CLIP and DALL-E.\n\n\nMedical imaging\nN/A\nThe application of machine learning techniques to analyze and interpret medical images. Common tasks include segmentation, classification, and anomaly detection. Libraries include MONAI, PyTorch, and TensorFlow.\n\n\nModel exploration\nN/A\nLibraries that facilitate trying out different model architectures and pretrained models.\n\n\nNLP\nNatural Language Processing\nA field of machine learning focused on the interaction between computers and humans using natural language. Common libraries include NLTK, SpaCy, and Hugging Face Transformers.\n\n\nOOD detection\nOut-of-Distribution Detection\nTechniques for identifying data points that do not belong to the distribution on which a model was trained. Important for building robust and trustworthy models. Common methods include Mahalanobis distance and energy-based models.\n\n\nPyTorch\nN/A\nAn open-source machine learning library based on the Torch library, primarily used for applications such as computer vision and natural language processing. Developed by Facebook’s AI Research lab.\n\n\nPython\nN/A\nA high-level programming language that has become the de facto standard for machine learning and data science due to its readability and vast ecosystem of libraries (e.g., NumPy, Pandas, Scikit-learn, TensorFlow).\n\n\nReproducibility\nN/A\nEnsuring that ML experiments and results can be consistently replicated by others, a key principle in scientific research. Often involves detailed documentation, version control, and the use of containers.\n\n\nSklearn\nScikit-learn\nA machine learning library for Python that provides simple and efficient tools for data mining and data analysis, built on NumPy, SciPy, and Matplotlib.\n\n\nText analysis\nN/A\nThe process of deriving information from text data, often involving techniques from NLP. Used in sentiment analysis, topic modeling, and more. Common libraries include NLTK, SpaCy, and Gensim.\n\n\nTrustworthy ML\nN/A\nApproaches and practices in machine learning that ensure models are reliable, fair, and transparent, especially in critical applications like healthcare or finance.\n\n\nUdacity\nN/A\nAn online learning platform offering courses, including nanodegree programs, on a variety of topics including machine learning and data science. Often includes projects and code-alongs.\n\n\nViT\nVision Transformer\nViT is based on the transformer architecture, originally designed for NLP tasks but adapted for vision tasks. Images are split into patches (usually 16x16), which are flattened and treated as input tokens. These patches are then processed similarly to how words are processed in NLP transformers like BERT.\n\n\n\nVLM | Visual-Language Model | A type of machine learning model that understands and generates content based on both visual and textual inputs, often used in tasks like image captioning and visual question answering. Examples include CLIP and LLaVA. |",
    "crumbs": [
      "Category glossary"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ML+X Nexus: Crowdsourced ML and AI Resources",
    "section": "",
    "text": "Nexus is the ML+X community’s centralized hub for sharing machine learning (ML) and AI resources. Visit the ML+X website to learn more about the community, and join the ML+X google group to stay informed on upcoming community events!\n\nWhat kinds of resources are hosted on Nexus?\nAny content (original or external) that can help make the practice of ML more connected, accessible, efficient, and reproducible is welcome on the Nexus platform! This includes, but is not limited to…\n\n🧠 Educational materials: Explore a library of educational materials (workshops, guides, books, videos, etc.) covering a wide range of ML-related topics, tools, and workflows, from foundational concepts to advanced techniques. These materials offer clear explanations, practical examples, and actionable insights to help you navigate the complexities of ML with confidence.\n🛠 Models, code, and more: Learn about popular pretrained & foundation models, useful scripts, and datasets that you can leverage for your next ML project. Learn about their features, how to use them effectively, and see examples of them in action.\n🧬 Applications & stories: Discover a curated collection of blogs, papers, and talks which dive into real-world ML applications and lessons learned by practitioners. This section also includes exploratory data analysis (EDA) case studies, which demonstrate the technical and domain knowledge needed to explore data from various fields.\n\n\n\nMake a contribution to Nexus!\nThis website is a team effort! We welcome and encourage fellow practitioners to contribute resources to Nexus. Learn more by visiting How to contribute.\n\n\nExplore resources\nTo narrow down your search, select one of the general category groupings from the left sidebar (e.g., Learn, Applications, Toolbox). You can also select one of the category tags on the right sidebar to filter resources. Visit the Category glossary if you are unsure about the meaning of any of these tags.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to AWS SageMaker for Predictive ML/AI\n\n\n\nWorkshops\n\n\nCode-along\n\n\nCompute\n\n\nAWS SageMaker\n\n\nPredictive modeling\n\n\nMachine learning\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-11-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKornia\n\n\n\nLibraries\n\n\nModel exploration\n\n\nComputer vision\n\n\nDeep learning\n\n\nPyTorch\n\n\nImage processing\n\n\nViT\n\n\nSAM\n\n\nLoFTR\n\n\nRT-DETR\n\n\n\n\n\n\n\nRadi Akbar\n\n\n2024-10-29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeaf Vein Dataset (LVD2021)\n\n\n\nData\n\n\nImage\n\n\nPlant phenotyping\n\n\nImage segmentation\n\n\nComputer vision\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-10-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nXGBoost: Tree-Based Gradient Boosting for Tabular Data\n\n\n\nModels\n\n\nGradient boosting\n\n\nDecision trees\n\n\nTabular data\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-10-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject Gutenberg: Text & Audio Books\n\n\n\nData\n\n\nText\n\n\nAudio\n\n\nMultimodal\n\n\nNLP\n\n\nText analysis\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-10-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring the Titanic Dataset\n\n\n\nEDA\n\n\nTabular\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-10-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nU-Net: Convolutional Networks for Biomedical Image Segmentation\n\n\n\nModels\n\n\nDeep learning\n\n\nMedical imaging\n\n\nImage segmentation\n\n\nCNN\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-09-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVision, Language, and Vision-Language Modeling in Radiology\n\n\n\nVideos\n\n\nML4MI\n\n\nMedical imaging\n\n\nVLM\n\n\nViT\n\n\nUNET\n\n\nLLaVA\n\n\nComputer vision\n\n\nCNN\n\n\nLLM\n\n\nDeep learning\n\n\nMultimodal learning\n\n\n\nIn this ML4MI seminar, Tyler Bradshaw highlights the history and current use of vision (e.g., UNET), language, and vision-language models in medical imaging.\n\n\n\nTyler Bradshaw, PhD\n\n\n2024-09-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat Tune Is That? A Humanities Application of Deep Learning\n\n\n\nBlogs\n\n\nDeep learning\n\n\nConformer\n\n\nTransformer\n\n\nCNN\n\n\nHumanities\n\n\nAudio\n\n\nMusic\n\n\nCSI\n\n\nTime-series\n\n\n\n\n\n\n\nAlan Ng\n\n\n2024-09-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMONAI: Medical Open Network for AI\n\n\n\nLibraries\n\n\nDeep learning\n\n\nPyTorch\n\n\nMedical imaging\n\n\nModel exploration\n\n\n\n\n\n\n\nAlan McMillan\n\n\n2024-08-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrokking\n\n\n\nVideos\n\n\nDeep learning\n\n\nEmpirical patterns\n\n\nGrokking\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Python (Carpentries)\n\n\n\nWorkshops\n\n\nPython\n\n\nCarpentries\n\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Machine Learning with Sklearn (Carpentries)\n\n\n\nWorkshops\n\n\nLibraries\n\n\nClassical ML\n\n\nSklearn\n\n\nCarpentries\n\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Deep Learning with Keras (Carpentries)\n\n\n\nWorkshops\n\n\nLibraries\n\n\nDeep learning\n\n\nKeras\n\n\nCarpentries\n\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Deep Learning with PyTorch (Udacity)\n\n\n\nWorkshops\n\n\nLibraries\n\n\nDeep learning\n\n\nPyTorch\n\n\nUdacity\n\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-15\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Deep Learning\n\n\n\nBooks\n\n\nDeep learning\n\n\nPyTorch\n\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Text Analysis / NLP (Carpentries)\n\n\n\nWorkshops\n\n\nDeep learning\n\n\nHugging Face\n\n\nText analysis\n\n\nNLP\n\n\nLLM\n\n\nCarpentries\n\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-13\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverview of Reproducibility Lecture\n\n\n\nVideos\n\n\nReproducibility\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVersion Control with GitHub Desktop\n\n\n\nGuides\n\n\nReproducibility\n\n\nGit/GitHub\n\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVersion Control with Git and GitHub (Carpentries)\n\n\n\nWorkshops\n\n\nVideos\n\n\nReproducibility\n\n\nGit/GitHub\n\n\nCode-along\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOut-of-Distribution Detection\n\n\n\nVideos\n\n\nOOD detection\n\n\nTrustworthy ML\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-07-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCenter for High Throughput Computing (CHTC)\n\n\n\nCompute\n\n\nGPU\n\n\n\n\n\n\n\nChris Endemann\n\n\n2024-06-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Contribute?\n\n\n\nGuides\n\n\nContribute\n\n\n\n\n\n\n\nML+X\n\n\n2024-06-24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Biophysics-based Protein Language Model for Protein Engineering\n\n\n\nVideos\n\n\nCross Labs AI\n\n\nTransfer learning\n\n\nBiophysics\n\n\nProtein language models\n\n\nFoundation models\n\n\nLLM\n\n\nDeep learning\n\n\nProtein engineering\n\n\nSimulations\n\n\n\nWe introduce Mutational Effect Transfer Learning (METL), a specialized protein language model that bridges the gap between traditional biophysics-based and machine learning…\n\n\n\nSam Gelman, PhD\n\n\n2024-06-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Large Language Models for Meteorological Fact Finding\n\n\n\nVideos\n\n\nLLM\n\n\nMeteorology\n\n\n\nThis talk demonstrates harnessing the power of AI to open new avenues in data analysis, including for meteorological fact-finding. Discover how cutting-edge large language…\n\n\n\nZekai Otles\n\n\n2024-05-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Model Sharing in the Age of Foundation Models\n\n\n\nVideos\n\n\nML+X\n\n\nMultimodal learning\n\n\nFoundation models\n\n\nModel sharing\n\n\nHugging Face\n\n\nLLM\n\n\nLMM\n\n\nLLaVA\n\n\nDeep learning\n\n\n\n\nModel sharing and reproducible ML\nLLaVA-NeXT and model sharing\n\n\n\n\nChris Endemann, Haotian Liu, PhD\n\n\n2024-03-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring AI at UW-Madison\n\n\n\nMultidisciplinary\n\n\nUW-Madison\n\n\nPlaylists\n\n\n\nA summer 2023 webinar series sponsored by the Division of Information Technology and the Data Science Institute.\n\n\n\nChris Endemann\n\n\n2023-06-23\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "Contributor-templates/template_toolbox-model.html",
    "href": "Contributor-templates/template_toolbox-model.html",
    "title": "Model Name",
    "section": "",
    "text": "[Model Name] is a [brief model description: purpose and key characteristics]. Introduced in [Year] by [Author(s)/Organization] in the paper “Model Paper Title,” [Model Name] has become a widely used model for [specific task(s), e.g., image classification, text generation]. Its [key architecture/approach: transformer, decision trees, etc.] allows it to excel in [key domains/tasks]. It is now used across domains, from [example application 1] to [example application 2].\n\n\n\n[Architecture]: [Description of model’s architecture, e.g., CNN, Transformer, Gradient Boosting]\n\nFor vision models: Can describe use of convolutional layers, attention mechanisms, etc.\nFor NLP models: Can describe tokenization, transformer-based layers, etc.\nFor tabular models: Can describe decision trees, boosting mechanisms, etc.\n\n[Feature 2]: [Describe another feature, e.g., skip connections, attention, gradient boosting]\n[Data Efficiency]: [Mention if the model is effective with small datasets or if it requires large-scale data]"
  },
  {
    "objectID": "Contributor-templates/template_toolbox-model.html#about-this-resource",
    "href": "Contributor-templates/template_toolbox-model.html#about-this-resource",
    "title": "Model Name",
    "section": "",
    "text": "[Model Name] is a [brief model description: purpose and key characteristics]. Introduced in [Year] by [Author(s)/Organization] in the paper “Model Paper Title,” [Model Name] has become a widely used model for [specific task(s), e.g., image classification, text generation]. Its [key architecture/approach: transformer, decision trees, etc.] allows it to excel in [key domains/tasks]. It is now used across domains, from [example application 1] to [example application 2].\n\n\n\n[Architecture]: [Description of model’s architecture, e.g., CNN, Transformer, Gradient Boosting]\n\nFor vision models: Can describe use of convolutional layers, attention mechanisms, etc.\nFor NLP models: Can describe tokenization, transformer-based layers, etc.\nFor tabular models: Can describe decision trees, boosting mechanisms, etc.\n\n[Feature 2]: [Describe another feature, e.g., skip connections, attention, gradient boosting]\n[Data Efficiency]: [Mention if the model is effective with small datasets or if it requires large-scale data]"
  },
  {
    "objectID": "Contributor-templates/template_toolbox-model.html#timeline-context",
    "href": "Contributor-templates/template_toolbox-model.html#timeline-context",
    "title": "Model Name",
    "section": "Timeline context",
    "text": "Timeline context\n[Model Name] fits into the broader development of models for [task/domain]. Here is a timeline placing [Model Name] in the context of other important models.\n\nModel Predecessor 1 (Year): [Short description of relevant model]\nModel Predecessor 2 (Year): [Short description of relevant model]\nModel Name (Year): [Short description of the current model and its contribution]\nMore Recent Models (Year): [Additional models for comparison]"
  },
  {
    "objectID": "Contributor-templates/template_toolbox-model.html#model-name-variants",
    "href": "Contributor-templates/template_toolbox-model.html#model-name-variants",
    "title": "Model Name",
    "section": "[Model Name] Variants",
    "text": "[Model Name] Variants\n\n[Variant 1 Name]: [Description of the variant and its improvements/changes over the base model]\n\nFor models like XGBoost: Could include different hyperparameter settings or regularization options.\nFor vision models: Could include variants with attention mechanisms or depth changes.\nFor NLP models: Could include pretrained vs fine-tuned versions.\n\n[Variant 2 Name]: [Description of another variant]\n[Popular Version]: [E.g., nnU-Net for U-Net, or larger CLIP models]"
  },
  {
    "objectID": "Contributor-templates/template_toolbox-model.html#model-playground",
    "href": "Contributor-templates/template_toolbox-model.html#model-playground",
    "title": "Model Name",
    "section": "Model playground",
    "text": "Model playground\n\nTutorials and Getting Started Notebooks\n\n[Model-specific tutorial/notebook]: Link to official tutorials, Colab notebooks, or GitHub repositories for getting started with the model.\n\n\n\nHigh-level tips for effective use\n\nPre-trained Models: [Guidance on using pre-trained models, if applicable]\nRegularization Techniques: [Suggestions for preventing overfitting, especially with smaller datasets]\n\nFor models like XGBoost: Feature regularization, early stopping\nFor deep learning: Dropout, weight decay\n\nData Augmentation: [Best practices for augmentation]\n\nFor vision models: Flipping, rotation, color jitter\nFor NLP: Data augmentation techniques like backtranslation\n\nLoss Function: [Optimizing loss functions for the specific task]\nArchitectural Adjustments: [Guidance on modifying the architecture based on dataset size, task complexity]\n\n\n\nRelated datasets & challenges\n\n[Relevant Dataset 1]: [Provide a relevant dataset for training or benchmarking the model]\n\nVision: COCO, ImageNet\nNLP: Common Crawl, Hugging Face datasets\nTabular: Kaggle, UCI ML Repository\n\n[Related Challenge 1]: [Link to a Kaggle challenge or relevant benchmark competition]"
  },
  {
    "objectID": "Contributor-templates/template_toolbox-model.html#questions",
    "href": "Contributor-templates/template_toolbox-model.html#questions",
    "title": "Model Name",
    "section": "Questions?",
    "text": "Questions?\nIf you have any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Contributor-templates/template_toolbox-model.html#see-also",
    "href": "Contributor-templates/template_toolbox-model.html#see-also",
    "title": "Model Name",
    "section": "See also",
    "text": "See also\n\n[Related Workshop or Playlist]: [Link to a relevant tutorial, seminar, or workshop]"
  },
  {
    "objectID": "Contributor-templates/template_toolbox-library.html",
    "href": "Contributor-templates/template_toolbox-library.html",
    "title": "Library Name",
    "section": "",
    "text": "[Library Name] is a [brief description of the library: its purpose and key features]. Developed by [Author(s)/Organization] and introduced in [Year], [Library Name] offers tools for [specific task(s), e.g., image transformations, data augmentation, visualization]. Its design allows users to [mention ease of use, efficiency, flexibility], making it ideal for [target audience or task, e.g., deep learning practitioners, data engineers, etc.].\n\n\n\n[Feature 1]: [Describe a core feature, e.g., fast image transformations, robust preprocessing for NLP, etc.]\n\nFor vision libraries: Could describe specific transformations like rotation, flipping, scaling.\nFor NLP libraries: Could describe tokenization methods, text augmentation.\nFor tabular data libraries: Could describe missing data imputation, feature scaling, etc.\n\n[Feature 2]: [Another key feature of the library]\nPerformance: [Mention performance benchmarks if available, or give an overview of efficiency and scalability]"
  },
  {
    "objectID": "Contributor-templates/template_toolbox-library.html#about-this-library",
    "href": "Contributor-templates/template_toolbox-library.html#about-this-library",
    "title": "Library Name",
    "section": "",
    "text": "[Library Name] is a [brief description of the library: its purpose and key features]. Developed by [Author(s)/Organization] and introduced in [Year], [Library Name] offers tools for [specific task(s), e.g., image transformations, data augmentation, visualization]. Its design allows users to [mention ease of use, efficiency, flexibility], making it ideal for [target audience or task, e.g., deep learning practitioners, data engineers, etc.].\n\n\n\n[Feature 1]: [Describe a core feature, e.g., fast image transformations, robust preprocessing for NLP, etc.]\n\nFor vision libraries: Could describe specific transformations like rotation, flipping, scaling.\nFor NLP libraries: Could describe tokenization methods, text augmentation.\nFor tabular data libraries: Could describe missing data imputation, feature scaling, etc.\n\n[Feature 2]: [Another key feature of the library]\nPerformance: [Mention performance benchmarks if available, or give an overview of efficiency and scalability]"
  },
  {
    "objectID": "Contributor-templates/template_toolbox-library.html#integration-and-compatibility",
    "href": "Contributor-templates/template_toolbox-library.html#integration-and-compatibility",
    "title": "Library Name",
    "section": "Integration and compatibility",
    "text": "Integration and compatibility\n[Library Name] integrates with various machine learning frameworks and libraries, making it versatile for a range of tasks.\n\nFrameworks Supported: [List supported frameworks, e.g., PyTorch, TensorFlow, scikit-learn]\nCompatible Libraries: [Mention compatible libraries, e.g., OpenCV, Hugging Face Transformers]\nInstallation Instructions: [Provide installation commands, e.g., pip install library-name, conda install library-name]"
  },
  {
    "objectID": "Contributor-templates/template_toolbox-library.html#use-cases",
    "href": "Contributor-templates/template_toolbox-library.html#use-cases",
    "title": "Library Name",
    "section": "Use cases",
    "text": "Use cases\nHere are some examples of how [Library Name] can be applied to different machine learning tasks.\n\nUse Case 1: [Description of a specific use case, e.g., augmenting image datasets for training CNNs]\nUse Case 2: [Another relevant application, e.g., processing large text corpora for NLP models]\nUse Case 3: [Optional: Add more use cases if applicable]"
  },
  {
    "objectID": "Contributor-templates/template_toolbox-library.html#tutorials-and-resources",
    "href": "Contributor-templates/template_toolbox-library.html#tutorials-and-resources",
    "title": "Library Name",
    "section": "Tutorials and resources",
    "text": "Tutorials and resources\n\nGetting started\n\n[Official Tutorial]: [Link to a beginner-friendly tutorial or notebook]\n[Example Colab/Notebook]: [Link to an example Colab notebook or GitHub repository showcasing the library in action]\n\n\n\nHigh-level tips for effective use\n\nOptimization: [Describe ways to optimize performance, e.g., batch processing for large datasets]\nMemory Management: [Tips on handling memory usage, especially for large datasets]\nCommon Pitfalls: [Mention common mistakes and how to avoid them, e.g., ensuring data formats match]\n\n\n\nRelated libraries & tools\n\n[Related Library 1]: [Mention a related library and what it adds/compares to the current library]\n[Tool/Plugin 1]: [Mention tools/plugins that complement this library]"
  },
  {
    "objectID": "Contributor-templates/template_toolbox-library.html#questions",
    "href": "Contributor-templates/template_toolbox-library.html#questions",
    "title": "Library Name",
    "section": "Questions?",
    "text": "Questions?\nIf you have any lingering questions about this library, please post to the Nexus Q&A on GitHub."
  },
  {
    "objectID": "Contributor-templates/template_toolbox-library.html#see-also",
    "href": "Contributor-templates/template_toolbox-library.html#see-also",
    "title": "Library Name",
    "section": "See also",
    "text": "See also\n\n[Related resource on Nexus or elsewhere]: [Link to relevant tutorials, webinars, or playlists]"
  },
  {
    "objectID": "Applications/Videos/SILO/index.html",
    "href": "Applications/Videos/SILO/index.html",
    "title": "SILO",
    "section": "",
    "text": "The Systems, Information, Learning and Optimization (SILO) research group at the University of Wisconsin-Madison hosts a weekly seminar that covers a variety of topics related to machine learning, optimization, and information theory. SILO breaks down the research “silos” created by academic department boundaries by providing time and space for researchers to present their work, interact, and find common threads.\nA non exhaustive list of recorded SILO seminars (feel free to add others!) can be found here on Nexus. Visit the SILO - Past Talks page to explore additional talks of interest.\n\nJoin the next live SILO!\n\nWhere: Orchard View Room (rm. 3280), Discovery Building & via Zoom.\nWhen: Check the SILO schedule and join the Google group to receive a calendar invite (with Zoom link) and other updates.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorld Knowledge in the Time of Large Models\n\n\n\n\n\n\nVideos\n\n\nSILO\n\n\nVLM\n\n\nLLM\n\n\nLMM\n\n\nMultimodal learning\n\n\nFoundation models\n\n\n\nThis talk will discuss the massive shift that has come about in the vision and ML community as a result of the large pre-trained language and language and vision models such as Flamingo, GPT-4, and other models.\n\n\n\n\n\nNovember 22, 2023\n\n\nKenneth Marino, PhD\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Applications",
      "Videos",
      "SILO"
    ]
  },
  {
    "objectID": "Applications/Videos/ML4MI/index.html",
    "href": "Applications/Videos/ML4MI/index.html",
    "title": "ML4MI",
    "section": "",
    "text": "Explore a library of ML4MI recordings below! Additional ML4MI talks can also be found on the UW-Madison Kaltura Mediaspace (UW NetID required).\nFrom the Machine Learning for Medical Imaging (ML4MI) webpage:\n\nA regular seminar series began in February 2018, and includes 1) seminars describing technical developments in ML with potential biomedical applications, 2) seminars by local or external Radiology researchers, describing problems that may benefit from ML approaches and ongoing projects involving ML techniques, and 3) seminars by biomedical researchers (not in Radiology), describing pioneering experiences applying ML in their fields of study. The seminar location will alternate between ECB/WID and SMPH/WIMR. These seminars will also provide an opportunity for UW researchers to become familiar with researchers “on the other side of campus.”\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVision, Language, and Vision-Language Modeling in Radiology\n\n\n\n\n\n\nVideos\n\n\nML4MI\n\n\nMedical imaging\n\n\nVLM\n\n\nViT\n\n\nUNET\n\n\nLLaVA\n\n\nComputer vision\n\n\nCNN\n\n\nLLM\n\n\nDeep learning\n\n\nMultimodal learning\n\n\n\nIn this ML4MI seminar, Tyler Bradshaw highlights the history and current use of vision (e.g., UNET), language, and vision-language models in medical imaging.\n\n\n\n\n\nSeptember 16, 2024\n\n\nTyler Bradshaw, PhD\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Applications",
      "Videos",
      "ML4MI"
    ]
  },
  {
    "objectID": "Applications/Videos/Other/CrossLabsAI-CrossRoads45-METL-Biophysics-based-Protein-Language-Model.html",
    "href": "Applications/Videos/Other/CrossLabsAI-CrossRoads45-METL-Biophysics-based-Protein-Language-Model.html",
    "title": "A Biophysics-based Protein Language Model for Protein Engineering",
    "section": "",
    "text": "Summary from Cross Labs AI:\n\nJust as words combine to form sentences that convey meaning in human languages, the specific arrangement of amino acids in proteins can be viewed as an information-rich language describing molecular structure and behavior.\nProtein language models harness advances in natural language processing to decode intricate patterns and relationships within protein sequences. These models learn meaningful, low-dimensional representations that capture the semantic organization of protein space and have broad utility in protein engineering. However, while protein language models are powerful, they do not take advantage of the extensive knowledge of protein biophysics and molecular mechanisms acquired over the last century. Thus, they are largely unaware of the underlying physical principles governing protein function.\nWe introduce Mutational Effect Transfer Learning (METL), a specialized protein language model that bridges the gap between traditional biophysics-based and machine learning approaches by incorporating synthetic data from molecular simulations. We pretrain a transformer on millions of molecular simulations to capture the relationship between protein sequence, structure, energetics, and stability. We then finetune the neural network to harness these fundamental biophysical signals and apply them when predicting protein functional scores from experimental assays. METL excels in protein engineering tasks like generalizing from small training sets and extrapolating to new sequence positions. We demonstrate METL’s ability to design functional green fluorescent protein variants when trained on only 64 experimental examples.\n\n\n\nLinks & code\n\nAbout the Speaker → samgelman.com\nCheck out the preprint\nAll code is available under the MIT license. A collection of METL software repositories is provided to reproduce the results of this manuscript and run METL on new data:\n\ngithub.com/gitter-lab/metl for pretraining and finetuning METL PLMs (archived at doi:10.5281/zenodo.10819483)\ngithub.com/gitter-lab/metl-sim for generating biophysical attributes with Rosetta (archived at doi:10.5281/zenodo.10819523)\ngithub.com/gitter-lab/metl-pretrained for making predictions with pretrained METL PLMs (archived at doi:10.5281/zenodo.10819499)\ngithub.com/gitter-lab/metl-pub for additional code and data to reproduce these results (archived at doi:10.5281/zenodo.10819536)\n\n\n\n\n\nJump to section\n\n[3:01] Intro\n[5:03] Proteins as nature’s molecular machines\n[6:58] Proteins defined by a sequence of amino acids\n[10:34] Challenge: Vastness of sequence space\n[11:18] Navigating sequence space\n[11:52] Challenge: Small changes can have a large impact\n[12:51] Protein language models (PLMs)\n[15:57] Incorporating biophysics\n[17:33] Mutational Effect Transfer Learning (METL)\n[19:50] Simulating protein structures with Rosetta\n[21:10] Local and global strategies for simulations\n[24:26] Train transformer encoder to predict Rosetta scores\n[25:38] Finetune to predict experimental fitness score\n[27:10] Evaluation baselines (evolutionary models): METL, ESM, and EVE\n[28:26] Generalizing from small datasets\n[31:08] Extrapolating beyond train set\n[33:50] Simulating specific functions\n[35:46] How much simulated/experimental data is needed?\n[38:01] Engineering GFP variants with METL\n[41:40] Q&A"
  },
  {
    "objectID": "Applications/Videos/index.html",
    "href": "Applications/Videos/index.html",
    "title": "Videos",
    "section": "",
    "text": "Discover a curated collection of talks which dive into AI/ML applications and lessons learned by practitioners.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVision, Language, and Vision-Language Modeling in Radiology\n\n\n\n\n\n\nVideos\n\n\nML4MI\n\n\nMedical imaging\n\n\nVLM\n\n\nViT\n\n\nUNET\n\n\nLLaVA\n\n\nComputer vision\n\n\nCNN\n\n\nLLM\n\n\nDeep learning\n\n\nMultimodal learning\n\n\n\nIn this ML4MI seminar, Tyler Bradshaw highlights the history and current use of vision (e.g., UNET), language, and vision-language models in medical imaging.\n\n\n\n\n\n2024-09-16\n\n\nTyler Bradshaw, PhD\n\n\n\n\n\n\n\n\n\n\n\n\nA Biophysics-based Protein Language Model for Protein Engineering\n\n\n\n\n\n\nVideos\n\n\nCross Labs AI\n\n\nTransfer learning\n\n\nBiophysics\n\n\nProtein language models\n\n\nFoundation models\n\n\nLLM\n\n\nDeep learning\n\n\nProtein engineering\n\n\nSimulations\n\n\n\nWe introduce Mutational Effect Transfer Learning (METL), a specialized protein language model that bridges the gap between traditional biophysics-based and machine learning approaches by incorporating synthetic data from molecular simulations.\n\n\n\n\n\n2024-06-18\n\n\nSam Gelman, PhD\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Large Language Models for Meteorological Fact Finding\n\n\n\n\n\n\nVideos\n\n\nLLM\n\n\nMeteorology\n\n\n\nThis talk demonstrates harnessing the power of AI to open new avenues in data analysis, including for meteorological fact-finding. Discover how cutting-edge large language models (LLMs) like OpenAI’s ChatGPT 3.5 and 4.0 hold are poised to help the field of meteorological data analysis. \n\n\n\n\n\n2024-05-30\n\n\nZekai Otles\n\n\n\n\n\n\n\n\n\n\n\n\nAdvancing Healthcare and Agriculture through Computer Vision\n\n\n\n\n\n\nVideos\n\n\nML+X\n\n\nComputer vision\n\n\nUltrasound\n\n\nMedical imaging\n\n\nAgriculture\n\n\nLSTM\n\n\nCNN-LSTM\n\n\nCNN\n\n\nDeep learning\n\n\n\n1. An ultrasound-based method to measure knee kinematics enabled by deep learning 2. Plant breeding in the age of computer vision \n\n\n\n\n\n2024-04-09\n\n\nMatthew Blomquist, Will de la Bretonne\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Model Sharing in the Age of Foundation Models\n\n\n\n\n\n\nVideos\n\n\nML+X\n\n\nMultimodal learning\n\n\nFoundation models\n\n\nModel sharing\n\n\nHugging Face\n\n\nLLM\n\n\nLMM\n\n\nLLaVA\n\n\nDeep learning\n\n\n\n1. Model sharing and reproducible ML 2. LLaVA-NeXT and model sharing \n\n\n\n\n\n2024-03-12\n\n\nChris Endemann, Haotian Liu, PhD\n\n\n\n\n\n\n\n\n\n\n\n\nNavigating Gravitational Waves with AI Insights\n\n\n\n\n\n\nVideos\n\n\nML+X\n\n\nPhysics\n\n\nSimulations\n\n\n\n1. Welcome and small group discussions 2. Classifying gravitational wave modes from core-collapse supernovae \n\n\n\n\n\n2024-02-13\n\n\nChris Endemann, Bella Finkel\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Science Communication and Drug Synergy Analysis using GPT\n\n\n\n\n\n\nVideos\n\n\nML+X\n\n\nScience communication\n\n\nHealthcare\n\n\nDrug synergy\n\n\nLLM\n\n\nText mining\n\n\n\n1. GPT for Science Communication: User-Interface and Developer Pipeline Approaches 2. Advancing Biomedical Research with GPT-4: A Novel Approach to Drug Synergy Analysis using Text Mining and Classification \n\n\n\n\n\n2023-12-12\n\n\nBen Rush, PhD, Jack Freeman\n\n\n\n\n\n\n\n\n\n\n\n\nWorld Knowledge in the Time of Large Models\n\n\n\n\n\n\nVideos\n\n\nSILO\n\n\nVLM\n\n\nLLM\n\n\nLMM\n\n\nMultimodal learning\n\n\nFoundation models\n\n\n\nThis talk will discuss the massive shift that has come about in the vision and ML community as a result of the large pre-trained language and language and vision models such as Flamingo, GPT-4, and other models.\n\n\n\n\n\n2023-11-22\n\n\nKenneth Marino, PhD\n\n\n\n\n\n\n\n\n\n\n\n\nLLMS in Genomic and Health Coaching\n\n\n\n\n\n\nVideos\n\n\nML+X\n\n\nHealthcare\n\n\nClustering\n\n\nDeep learning\n\n\nLLM\n\n\nGenomics\n\n\n\n1. Clustering of genomic sequences of mycoviruses using deep learning 2. Spurring self-improvement and intrinsic motivation using LLMs and reinforcement learning \n\n\n\n\n\n2023-11-07\n\n\nRohan Sontahlia, Michael Roytman\n\n\n\n\n\n\n\n\n\n\n\n\nTime-Series Analysis\n\n\n\n\n\n\nVideos\n\n\nML+X\n\n\nTime-series\n\n\nGenomics\n\n\nHealthcare\n\n\n\n1. Computational Methods for Comparative Time Clocks in Early Development and Tissue Regeneration 2. Controlled Differential Equations on Long Sequences via Non-standard Wavelets \n\n\n\n\n\n2023-10-10\n\n\nPeng Jiang, PhD, Sourav Pal\n\n\n\n\n\n\n\n\n\n\n\n\nMultimodal Learning\n\n\n\n\n\n\nVideos\n\n\nML+X\n\n\nMultimodal learning\n\n\nDeep learning\n\n\nComputer vision\n\n\nHealthcare\n\n\nGenomics\n\n\n\n1. Multimodal learning and analysis for understanding single-cell functional genomics in brains and brain diseases 2. Transforming healthcare: AI-enhanced disease quantification with vision-language models 3. The benefits of early fusion: deeply integrated audio-visual representation learning \n\n\n\n\n\n2023-09-19\n\n\nDaifeng Wang, PhD, Zachary Huemann, Pedro Morgado, PhD\n\n\n\n\n\n\n\n\n\n\n\n\nExploring AI at UW-Madison\n\n\n\n\n\n\nMultidisciplinary\n\n\nUW-Madison\n\n\nPlaylists\n\n\n\nA summer 2023 webinar series sponsored by the Division of Information Technology and the Data Science Institute.\n\n\n\n\n\n2023-06-23\n\n\nChris Endemann\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Applications",
      "Videos"
    ]
  },
  {
    "objectID": "Applications/Videos/Forums/mlx_2024-03-12.html",
    "href": "Applications/Videos/Forums/mlx_2024-03-12.html",
    "title": "Exploring Model Sharing in the Age of Foundation Models",
    "section": "",
    "text": "Model sharing (via platforms such as Hugging Face) has become more commonplace over the past few years as practitioners increasingly rely on pretrained models and foundation models to find patterns in their data. In this month’s forum, we discuss best practices surrounding model sharing and learn about the recently released LLaVA-NeXT model — a large multimodal model (LMM) which can be used for vision+text related tasks.\n\nModel sharing and reproducible ML - Chris Endemann\nDuring this facilitated discussion, we will share ideas and experiences surrounding best practices in reproducible ML. We will explore questions such as:\n\nWhy should you share your ML model?\nWhat are potential challenges, risks, or ethical concerns associated with model sharing and reproducing ML workflows?\nHow do you market or share your models in a way that ensures proper use?\nWhat pieces must be well-documented to ensure reproducible and responsible model sharing?\n\n\nLLaVA-NeXT and model sharing - Haotian Liu\nWe’ve been open sourcing the LLaVA-series multimodal models in the past year, and our LLaVA-series of work has inspired many creative researches and explorations based on our open-source releases. We’ll give a brief introduction of our LLaVA model and how we’ve been releasing model weights and maintaining our open-source code bases. We’ll also discuss some challenges we face on open-source release, including hosting cost, privacy, commercial license, etc."
  },
  {
    "objectID": "Applications/EDA/index.html",
    "href": "Applications/EDA/index.html",
    "title": "Exploratory analysis",
    "section": "",
    "text": "Share your EDA methods!\nAre you passionate about data and keen to share your insights? We invite you to contribute to our exploratory data analysis (EDA) case studies on Nexus.\nWhether you’re working with biological data, engineering datasets, social sciences information, or any other domain, your contributions can help elevate the community’s understanding and application of EDA techniques.\nGet started by posting a brief summary of your EDA case study idea as an Issue on GitHub! The Nexus development team will follow up with feedback and further instructions.\n\n\nExplore EDA case studies\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring the Titanic Dataset\n\n\n\n\n\n\nEDA\n\n\nTabular\n\n\n\n\n\n\n\n\n\nOctober 7, 2024\n\n\nChris Endemann\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Applications",
      "EDA"
    ]
  },
  {
    "objectID": "Applications/Blogs/blog-music-identification.html",
    "href": "Applications/Blogs/blog-music-identification.html",
    "title": "What Tune Is That? A Humanities Application of Deep Learning",
    "section": "",
    "text": "Deep learning (neural network training) can solve humanities challenges, too! Read about a successful project that trained a model to be able to identify Irish traditional dance tunes. Project GitHub: github.com/alanngnet/CoverHunterMPS"
  },
  {
    "objectID": "Applications/Blogs/blog-music-identification.html#the-challenge",
    "href": "Applications/Blogs/blog-music-identification.html#the-challenge",
    "title": "What Tune Is That? A Humanities Application of Deep Learning",
    "section": "The Challenge",
    "text": "The Challenge\nHey, this AI stuff isn’t just for hard-science people, we can solve humanities challenges with it, too!\nI hope this blog post might inspire more use of data science and machine learning in the humanities, and maybe even gather some people who want to help me solve musicological mysteries using data science.\nThe mystery I have been working on since about 1993 is how to systematically describe the Irish traditional dance music repertoire. This particular musical culture might be the healthiest European folk music tradition that has survived unbroken for centuries as an aurally transmitted culture, with something on the order of 10,000 musically distinct “tunes” (as musical works for dance use are called in this culture) and tens of thousands of active participants around the world. My main work is published at irishtune.info as a combination scholarly reference work and practical day-to-day tool for the global community of musicians at all levels.\nOne side benefit of the manual work I’ve been doing for 30 years to carefully describe the contents of about a thousand albums of commercially published Irish traditional music is that I (only somewhat intentionally) created an ideal dataset for training a machine-learning solution that can do what only very few human experts can do after a lifetime of experience and use of large archival resources: Hear any performance and identify what tune it is. This is the core challenge I have tackled here.\nBackground explanation: Folk musicians in aurally transmitted traditions generally do not know the “identity” of tunes they play, especially not in any kind of broadly reliable or generally agreed-upon way other than as often-contradictory informal assertions, each held within a subset of the global community. Welcome to the fuzziness of humanities research! :)\nSome benefits of solving this challenge:\n\nAccelerate my work as a human expert as I expand the coverage of albums represented in irishtune.info, which in turn benefits the global community of musicians and the health of the tradition itself. This goal was accomplished as of July 27, 2024, but refinement continues.\nEmpower the global community of Irish traditional musicians to identify their own tunes and recordings. (I am currently looking for help with this technical challenge.)\nNow that I have solved this for Irish traditional music, how do we enable musicologists and musicians interested in all the other folk musics on planet Earth to create the same kind of solution for those musical cultures?"
  },
  {
    "objectID": "Applications/Blogs/blog-music-identification.html#the-solution",
    "href": "Applications/Blogs/blog-music-identification.html#the-solution",
    "title": "What Tune Is That? A Humanities Application of Deep Learning",
    "section": "The Solution",
    "text": "The Solution\nFor better or worse, commercial music is a big-money industry. That means there are both well-funded organizations involved as well as financial incentives to analyze and automate all sorts of business processes related to pop music. For example, social media platforms have to worry about copyright and licensing issues in any kind of social-media post that includes music in any form. Youtube or Tiktok, for example, need to make sure that your video of yourself singing a Taylor Swift song in the car doesn’t get shared without the proper licensing fees being paid to the artist. And in order to do that, they needed to solve the problem of “What song is that?” in an automated way. So they have been funding CSI (Cover Song Identification) research for years, and that has occasionally shown up in public as open-source code repositories, like this ground-breaking one that appeared in July 2023 from an industry-funded team of researchers in China: github.com/Liu-Feng-deeplearning/CoverHunter\nWhile humanities research would never get the resources to build a solution from scratch to, say, figure out a way to catalog 10,000 hours of audio in an Armenian folk song archive, we can certainly take advantage of big-industry solutions that solve similar problems.\nAnd that’s exactly what I did. I forked that CoverHunter project and the result is published as github.com/alanngnet/CoverHunterMPS.\nIt took about 6 months of:\n\nReverse-engineering the very poorly documented CoverHunter code to understand it enough to proceed.\nTeaching myself just enough about machine learning, neural network training, and just enough new Python skills to proceed.\nFixing bugs and documenting both the Python code itself as well as how to use it. My correspondence with the lead author of CoverHunter confirmed that their industry sponsor required them to remove proprietary aspects of the code, which presumably broke things in the process. Plus they wrote their solution to run on big industry-scale server farms which humanities researchers typically have no way to fund, so I had to revise it to run on a desktop.\nData wrangling - mainly in the form of writing Python scripts - to automate the large-scale tasks of leveraging my own database and audio library to prepare training data in the format needed by the Python application.\nHyperparameter tuning (and adding more hyperparameters). So many long training runs, so many TensorBoard graphs to pore over!\nAdding features to take the CoverHunter solution, which was only built out enough to generate the training metrics needed to claim success as a CSI research breakthough, and turn it into a reliable, easy-to-use solution that answers “What tune is this?” for any arbitrary audio input.\n\nIn case you are interested in a more technical summary of the solution, it involves:\n\nConversion of raw audio data to CQT (Constant-Q Transform) 2-D arrays, representing time and frequency dimensions that can also be treated as a visual picture of the audio, so that visual machine-learning methods and models could also be leveraged. For example, a single-instrument melody appears as a line moving vertically higher for higher notes and lower for lower notes, and longer horizontally when notes are sustained for a longer time. CQT is a well-established method of audio analysis in the larger academic research field of MIR (music information retrieval).\nA lot of artificial data augmentation done both in pre-training preprocessing as well as on-the-fly augmentation done during training. For each real-world audio sample you give to this solution, it will generate 4 other variants in pre-processing, and then during training each of those will in turn get augmented (modified) in different, partly random ways during each training step. So by the end of a full training run, the model may have seen hundreds of artificial variants of each real-world data sample.\nA PyTorch-based implementation of a conformer neural network, a somewhat unusual or newer complex network that combines a CNN (convolutional neural network) for small-timescale learning of specific patterns with a transformer network that enables large-timescale and structurally flexible learning. You need the latter because real-world musicians (unlike, say, music played at the school level) are free to modify the structure of a work without hurting the musical identity of the work, and are free to improvise and vary within the structure, likewise without causing a human listener to fail to recognize the identity of the work.\nLeveraging a “bag of tricks” of various neural-network training optimization techniques that the CoverHunter authors adapted from previous deep-learning research, including taking a lot of source code from various - often unidentified - open-source projects in the field of automated speech recognition. This amalgamation of code from many undocumented sources was a big part of why my first task of even understanding their code was so challenging (I did fix a lot of that along the way).\nOutput of a 128-dimensional tensor (a 1x128-element array of numbers) that uniquely describes an audio performance. The tune-identification solution basically does a relatively simple mathematical comparison of the model-generated tensor for your mystery audio file against the model-generated tensors for all the known, identified audio files in your database, and finds the nearest neighbors for you."
  },
  {
    "objectID": "Applications/Blogs/blog-music-identification.html#how-good-is-it",
    "href": "Applications/Blogs/blog-music-identification.html#how-good-is-it",
    "title": "What Tune Is That? A Humanities Application of Deep Learning",
    "section": "How Good Is It?",
    "text": "How Good Is It?\nHere is a graph of mAP (mean average precision) during a late-August 2024 full training run using the K-fold cross-validation production-training tool in this project. The horizontal axis is a time dimension measured in training epochs. Each different colored line is one of the 5 folds, plus a 6th full-dataset training run.\n94% accuracy at identifying “easy reels” is very, very good! So is 74% accuracy for “hard reels,” which are performances I selected as probably going to be challenging for any ML solution to be able to recognize. After using it in the real world in the last month to identify tunes on new audio recordings, I am confident that this solution performs better than any human expert will ever be able to, especially given that it spits out its best guess in a couple seconds! In this specialized niche, it is already “superhuman AI.”\n\n\n\nGraph of training results for a production training run, measured as mAP (mean average precision) against two test datasets.\n\n\nDetails about the two test datasets, “reels50easy” and “reels50hard,” are published at www.irishtune.info/public/MLdata.htm"
  },
  {
    "objectID": "Applications/Blogs/blog-music-identification.html#how-did-i-learn-how-to-do-this",
    "href": "Applications/Blogs/blog-music-identification.html#how-did-i-learn-how-to-do-this",
    "title": "What Tune Is That? A Humanities Application of Deep Learning",
    "section": "How Did I Learn How to Do This?",
    "text": "How Did I Learn How to Do This?\nBefore this project, I had zero machine-learning experience, only intermediate-level Python experience, and a 30-year gap of disuse since my undergraduate coursework in college-level math.\n\nHuman Helpers\nMy first step in later 2023 - see the next section for the backstory before then - was to reach out to the global community of users of irishtune.info, as well as to the ML+X community here in Madison, to seek collaborators and advisors. That effort netted me several useful connections.\nJosh Liu, an Irish musician in the New England area and a data-science student, was the first to offer help, and I asked him to point me to where the state-of-the-art is at the moment in the field of CSI (Cover Song Identification), especially if any open-source code might be out there. He is the person who found the CoverHunter research paper for me, and shared that although his attempt to use their published code and pre-trained (pop-music trained) model failed for Irish music, he saw it had fundamental promise, especially knowing that I had a uniquely large and high-quality dataset to train it with.\nA couple other Irish musicians who work in the data-science/“AI” industry responded with interest, and ended up giving me some useful advice and encouragement in single video calls early in my struggles with the CoverHunter project. Knowing that an AI professional thinks your own newbie’s pie-in-the-sky idea is worth pursuing is a powerful motivator!\nProbably the most useful help I got was from Samuel Gauthier, an Irish musician in France who is a professional Python developer, even though he had no experience in machine learning. He got hands-on with me directly in my GitHub project, especially cleaning up and improving a lot of the inscrutable and inefficient code that the CoverHunter authors had left as-is during their creative but messy work of combining good ideas from other open-source machine learning projects.\n\n\nAI Chat Assistants\nAI chat-based assistants were absolutely crucial in my ability to do this project, much less in a mere 6 months as a side hobby. I needed them for everything from explaining cryptic bits of Python code to me at just the right level for my background, to teaching me exactly the right bits of data science and machine learning literacy that I needed for this project, to writing first-through-twentieth drafts of a lot of the Python code I needed to add.\nEven before I reached out for human help in late 2023, what sparked the entire project was when I tried out the then-new ChatGPT in early 2023, by asking it the very high-level question of how one might approach my decades-old dream of using machine learning with my database to solve the “What is this tune?” problem. The specificity and doability of its suggestions, and its ability to write the necessary code for me, was mind-blowing! Something that had previously seemed impossible - at least without me somehow earning a degree in data science or finding a wealthy donor to fund a team - looked suddenly within reach of my hobbyist time and skills. ChatGPT got me as far as building a simple CNN model from scratch to at least solve the problem of “is this tune a reel, jig, polka, hornpipe, (etc.). …?” which eventually fizzled out with unsatisfactory training results. But ChatGPT got the whole thing kicked off, so that I was now literate enough and confident enough to seek human helpers … see above for that story.\nDuring early-mid 2024, I did frequent comparisons between getting help from ChatGPT, Google Gemini, and Claude. I found early on that Gemini was helpful at explaining concepts and code, but was a waste of my time for trickier code-writing assistance. ChatGPT and Claude were kind of neck-and-neck with writing code, but as 2024 progressed, Claude began winning my anecdotal “contests.” In general both were good enough for it to clearly be worth my time to use them. With any AI assistant, it generally takes a longer conversation of back-and-forth, with me reading its code, testing it on my computer, and providing feedback to the assistant, to eventually reach my desired solution. And occasionally - really a minority of the time -, they just wasted my time, particularly when I ended up going in circles, with the assistant coming back around to fix the latest problem with something that removed or broke the fixes done earlier in the conversation! I suppose I at least got some good, guided practice at thinking through Python puzzles along the way …\nEspecially powerful was my use of the “Project” feature of Claude Pro that came out earlier this summer, in which I could give it my whole project full of Python code and have it write high-quality Python code that is compatible with and efficiently leverages the rest of the project, without my having to explain the background context. For example, the embedding-generation script (tools/make_embeds.py) was written by Claude Pro from scratch as a single response to a single prompt that worked on the first try. (I did eventually modify it further both by hand and with further Claude assistance.)\n\n\nGoogle CoLab\nGoogle CoLab was uniquely helpful to me very early on when I first attempted to get the CoverHunter code working. Having an industry-standard, pre-configured set of hardware and software available in CoLab - for free! - helped me figure out what problems might be unique to my own computer and software vs. what were basic problems in the CoverHunter code. And I hope it might continue to be useful to others attempting the same learning curve, since any interested passer-by curious to try out my project can do so without having to set up or even have access to any particular hardware and software of their own."
  },
  {
    "objectID": "Applications/Blogs/blog-music-identification.html#value-for-other-purposes",
    "href": "Applications/Blogs/blog-music-identification.html#value-for-other-purposes",
    "title": "What Tune Is That? A Humanities Application of Deep Learning",
    "section": "Value for Other Purposes",
    "text": "Value for Other Purposes\n\nI’m hopeful that my project will turn out to help solve other kinds of musicological mysteries than just tune identification. See github.com/alanngnet/CoverHunterMPS#coverhuntermps for some ideas I have.\nIf you’re looking for a high-level summary of the steps involved in preparing, training, evaluating, and productionizing any deep learning solution, consider the “Usage” section of the project Readme at github.com/alanngnet/CoverHunterMPS/blob/main/README.md\nSteal, borrow, and tweak the source code for your own purposes. There are a lot of generally useful bits in this project for any deep-learning classification solution. But even better would be to contribute improvements to this project to share your innovations more broadly.\nFeel free to contact me (alan@alan-ng.net) to tell this story if it can help expand the use of data science in the humanities."
  },
  {
    "objectID": "Applications/Blogs/blog-music-identification.html#questions",
    "href": "Applications/Blogs/blog-music-identification.html#questions",
    "title": "What Tune Is That? A Humanities Application of Deep Learning",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Applications/Blogs/blog-music-identification.html#see-also",
    "href": "Applications/Blogs/blog-music-identification.html#see-also",
    "title": "What Tune Is That? A Humanities Application of Deep Learning",
    "section": "See also",
    "text": "See also\n\nWorkshop: Intro to Deep Learning with PyTorch: Learn how to use the PyTorch deep learning framework"
  }
]