[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ML+X Nexus: Crowdsourced ML Resources",
    "section": "",
    "text": "Nexus is the ML+X community‚Äôs centralized hub for sharing machine learning (ML) resources. Visit the ML+X website to learn more about the community, and join the ML+X google group to stay informed on upcoming community events!\n\nWhat kinds of resources are hosted on Nexus?\nAny content (original or external) that can help make the practice of ML more connected, accessible, efficient, and reproducible is welcome on the Nexus platform! This includes, but is not limited to:\n\nüß† Educational materials: Explore a library of educational materials (workshops, guides, books, videos, etc.) covering a wide range of ML-related topics, tools, and workflows, from foundational concepts to advanced techniques. These materials offer clear explanations, practical examples, and actionable insights to help you navigate the complexities of ML with confidence.\nüß¨ Applications & stories: Discover a curated collection of blogs, papers, and talks which dive into ML applications and lessons learned by practitioners, including exploratory data analysis or EDA case studies. These case studies demonstrate the domain knowledge needed to explore different data types (time-series, tabular, images, etc.) from various fields.\nüõ† Coming soon! Models & data: Learn about useful pretrained models, foundation models, and datasets that you can leverage for your next ML project. Learn about their features, how to use them effectively, and see examples of them in action.\n\n\n\nMake a contribution to Nexus!\nThis website is a team effort! We welcome and encourage fellow practitioners to contribute a useful ML resource to Nexus. For instructions on how to contribute, visit How to contribute?.\n\n\nExplore ML Resources\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nCenter for Highthroughput Computing (CHTC)\n\n\n\n\n\n\n\n\n\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Contribute?\n\n\n\n\n\n\n\n\n\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Deep Learning with Keras (Carpentries)\n\n\n\n\n\n\n\n\n\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Deep Learning with PyTorch (Udacity)\n\n\n\n\n\n\n\n\n\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Machine Learning with Sklearn (Carpentries)\n\n\n\n\n\n\n\n\n\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Python (Carpentries)\n\n\n\n\n\n\n\n\n\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Text Analysis (Carpentries)\n\n\n\n\n\n\n\n\n\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nOut-of-Distribution Detection\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Deep Learning\n\n\n\n\n\n\n\n\n\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nVersion Control with Git and GitHub (Carpentries)\n\n\n\n\n\n\n\n\n\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nVersion Control with GitHub Desktop\n\n\n\n\n\n\n\n\n\n\n\n17 min\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "Resources/index.html",
    "href": "Resources/index.html",
    "title": "Resources",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nCenter for Highthroughput Computing (CHTC)\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Contribute?\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Deep Learning with Keras (Carpentries)\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Deep Learning with PyTorch (Udacity)\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Machine Learning with Sklearn (Carpentries)\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Python (Carpentries)\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Text Analysis (Carpentries)\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOut-of-Distribution Detection\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Deep Learning\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVersion Control with Git and GitHub (Carpentries)\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVersion Control with GitHub Desktop\n\n\n17 min\n\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Learn"
    ]
  },
  {
    "objectID": "Resources/Videos/OOD-detection.html",
    "href": "Resources/Videos/OOD-detection.html",
    "title": "Out-of-Distribution Detection",
    "section": "",
    "text": "The below tutorial from Sharon Li, an Assistant Professor in the Department of Computer Sciences at the University of Wisconsin-Madison, introduces a pervasive problem faced by many machine learning systems deployed in the wild ‚Äî out-of-distribution data.\nOut-of-distribution data, often overlooked but immensely consequential, poses a significant threat to the reliability and efficacy of machine learning models. Through Sharon‚Äôs presentation, viewers gain a comprehensive understanding of this complex phenomenon and its potential ramifications on predictive accuracy.\nCheck out the video below to learn more about this problem and the cutting-edge methods you can equip yourself with to prevent inaccurate model predictions.\n\n\nDetails, slides and videos from other talks at ICCV 2023: abursuc.github.io/many-faces-reliability/\n\n\n\n\nCheck this page again soon for worked examples and exercises (code provided)!"
  },
  {
    "objectID": "Resources/Videos/OOD-detection.html#about-this-resource",
    "href": "Resources/Videos/OOD-detection.html#about-this-resource",
    "title": "Out-of-Distribution Detection",
    "section": "",
    "text": "The below tutorial from Sharon Li, an Assistant Professor in the Department of Computer Sciences at the University of Wisconsin-Madison, introduces a pervasive problem faced by many machine learning systems deployed in the wild ‚Äî out-of-distribution data.\nOut-of-distribution data, often overlooked but immensely consequential, poses a significant threat to the reliability and efficacy of machine learning models. Through Sharon‚Äôs presentation, viewers gain a comprehensive understanding of this complex phenomenon and its potential ramifications on predictive accuracy.\nCheck out the video below to learn more about this problem and the cutting-edge methods you can equip yourself with to prevent inaccurate model predictions.\n\n\nDetails, slides and videos from other talks at ICCV 2023: abursuc.github.io/many-faces-reliability/\n\n\n\n\nCheck this page again soon for worked examples and exercises (code provided)!"
  },
  {
    "objectID": "Resources/Videos/OOD-detection.html#questions",
    "href": "Resources/Videos/OOD-detection.html#questions",
    "title": "Out-of-Distribution Detection",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Resources/Books/index.html",
    "href": "Resources/Books/index.html",
    "title": "Books",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nUnderstanding Deep Learning\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Learn",
      "Books"
    ]
  },
  {
    "objectID": "Resources/Workshops/Intro-ML_Sklearn.html",
    "href": "Resources/Workshops/Intro-ML_Sklearn.html",
    "title": "Intro to Machine Learning with Sklearn (Carpentries)",
    "section": "",
    "text": "The Intro to Machine Learning with Sklearn workshop from the Carpentries will walk you through introductory machine learning concepts as well as how to implement common ML methods (e.g., regression, clustering, classication) using the popular scikit-learn (‚Äúsklearn‚Äù) package. Sklearn makes it possible to quickly fit and evaluate many models in just a few lines of code. It also comes with convenient functions needed for nearly all ML pipelines (e.g., train/test split, gridsearchcv). Note: Don‚Äôt use Sklearn for neural networks (it is the slowest option!). Instead, explore Keras or PyTorch.\n\n\nLearners are expected to have the following knowledge:\n\nBasic Python programming skills and familiarity with the Pandas package. If you need a refresher, these Introductory Python lesson materials are available for independent study.\n\n\n\n\nThis workshop takes approximately 8 hours to complete.\n\n\n\nThe Carpentries is a global organization of researchers who volunteer their time and effort to create workshops that teach software engineering and data analysis skills to other researchers. UW-Madison has its own local Carpentries community which is actively engaged in developing new ML/AI workshops. To be notified of upcoming workshops offered by the Carpentries, make sure to subscribe to the Data Science @ UW Newsletter. The Intro Deep Learning workshop is typically taught in May each year.\n\n\n\nAll Carpentries lessons are published as open source educational materials. You are welcome and encouraged to visit the lesson materials to work through them on your own. If you are involved with a research lab at UW-Madison campus, you may attend Coding Meetup (Tue/Thur, 2:30-4:30pm) to get help working through the materials."
  },
  {
    "objectID": "Resources/Workshops/Intro-ML_Sklearn.html#about-this-resource",
    "href": "Resources/Workshops/Intro-ML_Sklearn.html#about-this-resource",
    "title": "Intro to Machine Learning with Sklearn (Carpentries)",
    "section": "",
    "text": "The Intro to Machine Learning with Sklearn workshop from the Carpentries will walk you through introductory machine learning concepts as well as how to implement common ML methods (e.g., regression, clustering, classication) using the popular scikit-learn (‚Äúsklearn‚Äù) package. Sklearn makes it possible to quickly fit and evaluate many models in just a few lines of code. It also comes with convenient functions needed for nearly all ML pipelines (e.g., train/test split, gridsearchcv). Note: Don‚Äôt use Sklearn for neural networks (it is the slowest option!). Instead, explore Keras or PyTorch.\n\n\nLearners are expected to have the following knowledge:\n\nBasic Python programming skills and familiarity with the Pandas package. If you need a refresher, these Introductory Python lesson materials are available for independent study.\n\n\n\n\nThis workshop takes approximately 8 hours to complete.\n\n\n\nThe Carpentries is a global organization of researchers who volunteer their time and effort to create workshops that teach software engineering and data analysis skills to other researchers. UW-Madison has its own local Carpentries community which is actively engaged in developing new ML/AI workshops. To be notified of upcoming workshops offered by the Carpentries, make sure to subscribe to the Data Science @ UW Newsletter. The Intro Deep Learning workshop is typically taught in May each year.\n\n\n\nAll Carpentries lessons are published as open source educational materials. You are welcome and encouraged to visit the lesson materials to work through them on your own. If you are involved with a research lab at UW-Madison campus, you may attend Coding Meetup (Tue/Thur, 2:30-4:30pm) to get help working through the materials."
  },
  {
    "objectID": "Resources/Workshops/Intro-ML_Sklearn.html#questions",
    "href": "Resources/Workshops/Intro-ML_Sklearn.html#questions",
    "title": "Intro to Machine Learning with Sklearn (Carpentries)",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Resources/Workshops/Intro-ML_Sklearn.html#see-also",
    "href": "Resources/Workshops/Intro-ML_Sklearn.html#see-also",
    "title": "Intro to Machine Learning with Sklearn (Carpentries)",
    "section": "See also",
    "text": "See also\n\nWorkshop: Intro to Deep Learning with Keras: Once you master sklearn, start using Keras to build neural networks quickly.\nWorkshop: Intro to Deep Learning with PyTorch: Explore PyTorch as an alternative deep learning framework (faster but more verbose than Keras)\nBook: Understanding Deep Learning - Simon J.D. Prince: This free textbook is a good modern overview of deep learning, and provides colab notebooks to explore deep learning concepts and implementations. The book uses PyTorch as its framework of choice. You may find additional details in this book that the workshop only briefly touches on."
  },
  {
    "objectID": "Resources/Workshops/Intro-Deeplearning_Keras.html",
    "href": "Resources/Workshops/Intro-Deeplearning_Keras.html",
    "title": "Intro to Deep Learning with Keras (Carpentries)",
    "section": "",
    "text": "The Intro to Deep Learning with Keras workshop from the Carpentries will walk you through introductory deep learning concepts as well as how to build a neural networks in Keras. Keras is high-level wrapper framework (uses PyTorch or Tensorflow in the backend) which allows you to train and evaluate neural networks in just a few lines of code. It may take slightly longer to train a Keras model (compared to PyTorch and Tensorflow), but the difference in performance is often negligible for those that only need to train a few models. The ability to quickly build and test models is the primary selling point of Keras.\n\n\nLearners are expected to have the following knowledge:\n\nBasic Python programming skills and familiarity with the Pandas package. If you need a refresher, these Introductory Python lesson materials are available for independent study.\nBasic knowledge on machine learning, including the following concepts: Data cleaning, train & test split, type of problems (regression, classification), overfitting & underfitting, metrics (accuracy, recall, etc.).The Intro to Machine Learning with Sklearn lesson materials are a good option for those that need a refresher on machine learning fundamentals.\n\n\n\n\nThis workshop takes approximately 15 hours to complete.\n\n\n\nThe Carpentries is a global organization of researchers who volunteer their time and effort to create workshops that teach software engineering and data analysis skills to other researchers. UW-Madison has its own local Carpentries community which is actively engaged in developing new ML/AI workshops. To be notified of upcoming workshops offered by the Carpentries, make sure to subscribe to the Data Science @ UW Newsletter. The Intro Deep Learning workshop is typically taught in May each year.\n\n\n\nAll Carpentries lessons are published as open source educational materials. You are welcome and encouraged to visit the lesson materials to work through them on your own. If you are involved with a research lab at UW-Madison campus, you may attend Coding Meetup (Tue/Thur, 2:30-4:30pm) to get help working through the materials."
  },
  {
    "objectID": "Resources/Workshops/Intro-Deeplearning_Keras.html#about-this-resource",
    "href": "Resources/Workshops/Intro-Deeplearning_Keras.html#about-this-resource",
    "title": "Intro to Deep Learning with Keras (Carpentries)",
    "section": "",
    "text": "The Intro to Deep Learning with Keras workshop from the Carpentries will walk you through introductory deep learning concepts as well as how to build a neural networks in Keras. Keras is high-level wrapper framework (uses PyTorch or Tensorflow in the backend) which allows you to train and evaluate neural networks in just a few lines of code. It may take slightly longer to train a Keras model (compared to PyTorch and Tensorflow), but the difference in performance is often negligible for those that only need to train a few models. The ability to quickly build and test models is the primary selling point of Keras.\n\n\nLearners are expected to have the following knowledge:\n\nBasic Python programming skills and familiarity with the Pandas package. If you need a refresher, these Introductory Python lesson materials are available for independent study.\nBasic knowledge on machine learning, including the following concepts: Data cleaning, train & test split, type of problems (regression, classification), overfitting & underfitting, metrics (accuracy, recall, etc.).The Intro to Machine Learning with Sklearn lesson materials are a good option for those that need a refresher on machine learning fundamentals.\n\n\n\n\nThis workshop takes approximately 15 hours to complete.\n\n\n\nThe Carpentries is a global organization of researchers who volunteer their time and effort to create workshops that teach software engineering and data analysis skills to other researchers. UW-Madison has its own local Carpentries community which is actively engaged in developing new ML/AI workshops. To be notified of upcoming workshops offered by the Carpentries, make sure to subscribe to the Data Science @ UW Newsletter. The Intro Deep Learning workshop is typically taught in May each year.\n\n\n\nAll Carpentries lessons are published as open source educational materials. You are welcome and encouraged to visit the lesson materials to work through them on your own. If you are involved with a research lab at UW-Madison campus, you may attend Coding Meetup (Tue/Thur, 2:30-4:30pm) to get help working through the materials."
  },
  {
    "objectID": "Resources/Workshops/Intro-Deeplearning_Keras.html#questions",
    "href": "Resources/Workshops/Intro-Deeplearning_Keras.html#questions",
    "title": "Intro to Deep Learning with Keras (Carpentries)",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Resources/Workshops/Intro-Deeplearning_Keras.html#see-also",
    "href": "Resources/Workshops/Intro-Deeplearning_Keras.html#see-also",
    "title": "Intro to Deep Learning with Keras (Carpentries)",
    "section": "See also",
    "text": "See also\n\nWorkshop: Intro to Deep Learning with PyTorch: Explore PyTorch as an alternative deep learning framework.\nBook: Understanding Deep Learning - Simon J.D. Prince: This free textbook is a good modern overview of deep learning, and provides colab notebooks to explore deep learning concepts and implementations. The book uses PyTorch as its framework of choice. You may find additional details in this book that the workshop only briefly touches on."
  },
  {
    "objectID": "Resources/Workshops/Intro-TextAnalysis_Python.html",
    "href": "Resources/Workshops/Intro-TextAnalysis_Python.html",
    "title": "Intro to Text Analysis (Carpentries)",
    "section": "",
    "text": "The Intro to Text Analysis workshop introduces the field of Natural Language Processing (NLP) and how to gain insights from collections of text data (i.e., a corpus). This includes a hands-on, step-by-step guide on how to source and prepare a corpus for analysis, generate text (document/sentence/word) embeddings, perform topic modeling, deploy common models (e.g., Word2Vec and large language models using Hugging Face), and ethical considerations. Students and researchers working with text data (especially the digital humanities!) are encouraged to take this workshop!\n\n\nLearners are expected to have basic Python programming skills and familiarity with the Pandas package. If you need a refresher, the Introductory Python lesson materials are available for independent study.\n\n\n\nThis workshop takes approximately 16 hours to complete.\n\n\n\nThe Carpentries is a global organization of researchers who volunteer their time and effort to create workshops that teach software engineering and data analysis skills to other researchers. UW-Madison has its own local Carpentries community which is actively engaged in developing new ML/AI workshops. To be notified of upcoming workshops offered by the Carpentries, make sure to subscribe to the Data Science @ UW Newsletter. The Intro Deep Learning workshop is typically taught in May each year.\n\n\n\nAll Carpentries lessons are published as open source educational materials. You are welcome and encouraged to visit the lesson materials to work through them on your own. If you are involved with a research lab at UW-Madison campus, you may attend Coding Meetup (Tue/Thur, 2:30-4:30pm) to get help working through the materials.\n\n\n\n\nWorkshop: Intro to Deep Learning with Keras: Explore deep learning concepts in greater detail. This will help you better understand the technology (neural networks) needed for Word2Vec and large language models.\nBook: Understanding Deep Learning - Simon J.D. Prince: This free textbook is a good modern overview of deep learning (and machine learning in general), and provides colab notebooks to explore deep learning concepts and implementations. The book uses PyTorch as its framework of choice. You may find additional details in this book that the workshop only briefly touches on."
  },
  {
    "objectID": "Resources/Workshops/Intro-TextAnalysis_Python.html#about-this-resource",
    "href": "Resources/Workshops/Intro-TextAnalysis_Python.html#about-this-resource",
    "title": "Intro to Text Analysis (Carpentries)",
    "section": "",
    "text": "The Intro to Text Analysis workshop introduces the field of Natural Language Processing (NLP) and how to gain insights from collections of text data (i.e., a corpus). This includes a hands-on, step-by-step guide on how to source and prepare a corpus for analysis, generate text (document/sentence/word) embeddings, perform topic modeling, deploy common models (e.g., Word2Vec and large language models using Hugging Face), and ethical considerations. Students and researchers working with text data (especially the digital humanities!) are encouraged to take this workshop!\n\n\nLearners are expected to have basic Python programming skills and familiarity with the Pandas package. If you need a refresher, the Introductory Python lesson materials are available for independent study.\n\n\n\nThis workshop takes approximately 16 hours to complete.\n\n\n\nThe Carpentries is a global organization of researchers who volunteer their time and effort to create workshops that teach software engineering and data analysis skills to other researchers. UW-Madison has its own local Carpentries community which is actively engaged in developing new ML/AI workshops. To be notified of upcoming workshops offered by the Carpentries, make sure to subscribe to the Data Science @ UW Newsletter. The Intro Deep Learning workshop is typically taught in May each year.\n\n\n\nAll Carpentries lessons are published as open source educational materials. You are welcome and encouraged to visit the lesson materials to work through them on your own. If you are involved with a research lab at UW-Madison campus, you may attend Coding Meetup (Tue/Thur, 2:30-4:30pm) to get help working through the materials.\n\n\n\n\nWorkshop: Intro to Deep Learning with Keras: Explore deep learning concepts in greater detail. This will help you better understand the technology (neural networks) needed for Word2Vec and large language models.\nBook: Understanding Deep Learning - Simon J.D. Prince: This free textbook is a good modern overview of deep learning (and machine learning in general), and provides colab notebooks to explore deep learning concepts and implementations. The book uses PyTorch as its framework of choice. You may find additional details in this book that the workshop only briefly touches on."
  },
  {
    "objectID": "Resources/Guides/Github-desktop.html",
    "href": "Resources/Guides/Github-desktop.html",
    "title": "Version Control with GitHub Desktop",
    "section": "",
    "text": "Navigating the world of version control systems like Git can initially feel daunting, especially for those new to programming or collaborative software development projects. However, with the right tools and guidance, anyone can quickly grasp the essentials and begin leveraging the power of Git for efficient project management and collaboration. While Git commands can be run via a Unix shell, there are alternatives which are more friendly for beginners. In this guide, we‚Äôll explore GitHub Desktop as a convenient and accessible gateway to Git, along with a step-by-step walkthrough on essential Git terminology, setup procedures, tracking changes, collaboration workflows, and even managing Kaggle notebooks seamlessly. Whether you‚Äôre embarking on your first coding adventure or seeking to streamline your team‚Äôs development process, this guide aims to demystify Git and empower you with practical knowledge to navigate the Git landscape with confidence."
  },
  {
    "objectID": "Resources/Guides/Github-desktop.html#about-this-resource",
    "href": "Resources/Guides/Github-desktop.html#about-this-resource",
    "title": "Version Control with GitHub Desktop",
    "section": "",
    "text": "Navigating the world of version control systems like Git can initially feel daunting, especially for those new to programming or collaborative software development projects. However, with the right tools and guidance, anyone can quickly grasp the essentials and begin leveraging the power of Git for efficient project management and collaboration. While Git commands can be run via a Unix shell, there are alternatives which are more friendly for beginners. In this guide, we‚Äôll explore GitHub Desktop as a convenient and accessible gateway to Git, along with a step-by-step walkthrough on essential Git terminology, setup procedures, tracking changes, collaboration workflows, and even managing Kaggle notebooks seamlessly. Whether you‚Äôre embarking on your first coding adventure or seeking to streamline your team‚Äôs development process, this guide aims to demystify Git and empower you with practical knowledge to navigate the Git landscape with confidence."
  },
  {
    "objectID": "Resources/Guides/Github-desktop.html#getting-started",
    "href": "Resources/Guides/Github-desktop.html#getting-started",
    "title": "Version Control with GitHub Desktop",
    "section": "Getting Started",
    "text": "Getting Started\n\nVersion Control\nVersion control is system that records changes to a file or set of files over time so that you can recall specific versions later. It helps in managing changes, keeping track of different versions, and collaborating with multiple people. Version control is an essential tool for reproducible science and software systems that can improve over time.\n\n\nGit\nGit is a free and open source version control system that has become the #1 choice for software developers both in research and industry. Unlike centralized version control systems where there is a single central repository, Git allows every user to have a full copy of the entire project history on their own machine. This distributed nature enables multiple people to work on a project simultaneously without interfering with each other‚Äôs work. Git stores the history of changes in a project, enabling users to track progress, revert to previous states, and manage branches for different features or versions of a project.\n\n\nGitHub\nGitHub is a web-based platform that uses Git for version control. It provides a collaborative environment where users can host and review code, manage projects, and build software alongside millions of other developers. GitHub also offers additional features such as issue tracking, project management tools, and continuous integration workflows.\n\n\nGitHub Desktop\nGitHub Desktop is a graphical user interface (GUI) application that simplifies the use of Git and GitHub. It is designed for users who prefer not to use the command line interface, offering a more intuitive and visual approach to version control. With GitHub Desktop, you can easily perform common Git tasks such as committing changes, creating branches, and resolving merge conflicts, all within a user-friendly interface.\n\n\nEssential Terminology\nBecoming familiar with version control terminology is half the battle in becoming fluent in Git/GitHub. Study the terms below to become better acquainted and revisit as needed. We‚Äôll refer to these terms often throughout this guide.\n\nRepository == repo: A project that is tracked via git/GitHub\n\nRemote repo: A git project that is stored on GitHub\nLocal repo: A git project that has been downloaded to your local machine\n\nClone: Cloning is the process of making a copy of a remote repo on your local machine. This allows you to work on the project locally and perform tasks like commits, branches, and pulls.\nCommit: A git command that marks the completion of new work to a repo (e.g., add a new script, add a feature, fill out README). You can always recover previous versions of your work by loading up a previous commit.\nPush: A git command that sends local changes (commits) stored in your local repo to the remote repo.\nPull: A git command that allows you to update your local repo based on changes made to the remote repo (e.g., if your colleague pushes to the remote repo)\nBranch: A branch in Git is a parallel line of development that allows you to work on features, bug fixes, or experiments without affecting the main codebase. You can create and switch between branches to isolate your work.\nMerge: Merging is the process of integrating changes from one branch into another. This is typically done to combine the changes made in a** feature branch** with the main branch (e.g., main or master).\nPull Request (PR): A pull request is a feature provided by platforms like GitHub, GitLab, and Bitbucket. It‚Äôs a way to propose changes (commits) to a project. Others can review the changes, and once approved, they can be merged into the main branch.\nFork: Forking a repository means creating a copy of someone else‚Äôs project in your GitHub account. This allows you to make changes independently and propose those changes back to the original project via pull requests. If everyone on your team has write-access to the repo, it‚Äôs best to use new branches instead of forks for pull requests.\nGitignore: A .gitignore file is used to specify which files and directories should be excluded from version control. It‚Äôs essential for preventing unnecessary or sensitive files (contains like API keys) from being included in the repository."
  },
  {
    "objectID": "Resources/Guides/Github-desktop.html#setup",
    "href": "Resources/Guides/Github-desktop.html#setup",
    "title": "Version Control with GitHub Desktop",
    "section": "Setup",
    "text": "Setup\nIn this section, we will walk you through setting up a GitHub repository for a collaborative software project. As an example case, imagine you and your team are preparing for a Kaggle competition and need a streamlined way to manage your code, track changes, and collaborate efficiently. By following the steps below, you‚Äôll learn how to create a new repository, add collaborators, set up secure access, and clone the repository to your local machine, ensuring everyone on your team is ready to contribute seamlessly.\n\nInstall GitHub Desktop\n\nVisit https://desktop.github.com/ to install\n\nCreate repo\n\nVisit https://github.com/ and sign in to your GitHub account (or create an account)\nClick the green ‚Äúnew‚Äù button to create a new repo\nProvide a name for the project, e.g., ‚Äúmy_kaggle_project‚Äù\nGive a description: ‚ÄúGit repo for collaborating on Kaggle project for MLM23.‚Äù\nSet to private if you‚Äôre worried about having your work scooped. Otherwise set to public.\nAdd a README file: best practice is to include a README file that explains how to use your code/repo\nChoose a license: https://choosealicense.com/. MIT license is usually best for open-source projects.\n\nAdd collaborator(s)\n\nFrom your repo homepage on GitHub, click the settings tab\nClick on the ‚ÄúCollaborators‚Äù menu option shown in the left panel\nClick ‚ÄúAdd people‚Äù and enter your collaborator‚Äôs username or GitHub email address\n\nSetup SSH key: SSH provides a secure way to authenticate and transfer data between your local machine and GitHub. You can also use HTTPS if you prefer, but it is less secure. HTTPS avoids having to generate an SSH key, but you will need to enter your GitHub login credentials from time to time.\n\nOpen GitBash (windows) or terminal (Mac) and run the following commands replacing the example email with your GitHub email:\n\nssh-keygen -t ed25519 -C ‚Äúyour_github_email@address.com‚Äù\ncat ~/.ssh/id_ed25519.pub\nThe ssh-keygen produces private and public keys, and make sure to copy and paste the output from the command\ncat ~/.ssh/id_ed25519.pub\n\nPaste output (starts like ssh-ed25519) into the new SSH key under GitHub settings (SSH and GPG Keys) and save the key\n\nClone repo\n\nFrom your GitHub repo homepage, click the green ‚ÄúCode‚Äù button\nSelect SSH if you setup an SSH key or select HTTPS if you don‚Äôt have one setup. Copy the URL shown.\nOpen GitHub Desktop\nClick File ‚Üí Clone repository ‚Üí URL\nPaste the repo URL and pay attention to the destination folder path so you can access this folder later\nClick ‚ÄúClone‚Äù"
  },
  {
    "objectID": "Resources/Guides/Github-desktop.html#tracking-changes",
    "href": "Resources/Guides/Github-desktop.html#tracking-changes",
    "title": "Version Control with GitHub Desktop",
    "section": "Tracking changes",
    "text": "Tracking changes\nNow that you have set up your collaborative Kaggle hackathon repository, it‚Äôs time to start working on your project and track the changes you make. In this section, we will guide you through the process of adding files to your local repository, viewing and committing changes, and pushing those changes to the remote repository on GitHub. By understanding how to track changes effectively, you and your team can ensure that all contributions are recorded, reviewed, and integrated smoothly into the project.\n\nAdd a blank text file to your local repo\n\nRight-click repo name in GitHub Desktop ‚Üí show in explorer (show in Finder and go to the directory on Mac)\nCreate a new text file and add to local repo folder\nAdd a line of text to the file, e.g., ‚Äúhello world‚Äù and save the file\n\nView local changes\n\nIn GitHub desktop, you can view this change under the ‚ÄúChanges‚Äù tab. Notice that we see the new file and added text under this tab.\n\nCommit the new file\n\nCommits mark a checkpoint in the progress you have made to your repo. Provide a short summary message and optionally provide more information in the ‚ÄúDescription‚Äù box.\n\nView remote changes (or lack thereof)\n\nVisit GitHub and notice that the change is not yet reflected on GitHub\n\nPush the change to GitHub \nView remote changes\n\nVisit GitHub again and notice the change has now been transferred to GitHub. Your collaborators can now access your changes through the remote repo (the repo stored on GitHub)"
  },
  {
    "objectID": "Resources/Guides/Github-desktop.html#ignoring-.ipynb-files",
    "href": "Resources/Guides/Github-desktop.html#ignoring-.ipynb-files",
    "title": "Version Control with GitHub Desktop",
    "section": "Ignoring .ipynb files",
    "text": "Ignoring .ipynb files\nAs you collaborate on your Kaggle hackathon project, you may encounter challenges with tracking changes in Jupyter notebooks (.ipynb files) due to their complex JSON format. These files can include a lot of metadata that makes version control difficult and cluttered. In this section, we‚Äôll show you how to use Jupytext to convert your Jupyter notebooks into a more manageable format and configure your repository to ignore .ipynb files. This approach will help you maintain a cleaner version history and focus on the actual code changes, making collaboration more efficient.\n\nAdd jupyter lab file to repo\n\nOpen anaconda prompt and cd into your local repo folder\nrun ‚Äújupyter lab‚Äù command to start a new jupyter lab instance\ncreate a new notebook, e.g., preprocess_data.ipynb\nadd a line of code, e.g., print(‚Äòhello world‚Äô)\nsave the notebook and open GitHub desktop\n\nIn GitHub desktop, notice the changes being tracked are wildly confusing. \n\nJupyter files are stored in JSON format which includes a lot of metadata unrelated to the changes you made to your file. The solution? Use Jupytext!\n\nInstall jupytext\n\npip install jupytext\njupytext ‚Äìset-formats ipynb,py *.ipynb # convert .ipynb files to .py\njupytext ‚Äìset-formats py,ipynb *.py\n\nalternatively to convert just one specific file: jupytext ‚Äìset-formats ipynb,py file_name.ipynb\n\n\ngit ignore .ipynb files\n\nright click one of the .ipynb files in GitHub Desktop\nignore all files of this type\n\ncommit changes\npush and view changes on GitHub"
  },
  {
    "objectID": "Resources/Guides/Github-desktop.html#pulling-updates-from-github",
    "href": "Resources/Guides/Github-desktop.html#pulling-updates-from-github",
    "title": "Version Control with GitHub Desktop",
    "section": "Pulling updates from GitHub",
    "text": "Pulling updates from GitHub\nAs your team collaborates on the Kaggle hackathon project, it‚Äôs essential to stay up-to-date with the latest changes made by your teammates. In this section, we‚Äôll explain how to pull updates from the remote repository on GitHub to your local machine. This process ensures that you always have the most recent version of the project and can integrate your work with the contributions of others seamlessly. By regularly pulling updates, you can avoid conflicts and ensure smooth collaboration throughout the project.\n\nPretend you are a collaborator and visit GitHub to find your repo\nAdd a new file to the remote repo (the version stored on GitHub): Add file ‚Üí create new file.\nCommit the file to the repo\nOpen your local repo folder and notice we don‚Äôt have this new file yet\nIn GitHub Desktop, click ‚ÄúFetch origin‚Äù by ‚ÄúPull origin‚Äù\n\nFetch origin will run and inform you of any changes made to the remote copy of the repo (the one stored on GitHub)\nIf changes have been made since you last pulled, you‚Äôll see the Fetch button turn into a ‚ÄúPull‚Äù option. Click this option to retrieve any updates from GitHub and pull them into the local version of your repo.\n\nCheck your local repo folder to verify the new file has been pulled from GitHub onto your machine"
  },
  {
    "objectID": "Resources/Guides/Github-desktop.html#reverting-to-a-previous-commit",
    "href": "Resources/Guides/Github-desktop.html#reverting-to-a-previous-commit",
    "title": "Version Control with GitHub Desktop",
    "section": "Reverting to a previous commit",
    "text": "Reverting to a previous commit\nDuring the course of your Kaggle hackathon project, there may be times when you need to revert to a previous version of your code. This could be due to a bug, an unwanted change, or simply the need to return to a stable state. In this section, we‚Äôll guide you through the process of reverting to a previous commit using GitHub Desktop. Understanding how to revert to an earlier commit ensures that you can quickly and safely undo changes, helping your team maintain a stable and functional codebase throughout the competition.\n\nFind the Commit to Revert To\n\nOpen GitHub Desktop and navigate to the repository you are working on.\nClick on the ‚ÄúHistory‚Äù tab to view the commit history of your repository.\nScroll through the list of commits and locate the commit you want to revert to. Click on the specific commit to select it.\n\nCreate a New Branch from the Selected Commit\n\nWith the desired commit selected, click on the ‚ÄúBranch‚Äù menu at the top of GitHub Desktop.\nSelect ‚ÄúNew Branch‚Äù from the dropdown menu.\nIn the dialog box that appears, name your new branch (e.g., ‚Äúrevert-to-commit‚Äù) and ensure that it is based on the selected commit. Click ‚ÄúCreate Branch‚Äù to proceed.\n\nMake Necessary Changes in the New Branch\n\nSwitch to the newly created branch by clicking on the ‚ÄúCurrent Branch‚Äù dropdown menu and selecting your new branch.\nMake any necessary changes in this branch to resolve issues or implement desired modifications.\nUse your code editor or IDE to make and save these changes.\n\nCommit the Changes to the New Branch\n\nReturn to GitHub Desktop and navigate to the ‚ÄúChanges‚Äù tab.\nReview the changes you made and provide a commit message summarizing them.\nClick the ‚ÄúCommit to ‚Äù button to commit these changes to the new branch.\n\nPush the Changes to GitHub\n\nClick the ‚ÄúPublish branch‚Äù button in GitHub Desktop to push your changes to the remote repository on GitHub.\nWait for the push process to complete.\n\nCreate a Pull Request (Optional)\n\nIf you want to merge these changes back into the main branch, go to your repository on GitHub.\nClick on the ‚ÄúPull requests‚Äù tab and then click the ‚ÄúNew pull request‚Äù button.\nSelect your new branch as the source and the main branch as the destination. Review the changes and click ‚ÄúCreate pull request.‚Äù\n\nReview and Merge (If Using a Pull Request)\n\nReviewers can now examine the pull request on GitHub. They can leave comments, request changes, or approve the pull request.\nOnce the changes are approved, the pull request can be merged into the main branch by clicking the ‚ÄúMerge pull request‚Äù button on GitHub."
  },
  {
    "objectID": "Resources/Guides/Github-desktop.html#using-pull-requests-to-review-each-others-work",
    "href": "Resources/Guides/Github-desktop.html#using-pull-requests-to-review-each-others-work",
    "title": "Version Control with GitHub Desktop",
    "section": "Using ‚Äúpull requests‚Äù to review each other‚Äôs‚Äô work",
    "text": "Using ‚Äúpull requests‚Äù to review each other‚Äôs‚Äô work\nPull requests are an essential feature of GitHub that facilitate collaborative development by allowing team members to propose changes to a codebase. They provide a structured way for team members to review, discuss, and approve changes before they are merged into the main branch. In this section, we will explore how to use pull requests effectively to ensure that your team‚Äôs work is consistently high-quality and integrated smoothly. By following best practices for creating, reviewing, and merging pull requests, you can maintain a clean and stable codebase while fostering a collaborative and transparent development process. These instructions are clear and structured well, but a few refinements can enhance clarity and flow. Here‚Äôs an improved version:\n\nCreate a New Branch:\n\nOpen GitHub Desktop and select your repository.\nClick the ‚ÄúCurrent Branch‚Äù dropdown.\nSelect ‚ÄúNew Branch‚Äù and give it a descriptive name (e.g., ‚Äúfeature-branch‚Äù or ‚Äúcollaborator-feature‚Äù).\nChoose the base branch, typically the default branch like main or master, and click ‚ÄúCreate Branch.‚Äù\n\nMake Changes in the New Branch:\n\nSwitch to the newly created branch by selecting it from the ‚ÄúCurrent Branch‚Äù dropdown.\nCollaborators can now make changes in this new branch. They can create, edit, or delete files as needed.\n\nCommit and Push Changes:\n\nAfter making changes, go to the ‚ÄúChanges‚Äù tab in GitHub Desktop.\nReview the changes, provide a meaningful commit message, and click ‚ÄúCommit to ‚Äù.\nClick ‚ÄúPush origin‚Äù to push the changes to the remote repository on GitHub.\n\nPreview Pull Request:\n\nIn GitHub Desktop, click on ‚ÄúBranch‚Äù in the menu bar.\nSelect ‚ÄúCreate Pull Request‚Äù to open a preview. This will show which branch is being merged into the main code base.\n\nCreate Pull Request:\n\nAfter confirming that the preview is correct, click ‚ÄúCreate Pull Request‚Äù.\nGitHub will open in your web browser. Fill out the details for the pull request, including a title and description.\nAssign reviewers (e.g., you and other collaborators) to review the changes, then click ‚ÄúCreate Pull Request.‚Äù\n\nReview and Submit the Pull Request on GitHub:\n\nCollaborators should review the pull request on the GitHub website.\nThey can add comments, suggestions, or request changes directly in the pull request interface.\n\nReview the Pull Request in GitHub Desktop:\n\nReturn to GitHub Desktop to see the newly created pull request listed in the ‚ÄúCurrent Branch‚Äù dropdown.\nClick on the pull request to view the changes, comments, and review the code.\nRespond to any feedback or comments in the GitHub Desktop interface.\n\nAccept or Request Changes:\n\nAfter reviewing the code, you and other collaborators can either accept the pull request if it‚Äôs ready to merge or request changes if there are issues to address.\nLeave comments, suggestions, and feedback in the pull request.\n\nCollaborators Make Changes:\n\nIf changes are requested, collaborators can make the necessary adjustments in their branch and push the updates.\nThe pull request will automatically update with the new commits.\n\nClose the Pull Request:\n\nOnce the pull request is approved and the changes have been successfully reviewed, merge the pull request into the main branch.\nAfter merging, you can delete the branch to keep the repository clean."
  },
  {
    "objectID": "Resources/Guides/Github-desktop.html#questions",
    "href": "Resources/Guides/Github-desktop.html#questions",
    "title": "Version Control with GitHub Desktop",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Resources/Guides/Github-desktop.html#see-also",
    "href": "Resources/Guides/Github-desktop.html#see-also",
    "title": "Version Control with GitHub Desktop",
    "section": "See also",
    "text": "See also\n\nWorkshop: Intro to Version Control with Git: If you‚Äôre curious to learn how to use Git via shell commands (or just want to become more fluent with Git), check out this YouTube playlist from the Data Science Hub!"
  },
  {
    "objectID": "Resources/Guides/CHTC.html",
    "href": "Resources/Guides/CHTC.html",
    "title": "Center for Highthroughput Computing (CHTC)",
    "section": "",
    "text": "Established in 2006, the Center for High Throughput Computing (CHTC) is committed to democratizing access to powerful computing resources across all research domains. High Throughput Computing (HTC) encompasses a set of principles and techniques designed to optimize computing resource utilization towards solving complex problems. When applied to scientific computing, HTC enhances resource efficiency, automation, and accelerates scientific breakthroughs, including those in machine learning.\nAre you a researcher at UW-Madison seeking to extend your computing capabilities beyond local resources, particularly for machine learning tasks? Request an account now to take advantage of the open computing services offered by the CHTC!\n\n\n\nVisit the CHTC Recipes Repository to discover a collection of common CHTC workflows or ‚Äúrecipes‚Äù, including those specifically geared towards machine learning tasks.\n\n\nExplore our collection of templates tailored for high throughput compute (HTC) systems utilizing GPUs, ideal for accelerating machine learning workflows. These templates streamline the process of job submission, maximizing the utilization of GPU resources for your computational tasks in machine learning. Dive into efficient computing with our GPU-based templates available on GitHub: CHTC GPU Templates\n\n\n\nEmpower your machine learning research endeavors with containerization! CHTC‚Äôs guides on Docker and Apptainer for HTC empower researchers to encapsulate their machine learning workflows, dependencies, and environments efficiently. Seamlessly integrate containers into your machine learning computing workflow for enhanced reproducibility and scalability.\n\nDocker Jobs Guide\nApptainer HTC Guide\nPython container\nR container\nPyTorch container\nAlphafold container"
  },
  {
    "objectID": "Resources/Guides/CHTC.html#about-this-resource",
    "href": "Resources/Guides/CHTC.html#about-this-resource",
    "title": "Center for Highthroughput Computing (CHTC)",
    "section": "",
    "text": "Established in 2006, the Center for High Throughput Computing (CHTC) is committed to democratizing access to powerful computing resources across all research domains. High Throughput Computing (HTC) encompasses a set of principles and techniques designed to optimize computing resource utilization towards solving complex problems. When applied to scientific computing, HTC enhances resource efficiency, automation, and accelerates scientific breakthroughs, including those in machine learning.\nAre you a researcher at UW-Madison seeking to extend your computing capabilities beyond local resources, particularly for machine learning tasks? Request an account now to take advantage of the open computing services offered by the CHTC!"
  },
  {
    "objectID": "Resources/Guides/CHTC.html#chtc-recipes",
    "href": "Resources/Guides/CHTC.html#chtc-recipes",
    "title": "Center for Highthroughput Computing (CHTC)",
    "section": "",
    "text": "Visit the CHTC Recipes Repository to discover a collection of common CHTC workflows or ‚Äúrecipes‚Äù, including those specifically geared towards machine learning tasks.\n\n\nExplore our collection of templates tailored for high throughput compute (HTC) systems utilizing GPUs, ideal for accelerating machine learning workflows. These templates streamline the process of job submission, maximizing the utilization of GPU resources for your computational tasks in machine learning. Dive into efficient computing with our GPU-based templates available on GitHub: CHTC GPU Templates\n\n\n\nEmpower your machine learning research endeavors with containerization! CHTC‚Äôs guides on Docker and Apptainer for HTC empower researchers to encapsulate their machine learning workflows, dependencies, and environments efficiently. Seamlessly integrate containers into your machine learning computing workflow for enhanced reproducibility and scalability.\n\nDocker Jobs Guide\nApptainer HTC Guide\nPython container\nR container\nPyTorch container\nAlphafold container"
  },
  {
    "objectID": "In-progress/pretrained-models.html",
    "href": "In-progress/pretrained-models.html",
    "title": "Pretrained models",
    "section": "",
    "text": "TODO"
  },
  {
    "objectID": "Applications/index.html",
    "href": "Applications/index.html",
    "title": "Applications",
    "section": "",
    "text": "Discover a curated collection of blogs, papers, and talks which dive into ML applications and lessons learned by practitioners, including exploratory data analysis or EDA case studies. These case studies demonstrate the domain knowledge needed to explore different data types (time-series, tabular, images, etc.) from various fields.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\nML+X Forum: Advancing Healthcare and Agriculture through Computer Vision\n\n\n1 min\n\n\n\n\n\n\n2024-04-09\n\n\n\n\n\n\n\n\n\n\n\n\nML+X Forum: Exploring Model Sharing in the Age of Foundation Models\n\n\n1 min\n\n\n\n\n\n\n2024-03-12\n\n\n\n\n\n\n\n\n\n\n\n\nML+X Forum: Exploring Science Communication and Drug Synergy Analysis using GPT\n\n\n1 min\n\n\n\n\n\n\n2023-12-12\n\n\n\n\n\n\n\n\n\n\n\n\nML+X Forum: LLMS in Genomic and Health Coaching\n\n\n1 min\n\n\n\n\n\n\n2023-11-07\n\n\n\n\n\n\n\n\n\n\n\n\nML+X Forum: Multimodal Learning\n\n\n1 min\n\n\n\n\n\n\n2023-09-19\n\n\n\n\n\n\n\n\n\n\n\n\nML+X Forum: Navigating Gravitational Waves with AI Insights\n\n\n1 min\n\n\n\n\n\n\n2024-02-13\n\n\n\n\n\n\n\n\n\n\n\n\nML+X Forum: Time-Series Analysis\n\n\n1 min\n\n\n\n\n\n\n2023-10-10\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Applications"
    ]
  },
  {
    "objectID": "Applications/Forums/mlx_2024-02-13.html",
    "href": "Applications/Forums/mlx_2024-02-13.html",
    "title": "ML+X Forum: Navigating Gravitational Waves with AI Insights",
    "section": "",
    "text": "Hosted by UW-Madison‚Äôs ML+X community, each monthly forum highlights two machine learning applications that share a theme followed by communal discussions.\n\nWelcome and small group discussions (discussion activity removed from recording), Chris Endemann\nClassifying gravitational wave modes from core-collapse supernovae, Bella Finkel, 1:34"
  },
  {
    "objectID": "Applications/Forums/index.html",
    "href": "Applications/Forums/index.html",
    "title": "ML+X Forums",
    "section": "",
    "text": "Explore our library of ML+X forum recordings below! Each monthly ML+X forum highlights two ML applications that share a theme followed by communal discussions and project feedback.\n\nWhere: Orchard View Room (rm. 3280), Discovery Building & via Zoom. Join the Google group to receive a calendar invite (with Zoom link) and other updates.\nWhen: Monthly on Tuesdays, 12-1pm CT. Check the ML+X website for the exact schedule each semester.\n\n\nShare Your Work!\nWe encourage anyone who is using ML in their work to present at one of the ML+X forums. If interested, please fill out this brief presenter sign-up form.\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n\nML+X Forum: Advancing Healthcare and Agriculture through Computer Vision\n\n\n\n\n\n\n\n\n\n\n\n2024-04-09\n\n\nMatthew Blomquist, Will de la Bretonne\n\n\n\n\n\n\n\nML+X Forum: Exploring Model Sharing in the Age of Foundation Models\n\n\n\n\n\n\n\n\n\n\n\n2024-03-12\n\n\nChris Endemann, Haotian Liu\n\n\n\n\n\n\n\nML+X Forum: Navigating Gravitational Waves with AI Insights\n\n\n\n\n\n\n\n\n\n\n\n2024-02-13\n\n\nChris Endemann, Bella Finkel\n\n\n\n\n\n\n\nML+X Forum: Exploring Science Communication and Drug Synergy Analysis using GPT\n\n\n\n\n\n\n\n\n\n\n\n2023-12-12\n\n\nBen Rush, Jack Freeman\n\n\n\n\n\n\n\nML+X Forum: LLMS in Genomic and Health Coaching\n\n\n\n\n\n\n\n\n\n\n\n2023-11-07\n\n\nRohan Sonthalia, Michael Roytman\n\n\n\n\n\n\n\nML+X Forum: Time-Series Analysis\n\n\n\n\n\n\n\n\n\n\n\n2023-10-10\n\n\nPeng Jiang, Sourav Pal\n\n\n\n\n\n\n\nML+X Forum: Multimodal Learning\n\n\n\n\n\n\n\n\n\n\n\n2023-09-19\n\n\nDaifeng Wang, Zachary Huemann, Pedro Morgado\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Applications",
      "Forums"
    ]
  },
  {
    "objectID": "Applications/Forums/mlx_2023-11-07.html",
    "href": "Applications/Forums/mlx_2023-11-07.html",
    "title": "ML+X Forum: LLMS in Genomic and Health Coaching",
    "section": "",
    "text": "Hosted by UW-Madison‚Äôs ML+X Community, each monthly forum highlights two machine learning applications that share a theme followed by communal discussions.\n\nClustering of genomic sequences of mycoviruses using deep learning, Rohan Sonthalia\nSpurring self-improvement and intrinsic motivation using LLMs and reinforcement learning, Michael Roytman"
  },
  {
    "objectID": "Applications/Forums/mlx_2023-10-10.html",
    "href": "Applications/Forums/mlx_2023-10-10.html",
    "title": "ML+X Forum: Time-Series Analysis",
    "section": "",
    "text": "Hosted by UW-Madison‚Äôs ML+X Community, each monthly forum highlights two machine learning applications that share a theme followed by communal discussions.\n\nComputational Methods for Comparative Time Clocks in Early Development and Tissue Regeneration, Peng Jiang\nControlled Differential Equations on Long Sequences via Non-standard Wavelets, Sourav Pal"
  },
  {
    "objectID": "Applications/Blogs/index.html",
    "href": "Applications/Blogs/index.html",
    "title": "Blogs",
    "section": "",
    "text": "Share your ML story!\nAre you currently immersed in an exciting ML project? We want to hear about it! Share your insights, challenges, and successes by contributing a blog post to Nexus, the ML+X resource sharing platform.\nWhether you‚Äôre exploring ML applications in biology, engineering, social sciences, or any other field, your unique perspective is invaluable. Showcase your innovation, research, and creativity to inspire others in the ML+X community.\nGet started by posting a brief summary of your blog idea as an Issue on GitHub! The Nexus development team will follow-up with feedback and further instructions.\n\n\nComing soon! Explore ML stories\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Applications",
      "Blogs"
    ]
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "Glossary of ML Terms",
    "section": "",
    "text": "Term\nMeaning\n\n\n\n\nML\nMachine learning\n\n\nEDA\nExploratory data analysis\n\n\nSVM\nSupport vector machines\n\n\nNLP\nNatural language processing\n\n\nPCA\nPrincipal component analysis\n\n\nCNN\nConvolutional neural network",
    "crumbs": [
      "Glossary of ML terms"
    ]
  },
  {
    "objectID": "Contributor-templates/template_external-resource.html",
    "href": "Contributor-templates/template_external-resource.html",
    "title": "Title/Topic of Resource",
    "section": "",
    "text": "Brief description of the resource, including a text embedded link in the first 1-2 sentences. Explain what the resource covers and its relevance. Mention any specific features, strengths, or weaknesses. This section should help potential users understand the value of the resource and what they can expect to learn or achieve by using it.\n\n\n\n\nPrerequisite Resource 1\nPrerequisite Resource 2"
  },
  {
    "objectID": "Contributor-templates/template_external-resource.html#about-this-resource",
    "href": "Contributor-templates/template_external-resource.html#about-this-resource",
    "title": "Title/Topic of Resource",
    "section": "",
    "text": "Brief description of the resource, including a text embedded link in the first 1-2 sentences. Explain what the resource covers and its relevance. Mention any specific features, strengths, or weaknesses. This section should help potential users understand the value of the resource and what they can expect to learn or achieve by using it.\n\n\n\n\nPrerequisite Resource 1\nPrerequisite Resource 2"
  },
  {
    "objectID": "Contributor-templates/template_external-resource.html#questions",
    "href": "Contributor-templates/template_external-resource.html#questions",
    "title": "Title/Topic of Resource",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Contributor-templates/template_external-resource.html#see-also",
    "href": "Contributor-templates/template_external-resource.html#see-also",
    "title": "Title/Topic of Resource",
    "section": "See also",
    "text": "See also\n\n\nRelated Resource 1: Brief description of related resource 1.\nRelated Resource 2: Brief description of related resource 2.\nRelated Resource 3: Brief description of related resource 3."
  },
  {
    "objectID": "Local-events/Workshops/index.html",
    "href": "Local-events/Workshops/index.html",
    "title": "Local Workshops",
    "section": "",
    "text": "Please see below for workshops taught locally at UW-Madison. To stay up-to-date on any new workshop offerings, we recommend subscribing to the Data Science @ UW newsletter.\nMost workshops listed have open source lesson materials which you are encouraged to independently study. If you are involved with a research lab on campus, you may attend Coding Meetup (Tue/Thur, 2:30-4:30pm) to get help working through the materials.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          offered\n        \n     \n  \n\n\n\n    \n        \n            \n                Intro to High-Dimensional Data Analysis\n                \n                    \n                    ML\n                    \n                    Regression\n                    \n                    High-Dimensional\n                    \n                \n                \n            \n                \n                    Organized by Data Science Hub, UW-Madison Carpentries\n                \n                \n                    Offered October 2024 |\n                \n                    \n                        Lesson Materials\n                     |\n                    \n                        Registration\n                    \n                \n                    12-16 hours to complete\n                \n            \n            \n            \n                Languages and Tools:\n                \n                Python\n                \n                Jupyter\n                \n            \n            \n            \n            \n                Packages and Libraries:\n                \n                sklearn\n                \n                pandas\n                \n                statsmodels\n                \n            \n            \n        \n        \n        \n            \n                Prerequisites\n                Introductory Python programming skills (variable assignments, how to create a function, for loops, etc.) and\nfamiliarity with the Pandas package.\nIf you need a refresher on Python before taking this workshop,\nplease review the lesson materials from this\n[Introductory Python Carpentries](https://swcarpentry.github.io/python-novice-inflammation/index.html) workshop.  \n\nFamiliarity with basic machine learning concepts including train/test splits and overfitting.\nFor a refresher on machine learning basics, please review the lesson materials from\nthe [Intro to Machine Learning with Sklearn](https://carpentries-incubator.github.io/machine-learning-novice-sklearn/)\nworkshop.\n\n            \n        \n        \n    \n        \n            \n                Intro to Deep Learning with Keras\n                \n                    \n                    ML\n                    \n                    Deep Learning\n                    \n                    Keras\n                    \n                \n                \n            \n                \n                    Organized by Data Science Hub, UW-Madison Carpentries\n                \n                \n                    Offered May 2024 |\n                \n                    \n                        Lesson Materials\n                     |\n                    \n                        Registration\n                    \n                \n                    12 hours to complete\n                \n            \n            \n            \n                Languages and Tools:\n                \n                Python\n                \n                Jupyter\n                \n            \n            \n            \n            \n                Packages and Libraries:\n                \n                keras\n                \n            \n            \n        \n        \n        \n            \n                Prerequisites\n                Basic Python programming skills and familiarity with the Pandas package. \n\nBasic knowledge on Machine learning, including the following concepts:\nData cleaning, train & test split, type of problems (regression, classification), overfitting & underfitting, metrics (accuracy, recall, etc.).\n\n            \n        \n        \n    \n        \n            \n                Intro to Text Analysis / Natural Language Processing\n                \n                    \n                    ML\n                    \n                    Deep Learning\n                    \n                    Large Language Models\n                    \n                    Text Embedding\n                    \n                    NLP\n                    \n                    Topic Modeling\n                    \n                \n                \n            \n                \n                    Organized by Data Science Hub, UW-Madison Carpentries\n                \n                \n                    Offered April 2024 |\n                \n                    \n                        Lesson Materials\n                     |\n                    \n                        Registration\n                    \n                \n                    16 hours to complete\n                \n            \n            \n            \n                Languages and Tools:\n                \n                Python\n                \n                Jupyter\n                \n            \n            \n            \n            \n                Packages and Libraries:\n                \n                gensim\n                \n                transformers\n                \n                sklearn\n                \n                pandas\n                \n            \n            \n        \n        \n        \n            \n                Prerequisites\n                Basic Python programming skills and familiarity with the Pandas package. \n\n            \n        \n        \n    \n        \n            \n                How to Analyze Data Using Python\n                \n                    \n                \n                \n            \n                \n                \n                    Offered March 2024 |\n                \n                    \n                        Lesson Materials\n                     |\n                    \n                        Registration\n                    \n                \n            \n            \n            \n        \n        \n    \n        \n            \n                How to Analyze Data Using R\n                \n                    \n                \n                \n            \n                \n                \n                    Offered March 2024 |\n                \n                    \n                        Lesson Materials\n                     |\n                    \n                        Registration\n                    \n                \n            \n            \n            \n        \n        \n    \n        \n            \n                Intro to Machine Learning with Scikit Learn\n                \n                    \n                    ML\n                    \n                \n                \n            \n                \n                    Organized by Data Science Hub, UW-Madison Carpentries\n                \n                \n                    Offered September 2023 |\n                \n                    \n                        Lesson Materials\n                     |\n                    \n                        Registration\n                    \n                \n                    8 hours to complete\n                \n            \n            \n            \n                Languages and Tools:\n                \n                Python\n                \n                Jupyter\n                \n            \n            \n            \n            \n                Packages and Libraries:\n                \n                sklearn\n                \n                pandas\n                \n            \n            \n        \n        \n        \n            \n                Prerequisites\n                A basic understanding of Python. You will need to know how to write a for loop, if statement, use functions, libraries and perform basic arithmetic. Either of the Software Carpentry Python courses cover sufficient background (e.g., https://swcarpentry.github.io/python-novice-inflammation/index.html)\n\n            \n        \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "Applications/Forums/mlx_2024-03-12.html",
    "href": "Applications/Forums/mlx_2024-03-12.html",
    "title": "ML+X Forum: Exploring Model Sharing in the Age of Foundation Models",
    "section": "",
    "text": "Hosted by UW-Madison‚Äôs ML+X Community, each monthly forum highlights two machine learning applications that share a theme followed by communal discussions.\n\nModel sharing and reproducible ML, Chris Endemann, 0:29\nLLaVA-NeXT and model sharing, Haotian Liu, 12:24"
  },
  {
    "objectID": "Applications/Forums/mlx_2024-04-09.html",
    "href": "Applications/Forums/mlx_2024-04-09.html",
    "title": "ML+X Forum: Advancing Healthcare and Agriculture through Computer Vision",
    "section": "",
    "text": "Hosted by UW-Madison‚Äôs ML+X Community, each monthly forum highlights two machine learning applications that share a theme followed by communal discussions."
  },
  {
    "objectID": "Applications/Forums/mlx_2023-12-12.html",
    "href": "Applications/Forums/mlx_2023-12-12.html",
    "title": "ML+X Forum: Exploring Science Communication and Drug Synergy Analysis using GPT",
    "section": "",
    "text": "Hosted by UW-Madison‚Äôs ML+X community, each monthly forum highlights two machine learning applications that share a theme followed by communal discussions.\n\nGPT for Science Communication: User-Interface and Developer Pipeline Approaches, Ben Rush, 0:10\nAdvancing Biomedical Research with GPT-4: A Novel Approach to Drug Synergy Analysis using Text Mining and Classification, Jack Freeman, 27:58"
  },
  {
    "objectID": "Applications/Forums/mlx_2023-09-19.html",
    "href": "Applications/Forums/mlx_2023-09-19.html",
    "title": "ML+X Forum: Multimodal Learning",
    "section": "",
    "text": "Hosted by UW-Madison‚Äôs ML+X Community, each monthly forum highlights 2-3 machine learning applications that share a theme followed by communal discussions.\n\nMultimodal learning and analysis for understanding single-cell functional genomics in brains and brain diseases, Daifeng Wang\nTransforming healthcare: AI-enhanced disease quantification with vision-language models, Zachary Huemann\nThe benefits of early fusion: deeply integrated audio-visual representation learning, Pedro Morgado"
  },
  {
    "objectID": "Applications/EDA-examples/index.html",
    "href": "Applications/EDA-examples/index.html",
    "title": "Exploratory analysis",
    "section": "",
    "text": "Share your EDA methods!\nAre you passionate about data and keen to share your insights? We invite you to contribute to our exploratory data analysis (EDA) case studies on Nexus.\nWhether you‚Äôre working with biological data, engineering datasets, social sciences information, or any other domain, your contributions can help elevate the community‚Äôs understanding and application of EDA techniques.\nGet started by posting a brief summary of your EDA case study idea as an Issue on GitHub! The Nexus development team will follow up with feedback and further instructions.\n\n\nComing soon! Explore EDA case studies\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Applications",
      "EDA"
    ]
  },
  {
    "objectID": "In-progress/Resources-drafts/Model-sharing.html",
    "href": "In-progress/Resources-drafts/Model-sharing.html",
    "title": "Model Sharing via Hugging Face",
    "section": "",
    "text": "Model Hub: https://huggingface.co/models\nNavigating the Model Hub: https://huggingface.co/docs/hub/en/models-the-hub\nModel cards\n\nhttps://huggingface.co/docs/hub/en/model-cards\nhttps://huggingface.co/blog/model-cards\n\nUploading a model to Hugging Face: https://huggingface.co/docs/hub/en/models-uploading\n\n\n\n\n\nNavigating the Model Hub: https://huggingface.co/docs/hub/en/models-the-hub\nModels are uploaded with tags associated with\n\ntask\nlibrary (PyTorch, Tensorflow, etc.)\ntraining data\nlanguage (english, spanish, etc.)\nlicense\n\nClicking a model will show you its model card\n\n\n\n\n\nSee: https://huggingface.co/docs/hub/en/model-cards and https://huggingface.co/blog/model-cards\nThe information included in any given model cards is, unfortunately, not well standardized. However, it is best practice to include at least the following pieces of information\n\nInfo on base model (e.g., if your model is finetuned)\nInfo on common model variations\n\nHelp others understand your model within the context of recent developments\n\nModel description\n\ngeneral purpose\narchitecture\n\nInfo on training data\n\nHow the data was collected\nData license and usage terms\nBasic descriptive statistics: number of samples, features, classes, etc. Describe and/or visualize data distribution.\nNote any class imbalance issues. Include assessments of bias and fairness, when possible.\n\nExample: The training dataset used for the facial recognition model might unintentionally be biased, containing predominantly images of people from certain demographic groups (e.g., predominantly light-skinned individuals).\n\n\nThe model‚Äôs evaluation results\nIntended uses and limitations\n\nThis is critical for all models, but especially models that can have a downstream impact on people.\nExample: If the model is not designed to handle diverse demographics, it may learn patterns that favor certain groups while performing poorly on others. In this case, the model may have higher accuracy for light-skinned individuals but lower accuracy for darker-skinned individuals.\n\n\n\n\n\n\n\nUploading a model to Hugging Face: https://huggingface.co/docs/hub/en/models-uploading\nAll models are stored as GitHub repositories\nSome frameworks (e.g., PyTorch) have integrations that make it straightforward to upload a model within your model training script\nYou get to decide what information and metadata you want to include in the GitHub repository\n\n\n\n\n\nIs there any way to help control the overall coding environment when sharing a model? For instance, if my goal is to reproduce paper results exactly rather than repurpose a model.\nHow documented should my data be within my model card? Should I save the details for a separate ‚Äúdata card‚Äù?\nIf you already have a convenient way to download and load models stored elsewhere (e.g.¬†Zenodo), how much additional benefit is there to refactoring your code to use Hugging Face to host the models?\nWhat is involved in running an interactive demo on the Hugging Face website with your model?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "In-progress/Resources-drafts/Model-sharing.html#links",
    "href": "In-progress/Resources-drafts/Model-sharing.html#links",
    "title": "Model Sharing via Hugging Face",
    "section": "",
    "text": "Model Hub: https://huggingface.co/models\nNavigating the Model Hub: https://huggingface.co/docs/hub/en/models-the-hub\nModel cards\n\nhttps://huggingface.co/docs/hub/en/model-cards\nhttps://huggingface.co/blog/model-cards\n\nUploading a model to Hugging Face: https://huggingface.co/docs/hub/en/models-uploading"
  },
  {
    "objectID": "In-progress/Resources-drafts/Model-sharing.html#navigating-model-hub",
    "href": "In-progress/Resources-drafts/Model-sharing.html#navigating-model-hub",
    "title": "Model Sharing via Hugging Face",
    "section": "",
    "text": "Navigating the Model Hub: https://huggingface.co/docs/hub/en/models-the-hub\nModels are uploaded with tags associated with\n\ntask\nlibrary (PyTorch, Tensorflow, etc.)\ntraining data\nlanguage (english, spanish, etc.)\nlicense\n\nClicking a model will show you its model card"
  },
  {
    "objectID": "In-progress/Resources-drafts/Model-sharing.html#model-cards",
    "href": "In-progress/Resources-drafts/Model-sharing.html#model-cards",
    "title": "Model Sharing via Hugging Face",
    "section": "",
    "text": "See: https://huggingface.co/docs/hub/en/model-cards and https://huggingface.co/blog/model-cards\nThe information included in any given model cards is, unfortunately, not well standardized. However, it is best practice to include at least the following pieces of information\n\nInfo on base model (e.g., if your model is finetuned)\nInfo on common model variations\n\nHelp others understand your model within the context of recent developments\n\nModel description\n\ngeneral purpose\narchitecture\n\nInfo on training data\n\nHow the data was collected\nData license and usage terms\nBasic descriptive statistics: number of samples, features, classes, etc. Describe and/or visualize data distribution.\nNote any class imbalance issues. Include assessments of bias and fairness, when possible.\n\nExample: The training dataset used for the facial recognition model might unintentionally be biased, containing predominantly images of people from certain demographic groups (e.g., predominantly light-skinned individuals).\n\n\nThe model‚Äôs evaluation results\nIntended uses and limitations\n\nThis is critical for all models, but especially models that can have a downstream impact on people.\nExample: If the model is not designed to handle diverse demographics, it may learn patterns that favor certain groups while performing poorly on others. In this case, the model may have higher accuracy for light-skinned individuals but lower accuracy for darker-skinned individuals."
  },
  {
    "objectID": "In-progress/Resources-drafts/Model-sharing.html#uploading-a-model",
    "href": "In-progress/Resources-drafts/Model-sharing.html#uploading-a-model",
    "title": "Model Sharing via Hugging Face",
    "section": "",
    "text": "Uploading a model to Hugging Face: https://huggingface.co/docs/hub/en/models-uploading\nAll models are stored as GitHub repositories\nSome frameworks (e.g., PyTorch) have integrations that make it straightforward to upload a model within your model training script\nYou get to decide what information and metadata you want to include in the GitHub repository"
  },
  {
    "objectID": "In-progress/Resources-drafts/Model-sharing.html#open-questions",
    "href": "In-progress/Resources-drafts/Model-sharing.html#open-questions",
    "title": "Model Sharing via Hugging Face",
    "section": "",
    "text": "Is there any way to help control the overall coding environment when sharing a model? For instance, if my goal is to reproduce paper results exactly rather than repurpose a model.\nHow documented should my data be within my model card? Should I save the details for a separate ‚Äúdata card‚Äù?\nIf you already have a convenient way to download and load models stored elsewhere (e.g.¬†Zenodo), how much additional benefit is there to refactoring your code to use Hugging Face to host the models?\nWhat is involved in running an interactive demo on the Hugging Face website with your model?"
  },
  {
    "objectID": "In-progress/Resources-drafts/Model-sharing.html#questions",
    "href": "In-progress/Resources-drafts/Model-sharing.html#questions",
    "title": "Model Sharing via Hugging Face",
    "section": "",
    "text": "If you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Resources/Guides/How-to-contribute.html",
    "href": "Resources/Guides/How-to-contribute.html",
    "title": "How to Contribute?",
    "section": "",
    "text": "We want Nexus to serve also as a place where members of the community can share their knowledge. This guide answers the question, how to contribute to a useful resource to Nexus?",
    "crumbs": [
      "How to contribute?"
    ]
  },
  {
    "objectID": "Resources/Guides/How-to-contribute.html#what-kinds-of-resources-are-hosted-on-nexus",
    "href": "Resources/Guides/How-to-contribute.html#what-kinds-of-resources-are-hosted-on-nexus",
    "title": "How to Contribute?",
    "section": "What kinds of resources are hosted on Nexus?",
    "text": "What kinds of resources are hosted on Nexus?\nAny content (original or external) that can help make the practice of ML more connected, accessible, efficient, and reproducible is welcome on the Nexus platform! This includes, but is not limited to:\n\nüß† Educational materials: Explore a library of educational materials (workshops, guides, books, videos, etc.) covering a wide range of ML-related topics, tools, and workflows, from foundational concepts to advanced techniques. These materials offer clear explanations, practical examples, and actionable insights to help you navigate the complexities of ML with confidence.\nüß¨ Applications & stories: Discover a curated collection of blogs, papers, and talks which dive into ML applications and lessons learned by practitioners, including exploratory data analysis or EDA case studies. These case studies demonstrate the domain knowledge needed to explore different data types (time-series, tabular, images, etc.) from various fields.\nüõ† Coming soon! Models & data: Learn about useful pretrained models, foundation models, and datasets that you can leverage for your next ML project. Learn about their features, how to use them effectively, and see examples of them in action.",
    "crumbs": [
      "How to contribute?"
    ]
  },
  {
    "objectID": "Resources/Guides/How-to-contribute.html#need-inspiration-for-a-good-topic-to-post-about",
    "href": "Resources/Guides/How-to-contribute.html#need-inspiration-for-a-good-topic-to-post-about",
    "title": "How to Contribute?",
    "section": "Need inspiration for a good topic to post about?",
    "text": "Need inspiration for a good topic to post about?\nAn ever-expanding list of requested resources can be found on the Issues page (on GitHub). Search for open issues that have the ‚ÄúResource‚Äù label to check out some of our top priorities. If you‚Äôd like to tackle a given issue, please comment on the issue to let others know.",
    "crumbs": [
      "How to contribute?"
    ]
  },
  {
    "objectID": "Resources/Guides/How-to-contribute.html#what-makes-a-good-post",
    "href": "Resources/Guides/How-to-contribute.html#what-makes-a-good-post",
    "title": "How to Contribute?",
    "section": "What makes a good post?",
    "text": "What makes a good post?\nCreating a useful and engaging post for the ML+X Nexus involves a few key elements to ensure it is beneficial for the community. Here are some general guidelines to follow:\n\nClear and concise title\nThe title should accurately reflect the content and main focus of the post. It should be engaging and specific, allowing readers to quickly understand what they can expect.\n\n\nDetailed description\nProvide a comprehensive overview of the resource or topic. This should include:\n\nPurpose and scope: Clearly state what the resource covers and its main objectives. Explain why the content is valuable and how it can help practitioners.\nKey features: Highlight the unique aspects or strengths of the resource. This could include practical examples, interactive elements, or real-world applications.\nStrengths and weaknesses: Provide a balanced view by discussing both the strengths and any potential limitations of the resource. This helps users make informed decisions about whether the resource is right for them.\n\n\n\nPrerequisites\nList any necessary background knowledge or skills required to fully benefit from the resource. This helps set expectations and ensures that users are adequately prepared. Include links to additional resources or tutorials that can help users gain the required knowledge.\n\n\nEstimated time to complete\nOffer an estimate of the time commitment needed to complete the resource. This helps users plan their learning activities and manage their time effectively.\n\n\nAccessibility and usability\nEnsure the resource is easy to access and use. We want the majority of resources on Nexus to be free and open source (with possibly a few rare exceptions for tools/resources in high-demand). Provide clear instructions on how to navigate and utilize the content. If the resource is hosted externally, include a direct link and any necessary login or access information.\n\n\nAdditional related resources\nInclude links to related materials or further readings that can enhance the user‚Äôs understanding and provide more in-depth knowledge on the topic. This can include books, articles, other workshops, or case studies. When possible, link to any relevant materials which are already hosted on the Nexus platform.\n\n\nExample of a good posts\nExternal recommendations\n\nWorkshop\nBook\nVideo\n\nOriginal content\n\nGuide",
    "crumbs": [
      "How to contribute?"
    ]
  },
  {
    "objectID": "Resources/Guides/How-to-contribute.html#how-to-make-a-new-post-with-github",
    "href": "Resources/Guides/How-to-contribute.html#how-to-make-a-new-post-with-github",
    "title": "How to Contribute?",
    "section": "How to make a new post with GitHub?",
    "text": "How to make a new post with GitHub?\n\nGitHub collaboration model\nWe follow GitHub‚Äôs collaboration model, so the general idea to make a post or edit a document is the same. The high-level steps include:\n\nCreate an issue announcing your plan to add a resource ‚Äî see ML+X Nexus Issues\nFork the ML+X-Nexus repository\nClone the forked repository onto your local machine\nCreate a new branch\nWrite the post, commit and push the changes, and make a pull request\n\nIf you don‚Äôt know how to use Git / GitHub already, it can be a little intimidating at first. A friendlier alternative could be to download GitHub desktop and add your post using the instructions provided below. If you‚Äôd like to learn more about GitHub Desktop, check out the Version Control with GitHub Desktop guide on Nexus. If you need additional help (and work in a research lab at UW-Madison), you may also seek help at the Data Science Hub‚Äôs office hours (‚ÄúCoding Meetup‚Äù).\n\n1. Get Started with GitHub Desktop\n\nDownload GitHub Desktop: Go to the GitHub Desktop website and download the application for your operating system. Install GitHub Desktop by following the on-screen instructions.\nOpen GitHub Desktop: Sign in with your GitHub account. If you don‚Äôt have one, you will need to create your GitHub account first.\n\n\n\n2. Create an Issue on Nexus GitHub\nBefore you start writing your content, create an issue on the Nexus GitHub to announce your intended addition. This helps the Nexus development team keep track of new contributions and provides an opportunity for feedback.\n\nGo to the Nexus GitHub Issues page\nClick on the New Issue button.\nTitle the issue starting with ‚Äú[Resource]‚Äù and add a ‚ÄúResource‚Äù tag.\nDescribe why you think this resource (or original content) should be included on the Nexus platform.\nWait for feedback: Wait for one of the Nexus developers to provide feedback or comments on your issue before proceeding.\n\n\n\n3. Fork the Repository\n\nGo to the ML+X-Nexus repository on GitHub.\nClick the Fork button at the top-right corner of the page. This will create a copy of the repository under your GitHub account.\n\n\n\n4. Clone the Repository to Your System to Your Local System\n\nFrom your new forked version of the repo on GitHub, click the green Code button to copy the HTTPS URL of the fork\nIn GitHub Desktop, click on File &gt; Clone Repository.\nPaste the URL and click Clone\n\n\n\n5. Create a New Branch\n\nIn GitHub Desktop, click on Branch &gt; New Branch.\nName your new branch descriptively based on the resource type and name (e.g., workshop-introDL, video-NeurIPS2024, etc.).\n\n\n\n6. Write Your Post\n\nOpen your favorite text editor or IDE (e.g., Visual Studio Code, Sublime Text).\nWrite your post in the appropriate format. Follow the guidelines in the next section for writing a good post, making use the template file provided.\n\n\n\n7. Commit and Push Changes\n\nIn GitHub Desktop, you should see your changes listed under Changes.\nWrite a descriptive commit message (e.g., Add new resource: workshop-introDL).\nClick Commit to your-branch-name.\nClick on Repository &gt; Push to push your changes to GitHub.\n\n\n\n8. Make a Pull Request\n\nGo to your forked repository on GitHub.\nClick on the Compare & pull request button.\nEnsure you are merging into the ‚Äúmain‚Äù branch of the ML+X-Nexus repository.\nWrite a descriptive title and comment for your pull request.\nClick Create pull request.\n\n\n\n9. Wait for Review\n\nOne of the Nexus developers will review your pull request. They may provide feedback or request changes.\nAddress any feedback and push additional commits as needed.\n\n\n\n\nHow to write the post?\n\nExternal resources\nIf you are adding an external resource, please start with the template file. There are comments in the template which will help you make the appropriate edits for your resource. You can also check out how other posts have been formatted by clicking ‚ÄúImprove this page‚Äù from a given post‚Äôs webpage. This will bring you directly to the qmd file for that post.\n\n\nOrginal content\nTo write a post, there are many options: Write it using quarto, rmarkdown, or jupyter. The post could be a new file in the appropriate folder (Resources, ML-stories, or EDA), or a named folder with an index.[ipynb|qmd|rmd|md] extension. In any case, the header of the post needs to be a yaml section with the fields:\n---\ntitle: An Example\ndescription: |\n  An exploratory data analysis example\nauthor: ML+X\ndate-modified: \"last-modified\"\ndate-format: long\ncategories:\n  - EDA\n  - PCA\n---\nThe only fields that need to be changed are title, description, author and the categories. Ideally the categories should match the tags that are already in use in the site, e.g.¬†if tag that we are using for support vector machines is SVM then use that one instead of writing another one like support-vector-machines. You can also check out how other posts have been formatted by clicking ‚ÄúImprove this page‚Äù from a given post‚Äôs webpage. This will bring you directly to the qmd file for that post.\n\n\n\nWhere to locate your post?\nWe want the site to be constantly evolving with the community, and our intention is to keep the contributions to the site as free as possible. However, we added some sections to structure the site a little bit:\n.\n‚îú‚îÄ‚îÄ Resources\n‚îÇ   ‚îú‚îÄ‚îÄ Books\n‚îÇ   ‚îú‚îÄ‚îÄ Blogs\n‚îÇ   ‚îú‚îÄ‚îÄ Datasets \n‚îÇ   ‚îú‚îÄ‚îÄ EDA\n‚îÇ   ‚îú‚îÄ‚îÄ Guides\n‚îÇ   ‚îú‚îÄ‚îÄ Models \n‚îÇ   ‚îú‚îÄ‚îÄ Papers\n‚îÇ   ‚îú‚îÄ‚îÄ Podcasts \n‚îÇ   ‚îú‚îÄ‚îÄ Workshops\n‚îÇ   ‚îú‚îÄ‚îÄ Videos\n‚îÇ   ‚îú‚îÄ‚îÄ Other \nNote: Some subfolders may not exist yet (e.g., Resources/Podcasts, Resources/Datasets) since no one has contributed a resource from one of those categories yet (and git doesn‚Äôt allow empty folders). Feel free to start one of the missing folders, if applicable. If your resource doesn‚Äôt belong to one of the categories listed above, add it to a new Resources/Other subfolder for now.",
    "crumbs": [
      "How to contribute?"
    ]
  },
  {
    "objectID": "Resources/Guides/How-to-contribute.html#how-to-improve-an-existing-post",
    "href": "Resources/Guides/How-to-contribute.html#how-to-improve-an-existing-post",
    "title": "How to Contribute?",
    "section": "How to improve an existing post?",
    "text": "How to improve an existing post?\nAnyone is welcome and encouraged to suggest improvements to existing materials hosted on Nexus! The most straightforward way to do this is to click ‚ÄúImprove this page‚Äù from the post‚Äôs webpage on Nexus. This button can be found near the top of each post webpage on the right side.",
    "crumbs": [
      "How to contribute?"
    ]
  },
  {
    "objectID": "Resources/Guides/How-to-contribute.html#questions",
    "href": "Resources/Guides/How-to-contribute.html#questions",
    "title": "How to Contribute?",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions on how to contribute, please feel free to post to the Nexus Q&A on GitHub. We will improve this guide based on additional questions/comments we receive.",
    "crumbs": [
      "How to contribute?"
    ]
  },
  {
    "objectID": "Resources/Guides/index.html",
    "href": "Resources/Guides/index.html",
    "title": "Guides",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nCenter for Highthroughput Computing (CHTC)\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Contribute?\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOut-of-Distribution Detection\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVersion Control with GitHub Desktop\n\n\n17 min\n\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Learn",
      "Guides"
    ]
  },
  {
    "objectID": "Resources/Workshops/Intro-Python_Gapminder.html",
    "href": "Resources/Workshops/Intro-Python_Gapminder.html",
    "title": "Intro to Python (Carpentries)",
    "section": "",
    "text": "The Plotting and Programming in Python workshop provides an introduction to programming in Python 3 for people with little or no previous programming experience. It uses plotting as its motivating example to learn Python fundamentals, including functions, conditional logic, loops, and popular packages (e.g., Pandas).\n\n\nNo previous experience with programming necessary.\n\n\n\nThis workshop takes approximately 8 hours to complete.\n\n\n\nThe Carpentries is a global organization of researchers who volunteer their time and effort to create workshops that teach software engineering and data analysis skills to other researchers. UW-Madison has its own local Carpentries community which is actively engaged in developing new ML/AI workshops. To be notified of upcoming workshops offered by the Carpentries, make sure to subscribe to the Data Science @ UW Newsletter. The Intro Deep Learning workshop is typically taught in May each year.\n\n\n\nAll Carpentries lessons are published as open source educational materials. You are welcome and encouraged to visit the lesson materials to work through them on your own. If you are involved with a research lab at UW-Madison campus, you may attend Coding Meetup (Tue/Thur, 2:30-4:30pm) to get help working through the materials."
  },
  {
    "objectID": "Resources/Workshops/Intro-Python_Gapminder.html#about-this-resource",
    "href": "Resources/Workshops/Intro-Python_Gapminder.html#about-this-resource",
    "title": "Intro to Python (Carpentries)",
    "section": "",
    "text": "The Plotting and Programming in Python workshop provides an introduction to programming in Python 3 for people with little or no previous programming experience. It uses plotting as its motivating example to learn Python fundamentals, including functions, conditional logic, loops, and popular packages (e.g., Pandas).\n\n\nNo previous experience with programming necessary.\n\n\n\nThis workshop takes approximately 8 hours to complete.\n\n\n\nThe Carpentries is a global organization of researchers who volunteer their time and effort to create workshops that teach software engineering and data analysis skills to other researchers. UW-Madison has its own local Carpentries community which is actively engaged in developing new ML/AI workshops. To be notified of upcoming workshops offered by the Carpentries, make sure to subscribe to the Data Science @ UW Newsletter. The Intro Deep Learning workshop is typically taught in May each year.\n\n\n\nAll Carpentries lessons are published as open source educational materials. You are welcome and encouraged to visit the lesson materials to work through them on your own. If you are involved with a research lab at UW-Madison campus, you may attend Coding Meetup (Tue/Thur, 2:30-4:30pm) to get help working through the materials."
  },
  {
    "objectID": "Resources/Workshops/Intro-Python_Gapminder.html#questions",
    "href": "Resources/Workshops/Intro-Python_Gapminder.html#questions",
    "title": "Intro to Python (Carpentries)",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Resources/Workshops/Intro-Python_Gapminder.html#see-also",
    "href": "Resources/Workshops/Intro-Python_Gapminder.html#see-also",
    "title": "Intro to Python (Carpentries)",
    "section": "See also",
    "text": "See also\n\nWorkshop: Intro to Machine Learning with Sklearn: Once you master Python fundamentals, start using the scikit-learn package to begin exploring ‚Äúclassical‚Äù ML methods (e.g., regression, clustering, decision trees)."
  },
  {
    "objectID": "Resources/Workshops/Intro-Deeplearning_PyTorch.html",
    "href": "Resources/Workshops/Intro-Deeplearning_PyTorch.html",
    "title": "Intro to Deep Learning with PyTorch (Udacity)",
    "section": "",
    "text": "The Intro to Deep Learning with PyTorch workshop from Udacity will walk you through introductory deep learning concepts as well as how to build a neural networks in PyTorch. PyTorch is one of the most popular deep learning frameworks. Known for its speed and more ‚ÄúPythonic‚Äù feel, it is frequently the go-to choice for most researchers. The biggest downside of PyTorch, compared to a high-level framework like Keras, is that it is quite verbose. That is, you‚Äôll need to write a couple hundred lines of code to train and evaluate your neural network. Keras is a great alternative for those who are just getting started with neural networks or those that don‚Äôt need to train many models, as you can train/evaluate in just a dozen or so lines of code.\n\n\nLearners are expected to have the following knowledge:\n\nBasic Python programming skills and familiarity with the Pandas package. If you need a refresher, these Introductory Python lesson materials are available for independent study.\n\n\n\n\nTBD: Use the Improve this page functionality to add your own estimate!"
  },
  {
    "objectID": "Resources/Workshops/Intro-Deeplearning_PyTorch.html#about-this-resource",
    "href": "Resources/Workshops/Intro-Deeplearning_PyTorch.html#about-this-resource",
    "title": "Intro to Deep Learning with PyTorch (Udacity)",
    "section": "",
    "text": "The Intro to Deep Learning with PyTorch workshop from Udacity will walk you through introductory deep learning concepts as well as how to build a neural networks in PyTorch. PyTorch is one of the most popular deep learning frameworks. Known for its speed and more ‚ÄúPythonic‚Äù feel, it is frequently the go-to choice for most researchers. The biggest downside of PyTorch, compared to a high-level framework like Keras, is that it is quite verbose. That is, you‚Äôll need to write a couple hundred lines of code to train and evaluate your neural network. Keras is a great alternative for those who are just getting started with neural networks or those that don‚Äôt need to train many models, as you can train/evaluate in just a dozen or so lines of code.\n\n\nLearners are expected to have the following knowledge:\n\nBasic Python programming skills and familiarity with the Pandas package. If you need a refresher, these Introductory Python lesson materials are available for independent study.\n\n\n\n\nTBD: Use the Improve this page functionality to add your own estimate!"
  },
  {
    "objectID": "Resources/Workshops/Intro-Deeplearning_PyTorch.html#questions",
    "href": "Resources/Workshops/Intro-Deeplearning_PyTorch.html#questions",
    "title": "Intro to Deep Learning with PyTorch (Udacity)",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Resources/Workshops/Intro-Deeplearning_PyTorch.html#see-also",
    "href": "Resources/Workshops/Intro-Deeplearning_PyTorch.html#see-also",
    "title": "Intro to Deep Learning with PyTorch (Udacity)",
    "section": "See also",
    "text": "See also\n\nWorkshop: Intro to Deep Learning with Keras: Explore Keras as an alternative deep learning framework\nBook: Understanding Deep Learning - Simon J.D. Prince: This free textbook is a good modern overview of deep learning, and provides colab notebooks to explore deep learning concepts and implementations. The book uses PyTorch as its framework of choice. You may find additional details in this book that the workshop only briefly touches on."
  },
  {
    "objectID": "Resources/Workshops/index.html",
    "href": "Resources/Workshops/index.html",
    "title": "Workshops",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nIntro to Deep Learning with Keras (Carpentries)\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Deep Learning with PyTorch (Udacity)\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Machine Learning with Sklearn (Carpentries)\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Python (Carpentries)\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Text Analysis (Carpentries)\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVersion Control with Git and GitHub (Carpentries)\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Learn",
      "Workshops"
    ]
  },
  {
    "objectID": "Resources/Books/Intro-Deeplearning_SimonJDPrince.html",
    "href": "Resources/Books/Intro-Deeplearning_SimonJDPrince.html",
    "title": "Understanding Deep Learning",
    "section": "",
    "text": "Nowadays, nearly anyone can implement a deep learning model in a just a few lines of code. What separates the novices from the experts, however, is the ability to understand (or at least predict!) how these models work in different circumstances.\nSimon J.D. Prince‚Äôs free textbook, Understanding Deep Learning, provides a modern overview of deep learning (including newer topics like double descent and transformer models), and provides colab notebooks (!!!) to explore deep learning concepts and implementations. The book uses PyTorch as its framework of choice.\n\n\nThe title of this book is ‚ÄúUnderstanding Deep Learning‚Äù to distinguish it from volumes that cover coding and other practical aspects. This text is primarily about the ideas that underlie deep learning. The first part of the book introduces deep learning models and discusses how to train them, measure their performance, and improve this performance. The next part considers architectures that are specialized to images, text, and graph data. These chapters require only introductory linear algebra, calculus, and probability and should be accessible to any second-year undergraduate in a quantitative discipline. Subsequent parts of the book tackle generative models and reinforcement learning. These chapters require more knowledge of probability and calculus and target more advanced students. The title is also partly a joke ‚Äî no-one really understands deep learning at the time of writing. Modern deep networks learn piecewise linear functions with more regions than there are atoms in the universe and can be trained with fewer data examples than model parameters. It is neither obvious that we should be able to fit these functions reliably nor that they should generalize well to new data. The penultimate chapter addresses these and other aspects that are not yet fully understood. Regardless, deep learning will change the world for better or worse. The final chapter discusses AI ethics and concludes with an appeal for practitioners to consider the moral implications of their work.\nThe title is also partly a joke ‚Äî no-one really understands deep learning at the time of writing. Modern deep networks learn piecewise linear functions with more regions than there are atoms in the universe and can be trained with fewer data examples than model parameters. It is neither obvious that we should be able to fit these functions reliably nor that they should generalize well to new data. The penultimate chapter addresses these and other aspects that are not yet fully understood. Regardless, deep learning will change the world for better or worse. The final chapter discusses AI ethics and concludes with an appeal for practitioners to consider the moral implications of their work.\n\n\nLearners are expected to have the following knowledge:\n\nLinear algebra: linear algebra is the language of machine learning\nCalculus: recommended to understand gradient descent\nProbability theory: needed for reinforcement learning\nPyTorch: recommended for following along with Colab notebooks\n\n\n\n\nTBD: Use the Improve this page functionality to add your own estimate!"
  },
  {
    "objectID": "Resources/Books/Intro-Deeplearning_SimonJDPrince.html#about-this-resource",
    "href": "Resources/Books/Intro-Deeplearning_SimonJDPrince.html#about-this-resource",
    "title": "Understanding Deep Learning",
    "section": "",
    "text": "Nowadays, nearly anyone can implement a deep learning model in a just a few lines of code. What separates the novices from the experts, however, is the ability to understand (or at least predict!) how these models work in different circumstances.\nSimon J.D. Prince‚Äôs free textbook, Understanding Deep Learning, provides a modern overview of deep learning (including newer topics like double descent and transformer models), and provides colab notebooks (!!!) to explore deep learning concepts and implementations. The book uses PyTorch as its framework of choice.\n\n\nThe title of this book is ‚ÄúUnderstanding Deep Learning‚Äù to distinguish it from volumes that cover coding and other practical aspects. This text is primarily about the ideas that underlie deep learning. The first part of the book introduces deep learning models and discusses how to train them, measure their performance, and improve this performance. The next part considers architectures that are specialized to images, text, and graph data. These chapters require only introductory linear algebra, calculus, and probability and should be accessible to any second-year undergraduate in a quantitative discipline. Subsequent parts of the book tackle generative models and reinforcement learning. These chapters require more knowledge of probability and calculus and target more advanced students. The title is also partly a joke ‚Äî no-one really understands deep learning at the time of writing. Modern deep networks learn piecewise linear functions with more regions than there are atoms in the universe and can be trained with fewer data examples than model parameters. It is neither obvious that we should be able to fit these functions reliably nor that they should generalize well to new data. The penultimate chapter addresses these and other aspects that are not yet fully understood. Regardless, deep learning will change the world for better or worse. The final chapter discusses AI ethics and concludes with an appeal for practitioners to consider the moral implications of their work.\nThe title is also partly a joke ‚Äî no-one really understands deep learning at the time of writing. Modern deep networks learn piecewise linear functions with more regions than there are atoms in the universe and can be trained with fewer data examples than model parameters. It is neither obvious that we should be able to fit these functions reliably nor that they should generalize well to new data. The penultimate chapter addresses these and other aspects that are not yet fully understood. Regardless, deep learning will change the world for better or worse. The final chapter discusses AI ethics and concludes with an appeal for practitioners to consider the moral implications of their work.\n\n\nLearners are expected to have the following knowledge:\n\nLinear algebra: linear algebra is the language of machine learning\nCalculus: recommended to understand gradient descent\nProbability theory: needed for reinforcement learning\nPyTorch: recommended for following along with Colab notebooks\n\n\n\n\nTBD: Use the Improve this page functionality to add your own estimate!"
  },
  {
    "objectID": "Resources/Books/Intro-Deeplearning_SimonJDPrince.html#questions",
    "href": "Resources/Books/Intro-Deeplearning_SimonJDPrince.html#questions",
    "title": "Understanding Deep Learning",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Resources/Books/Intro-Deeplearning_SimonJDPrince.html#see-also",
    "href": "Resources/Books/Intro-Deeplearning_SimonJDPrince.html#see-also",
    "title": "Understanding Deep Learning",
    "section": "See also",
    "text": "See also\n\nWorkshop: Intro to Deep Learning with PyTorch: Explore PyTorch as an alternative deep learning framework.\nWorkshop: Intro to Deep Learning with Keras: Explore Keras as an alternative deep learning framework"
  },
  {
    "objectID": "Resources/Videos/Intro-git-github.html",
    "href": "Resources/Videos/Intro-git-github.html",
    "title": "Version Control with Git and GitHub (Carpentries)",
    "section": "",
    "text": "The below video (and the 7 subsequent videos in the workshop playlist) will walk you through this introductory Git workshop from the Carpentries: Version Control with Git/GitHub. Git is a free and open source version control system that has become the #1 choice for software developers both in research and industry. Unlike centralized version control systems where there is a single central repository, Git allows every user to have a full copy of the entire project history on their own machine. This distributed nature enables multiple people to work on a project simultaneously without interfering with each other‚Äôs work. Git stores the history of changes in a project, enabling users to track progress, revert to previous states, and manage branches for different features or versions of a project. GitHub is a web-based platform that uses Git for version control. It provides a collaborative environment where users can host and review code, manage projects, and build software alongside millions of other developers. GitHub also offers additional features such as issue tracking, project management tools, and continuous integration workflows.\n\n\n\nIn this lesson we use Git from the Unix Shell. Some previous experience with the shell is expected, but isn‚Äôt mandatory. For help with Unix Shell, check out the Intro to Unix Shell workshop (Carpentries).\n\n\n\n\n\nThis workshop takes rough 3-4 hours to complete."
  },
  {
    "objectID": "Resources/Videos/Intro-git-github.html#about-this-resource",
    "href": "Resources/Videos/Intro-git-github.html#about-this-resource",
    "title": "Version Control with Git and GitHub (Carpentries)",
    "section": "",
    "text": "The below video (and the 7 subsequent videos in the workshop playlist) will walk you through this introductory Git workshop from the Carpentries: Version Control with Git/GitHub. Git is a free and open source version control system that has become the #1 choice for software developers both in research and industry. Unlike centralized version control systems where there is a single central repository, Git allows every user to have a full copy of the entire project history on their own machine. This distributed nature enables multiple people to work on a project simultaneously without interfering with each other‚Äôs work. Git stores the history of changes in a project, enabling users to track progress, revert to previous states, and manage branches for different features or versions of a project. GitHub is a web-based platform that uses Git for version control. It provides a collaborative environment where users can host and review code, manage projects, and build software alongside millions of other developers. GitHub also offers additional features such as issue tracking, project management tools, and continuous integration workflows.\n\n\n\nIn this lesson we use Git from the Unix Shell. Some previous experience with the shell is expected, but isn‚Äôt mandatory. For help with Unix Shell, check out the Intro to Unix Shell workshop (Carpentries).\n\n\n\n\n\nThis workshop takes rough 3-4 hours to complete."
  },
  {
    "objectID": "Resources/Videos/Intro-git-github.html#questions",
    "href": "Resources/Videos/Intro-git-github.html#questions",
    "title": "Version Control with Git and GitHub (Carpentries)",
    "section": "Questions?",
    "text": "Questions?\nIf you any lingering questions about this resource, please feel free to post to the Nexus Q&A on GitHub. We will improve materials on this website as additional questions come in."
  },
  {
    "objectID": "Resources/Videos/Intro-git-github.html#see-also",
    "href": "Resources/Videos/Intro-git-github.html#see-also",
    "title": "Version Control with Git and GitHub (Carpentries)",
    "section": "See also",
    "text": "See also\n\nGuide: Version Control with GitHub Desktop: GitHub Desktop is a graphical user interface (GUI) application that simplifies the use of Git and GitHub. It is designed for users who prefer not to use the command line interface, offering a more intuitive and visual approach to version control. With GitHub Desktop, you can easily perform common Git tasks such as committing changes, creating branches, and resolving merge conflicts, all within a user-friendly interface."
  },
  {
    "objectID": "Resources/Videos/index.html",
    "href": "Resources/Videos/index.html",
    "title": "Videos",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nOut-of-Distribution Detection\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVersion Control with Git and GitHub (Carpentries)\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Learn",
      "Videos"
    ]
  },
  {
    "objectID": "includes/common-resources-text.html",
    "href": "includes/common-resources-text.html",
    "title": "Nexus: Crowdsourced ML Resources",
    "section": "",
    "text": "Any content (original or external) that can help make the practice of ML more connected, accessible, efficient, and reproducible is welcome on the Nexus platform! This includes, but is not limited to:\n\nüß† Educational materials: Explore a library of educational materials (workshops, guides, books, videos, etc.) covering a wide range of ML-related topics, tools, and workflows, from foundational concepts to advanced techniques. These materials offer clear explanations, practical examples, and actionable insights to help you navigate the complexities of ML with confidence.\nüß¨ Applications & stories: Discover a curated collection of blogs, papers, and talks which dive into ML applications and lessons learned by practitioners, including exploratory data analysis or EDA case studies. These case studies demonstrate the domain knowledge needed to explore different data types (time-series, tabular, images, etc.) from various fields.\nüõ† Coming soon! Models & data: Learn about useful pretrained models, foundation models, and datasets that you can leverage for your next ML project. Learn about their features, how to use them effectively, and see examples of them in action."
  }
]